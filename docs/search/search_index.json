{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DocArray","text":"<p> The data structure for multimodal data </p> <p> </p> <p>Note The README you're currently viewing is for DocArray&gt;0.30, which introduces some significant changes from DocArray 0.21. If you wish to continue using the older DocArray &lt;=0.21, ensure you install it via <code>pip install docarray==0.21</code>. Refer to its codebase, documentation, and its hot-fixes branch for more information.</p> <p>DocArray is a Python library expertly crafted for the representation, transmission, storage, and retrieval of multimodal data. Tailored for the development of multimodal AI applications, its design guarantees seamless integration with the extensive Python and machine learning ecosystems. As of January 2022, DocArray is openly distributed under the Apache License 2.0 and currently enjoys the status of a sandbox project within the LF AI &amp; Data Foundation.</p> <ul> <li> Offers native support for NumPy, PyTorch, TensorFlow, and JAX, catering specifically to model training scenarios.</li> <li> Based on Pydantic, and instantly compatible with web and microservice frameworks like FastAPI and Jina.</li> <li> Provides support for vector databases such as Weaviate, Qdrant, ElasticSearch, Redis, and HNSWLib.</li> <li> Allows data transmission as JSON over HTTP or as Protobuf over gRPC.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install DocArray from the CLI, run the following command:</p> <pre><code>pip install -U docarray\n</code></pre> <p>Note To use DocArray &lt;=0.21, make sure you install via <code>pip install docarray==0.21</code> and check out its codebase and docs and its hot-fixes branch.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>New to DocArray? Depending on your use case and background, there are multiple ways to learn about DocArray:</p> <ul> <li>Coming from pure PyTorch or TensorFlow</li> <li>Coming from Pydantic</li> <li>Coming from FastAPI</li> <li>Coming from Jina</li> <li>Coming from a vector database</li> <li>Coming from Langchain</li> </ul>"},{"location":"#represent","title":"Represent","text":"<p>DocArray empowers you to represent your data in a manner that is inherently attuned to machine learning.</p> <p>This is particularly beneficial for various scenarios:</p> <ul> <li>:running: You are training a model: You're dealing with tensors of varying shapes and sizes, each signifying different elements. You desire a method to logically organize them.</li> <li> You are serving a model: Let's say through FastAPI, and you wish to define your API endpoints precisely.</li> <li> You are parsing data: Perhaps for future deployment in your machine learning or data science projects.</li> </ul> <p> Familiar with Pydantic? You'll be pleased to learn that DocArray is not only constructed atop Pydantic but also maintains complete compatibility with it! Furthermore, we have a specific section dedicated to your needs!</p> <p>In essence, DocArray facilitates data representation in a way that mirrors Python dataclasses, with machine learning being an integral component:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor, ImageUrl\nimport torch\n# Define your data model\nclass MyDocument(BaseDoc):\ndescription: str\nimage_url: ImageUrl  # could also be VideoUrl, AudioUrl, etc.\nimage_tensor: TorchTensor[1704, 2272, 3]  # you can express tensor shapes!\n# Stack multiple documents in a Document Vector\nfrom docarray import DocVec\nvec = DocVec[MyDocument](\n[\nMyDocument(\ndescription=\"A cat\",\nimage_url=\"https://example.com/cat.jpg\",\nimage_tensor=torch.rand(1704, 2272, 3),\n),\n]\n* 10\n)\nprint(vec.image_tensor.shape)  # (10, 1704, 2272, 3)\n</code></pre> Click for more details <p>Let's take a closer look at how you can represent your data with DocArray:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor, ImageUrl\nfrom typing import Optional\nimport torch\n# Define your data model\nclass MyDocument(BaseDoc):\ndescription: str\nimage_url: ImageUrl  # could also be VideoUrl, AudioUrl, etc.\nimage_tensor: Optional[\nTorchTensor[1704, 2272, 3]\n] = None  # could also be NdArray or TensorflowTensor\nembedding: Optional[TorchTensor] = None\n</code></pre> <p>So not only can you define the types of your data, you can even specify the shape of your tensors!</p> <pre><code># Create a document\ndoc = MyDocument(\ndescription=\"This is a photo of a mountain\",\nimage_url=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\n)\n# Load image tensor from URL\ndoc.image_tensor = doc.image_url.load()\n# Compute embedding with any model of your choice\ndef clip_image_encoder(image_tensor: TorchTensor) -&gt; TorchTensor:  # dummy function\nreturn torch.rand(512)\ndoc.embedding = clip_image_encoder(doc.image_tensor)\nprint(doc.embedding.shape)  # torch.Size([512])\n</code></pre>"},{"location":"#compose-nested-documents","title":"Compose nested Documents","text":"<p>Of course, you can compose Documents into a nested structure:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc\nimport numpy as np\nclass MultiModalDocument(BaseDoc):\nimage_doc: ImageDoc\ntext_doc: TextDoc\ndoc = MultiModalDocument(\nimage_doc=ImageDoc(tensor=np.zeros((3, 224, 224))), text_doc=TextDoc(text='hi!')\n)\n</code></pre> <p>You rarely work with a single data point at a time, especially in machine learning applications. That's why you can easily collect multiple <code>Documents</code>:</p>"},{"location":"#collect-multiple-documents","title":"Collect multiple <code>Documents</code>","text":"<p>When building or interacting with an ML system, usually you want to process multiple Documents (data points) at once.</p> <p>DocArray offers two data structures for this:</p> <ul> <li><code>DocVec</code>: A vector of <code>Documents</code>. All tensors in the documents are stacked into a single tensor. Perfect for batch processing and use inside of ML models.</li> <li><code>DocList</code>: A list of <code>Documents</code>. All tensors in the documents are kept as-is. Perfect for streaming, re-ranking, and shuffling of data.</li> </ul> <p>Let's take a look at them, starting with <code>DocVec</code>:</p> <pre><code>from docarray import DocVec, BaseDoc\nfrom docarray.typing import AnyTensor, ImageUrl\nimport numpy as np\nclass Image(BaseDoc):\nurl: ImageUrl\ntensor: AnyTensor  # this allows torch, numpy, and tensor flow tensors\nvec = DocVec[Image](  # the DocVec is parametrized by your personal schema!\n[\nImage(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\n)\nfor _ in range(100)\n]\n)\n</code></pre> <p>In the code snippet above, <code>DocVec</code> is parametrized by the type of document you want to use with it: <code>DocVec[Image]</code>.</p> <p>This may look weird at first, but we're confident that you'll get used to it quickly! Besides, it lets us do some cool things, like having bulk access to the fields that you defined in your document:</p> <pre><code>tensor = vec.tensor  # gets all the tensors in the DocVec\nprint(tensor.shape)  # which are stacked up into a single tensor!\nprint(vec.url)  # you can bulk access any other field, too\n</code></pre> <p>The second data structure, <code>DocList</code>, works in a similar way:</p> <pre><code>from docarray import DocList\ndl = DocList[Image](  # the DocList is parametrized by your personal schema!\n[\nImage(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\n)\nfor _ in range(100)\n]\n)\n</code></pre> <p>You can still bulk access the fields of your document:</p> <pre><code>tensors = dl.tensor  # gets all the tensors in the DocList\nprint(type(tensors))  # as a list of tensors\nprint(dl.url)  # you can bulk access any other field, too\n</code></pre> <p>And you can insert, remove, and append documents to your <code>DocList</code>:</p> <pre><code># append\ndl.append(\nImage(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\n)\n)\n# delete\ndel dl[0]\n# insert\ndl.insert(\n0,\nImage(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\n),\n)\n</code></pre> <p>And you can seamlessly switch between <code>DocVec</code> and <code>DocList</code>:</p> <pre><code>vec_2 = dl.to_doc_vec()\nassert isinstance(vec_2, DocVec)\ndl_2 = vec_2.to_doc_list()\nassert isinstance(dl_2, DocList)\n</code></pre>"},{"location":"#send","title":"Send","text":"<p>DocArray facilitates the transmission of your data in a manner inherently compatible with machine learning.</p> <p>This includes native support for Protobuf and gRPC, along with HTTP and serialization to JSON, JSONSchema, Base64, and Bytes.</p> <p>This feature proves beneficial for several scenarios:</p> <ul> <li> You are serving a model, perhaps through frameworks like Jina or FastAPI</li> <li> You are distributing your model across multiple machines and need an efficient means of transmitting your data between nodes</li> <li> You are architecting a microservice environment and require a method for data transmission between microservices</li> </ul> <p> Are you familiar with FastAPI? You'll be delighted to learn that DocArray maintains full compatibility with FastAPI! Plus, we have a dedicated section specifically for you!</p> <p>When it comes to data transmission, serialization is a crucial step. Let's delve into how DocArray streamlines this process:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageTorchTensor\nimport torch\n# model your data\nclass MyDocument(BaseDoc):\ndescription: str\nimage: ImageTorchTensor[3, 224, 224]\n# create a Document\ndoc = MyDocument(\ndescription=\"This is a description\",\nimage=torch.zeros((3, 224, 224)),\n)\n# serialize it!\nproto = doc.to_protobuf()\nbytes_ = doc.to_bytes()\njson = doc.json()\n# deserialize it!\ndoc_2 = MyDocument.from_protobuf(proto)\ndoc_4 = MyDocument.from_bytes(bytes_)\ndoc_5 = MyDocument.parse_raw(json)\n</code></pre> <p>Of course, serialization is not all you need. So check out how DocArray integrates with Jina and FastAPI.</p>"},{"location":"#store","title":"Store","text":"<p>After modeling and possibly distributing your data, you'll typically want to store it somewhere. That's where DocArray steps in!</p> <p>Document Stores provide a seamless way to, as the name suggests, store your Documents. Be it locally or remotely, you can do it all through the same user interface:</p> <ul> <li> On disk, as a file in your local filesystem</li> <li> On AWS S3</li> <li> On Jina AI Cloud</li> </ul> <p>The Document Store interface lets you push and pull Documents to and from multiple data sources, all with the same user interface.</p> <p>For example, let's see how that works with on-disk storage:</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndocs = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(8)])\ndocs.push('file://simple_docs')\ndocs_pull = DocList[SimpleDoc].pull('file://simple_docs')\n</code></pre>"},{"location":"#retrieve","title":"Retrieve","text":"<p>Document Indexes let you index your Documents in a vector database for efficient similarity-based retrieval.</p> <p>This is useful for:</p> <ul> <li> Augmenting LLMs and Chatbots with domain knowledge (Retrieval Augmented Generation)</li> <li> Neural search applications</li> <li> Recommender systems</li> </ul> <p>Currently, Document Indexes support Weaviate, Qdrant, ElasticSearch,  Redis, and HNSWLib, with more to come!</p> <p>The Document Index interface lets you index and retrieve Documents from multiple vector databases, all with the same user interface.</p> <p>It supports ANN vector search, text search, filtering, and hybrid search.</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.index import HnswDocumentIndex\nimport numpy as np\nfrom docarray.typing import ImageUrl, ImageTensor, NdArray\nclass ImageDoc(BaseDoc):\nurl: ImageUrl\ntensor: ImageTensor\nembedding: NdArray[128]\n# create some data\ndl = DocList[ImageDoc](\n[\nImageDoc(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\nembedding=np.random.random((128,)),\n)\nfor _ in range(100)\n]\n)\n# create a Document Index\nindex = HnswDocumentIndex[ImageDoc](work_dir='/tmp/test_index')\n# index your data\nindex.index(dl)\n# find similar Documents\nquery = dl[0]\nresults, scores = index.find(query, limit=10, search_field='embedding')\n</code></pre>"},{"location":"#learn-docarray","title":"Learn DocArray","text":"<p>Depending on your background and use case, there are different ways for you to understand DocArray.</p>"},{"location":"#coming-from-docarray-021","title":"Coming from DocArray &lt;=0.21","text":"Click to expand <p>If you are using DocArray version 0.30.0 or lower, you will be familiar with its dataclass API.</p> <p>DocArray &gt;=0.30 is that idea, taken seriously. Every document is created through a dataclass-like interface, courtesy of Pydantic.</p> <p>This gives the following advantages: - Flexibility: No need to conform to a fixed set of fields -- your data defines the schema - Multimodality: At their core, documents are just dictionaries. This makes it easy to create and send them from any language, not just Python.</p> <p>You may also be familiar with our old Document Stores for vector DB integration. They are now called Document Indexes and offer the following improvements (see here for the new API):</p> <ul> <li>Hybrid search: You can now combine vector search with text search, and even filter by arbitrary fields</li> <li>Production-ready: The new Document Indexes are a much thinner wrapper around the various vector DB libraries, making them more robust and easier to maintain</li> <li>Increased flexibility: We strive to support any configuration or setting that you could perform through the DB's first-party client</li> </ul> <p>For now, Document Indexes support Weaviate, Qdrant, ElasticSearch, Redis,  Exact Nearest Neighbour search and HNSWLib, with more to come.</p>"},{"location":"#coming-from-pydantic","title":"Coming from Pydantic","text":"Click to expand <p>If you come from Pydantic, you can see DocArray documents as juiced up Pydantic models, and DocArray as a collection of goodies around them.</p> <p>More specifically, we set out to make Pydantic fit for the ML world - not by replacing it, but by building on top of it!</p> <p>This means you get the following benefits:</p> <ul> <li>ML-focused types: Tensor, TorchTensor, Embedding, ..., including tensor shape validation</li> <li>Full compatibility with FastAPI</li> <li>DocList and DocVec generalize the idea of a model to a sequence or batch of models. Perfect for use in ML models and other batch processing tasks.</li> <li>Types that are alive: ImageUrl can <code>.load()</code> a URL to image tensor, TextUrl can load and tokenize text documents, etc.</li> <li>Cloud-ready: Serialization to Protobuf for use with microservices and gRPC</li> <li>Pre-built multimodal documents for different data modalities: Image, Text, 3DMesh, Video, Audio and more. Note that all of these are valid Pydantic models!</li> <li>Document Stores and Document Indexes let you store your data and retrieve it using vector search</li> </ul> <p>The most obvious advantage here is first-class support for ML centric data, such as <code>{Torch, TF, ...}Tensor</code>, <code>Embedding</code>, etc.</p> <p>This includes handy features such as validating the shape of a tensor:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor\nimport torch\nclass MyDoc(BaseDoc):\ntensor: TorchTensor[3, 224, 224]\ndoc = MyDoc(tensor=torch.zeros(3, 224, 224))  # works\ndoc = MyDoc(tensor=torch.zeros(224, 224, 3))  # works by reshaping\ntry:\ndoc = MyDoc(tensor=torch.zeros(224))  # fails validation\nexcept Exception as e:\nprint(e)\n# tensor\n# Cannot reshape tensor of shape (224,) to shape (3, 224, 224) (type=value_error)\nclass Image(BaseDoc):\ntensor: TorchTensor[3, 'x', 'x']\nImage(tensor=torch.zeros(3, 224, 224))  # works\ntry:\nImage(\ntensor=torch.zeros(3, 64, 128)\n)  # fails validation because second dimension does not match third\nexcept Exception as e:\nprint()\ntry:\nImage(\ntensor=torch.zeros(4, 224, 224)\n)  # fails validation because of the first dimension\nexcept Exception as e:\nprint(e)\n# Tensor shape mismatch. Expected(3, 'x', 'x'), got(4, 224, 224)(type=value_error)\ntry:\nImage(\ntensor=torch.zeros(3, 64)\n)  # fails validation because it does not have enough dimensions\nexcept Exception as e:\nprint(e)\n# Tensor shape mismatch. Expected (3, 'x', 'x'), got (3, 64) (type=value_error)\n</code></pre>"},{"location":"#coming-from-pytorch","title":"Coming from PyTorch","text":"Click to expand <p>If you come from PyTorch, you can see DocArray mainly as a way of organizing your data as it flows through your model.</p> <p>It offers you several advantages:</p> <ul> <li>Express tensor shapes in type hints</li> <li>Group tensors that belong to the same object, e.g. an audio track and an image</li> <li>Go directly to deployment, by re-using your data model as a FastAPI or Jina API schema</li> <li>Connect model components between microservices, using Protobuf and gRPC</li> </ul> <p>DocArray can be used directly inside ML models to handle and represent multimodaldata. This allows you to reason about your data using DocArray's abstractions deep inside of <code>nn.Module</code>, and provides a FastAPI-compatible schema that eases the transition between model training and model serving.</p> <p>To see the effect of this, let's first observe a vanilla PyTorch implementation of a tri-modal ML model:</p> <pre><code>import torch\nfrom torch import nn\ndef encoder(x):\nreturn torch.rand(512)\nclass MyMultiModalModel(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.audio_encoder = encoder()\nself.image_encoder = encoder()\nself.text_encoder = encoder()\ndef forward(self, text_1, text_2, image_1, image_2, audio_1, audio_2):\nembedding_text_1 = self.text_encoder(text_1)\nembedding_text_2 = self.text_encoder(text_2)\nembedding_image_1 = self.image_encoder(image_1)\nembedding_image_2 = self.image_encoder(image_2)\nembedding_audio_1 = self.image_encoder(audio_1)\nembedding_audio_2 = self.image_encoder(audio_2)\nreturn (\nembedding_text_1,\nembedding_text_2,\nembedding_image_1,\nembedding_image_2,\nembedding_audio_1,\nembedding_audio_2,\n)\n</code></pre> <p>Not very easy on the eyes if you ask us. And even worse, if you need to add one more modality you have to touch every part of your code base, changing the <code>forward()</code> return type and making a whole lot of changes downstream from that.</p> <p>So, now let's see what the same code looks like with DocArray:</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc, AudioDoc\nfrom docarray.typing import TorchTensor\nfrom torch import nn\nimport torch\ndef encoder(x):\nreturn torch.rand(512)\nclass Podcast(BaseDoc):\ntext: TextDoc\nimage: ImageDoc\naudio: AudioDoc\nclass PairPodcast(BaseDoc):\nleft: Podcast\nright: Podcast\nclass MyPodcastModel(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.audio_encoder = encoder()\nself.image_encoder = encoder()\nself.text_encoder = encoder()\ndef forward_podcast(self, docs: DocList[Podcast]) -&gt; DocList[Podcast]:\ndocs.audio.embedding = self.audio_encoder(docs.audio.tensor)\ndocs.text.embedding = self.text_encoder(docs.text.tensor)\ndocs.image.embedding = self.image_encoder(docs.image.tensor)\nreturn docs\ndef forward(self, docs: DocList[PairPodcast]) -&gt; DocList[PairPodcast]:\ndocs.left = self.forward_podcast(docs.left)\ndocs.right = self.forward_podcast(docs.right)\nreturn docs\n</code></pre> <p>Looks much better, doesn't it? You instantly win in code readability and maintainability. And for the same price you can turn your PyTorch model into a FastAPI app and reuse your Document schema definition (see below). Everything is handled in a pythonic manner by relying on type hints.</p>"},{"location":"#coming-from-tensorflow","title":"Coming from TensorFlow","text":"Click to expand <p>Like the PyTorch approach, you can also use DocArray with TensorFlow to handle and represent multimodal data inside your ML model.</p> <p>First off, to use DocArray with TensorFlow we first need to install it as follows:</p> <pre><code>pip install tensorflow==2.12.0\npip install protobuf==3.19.0\n</code></pre> <p>Compared to using DocArray with PyTorch, there is one main difference when using it with TensorFlow: While DocArray's <code>TorchTensor</code> is a subclass of <code>torch.Tensor</code>, this is not the case for the <code>TensorFlowTensor</code>: Due to some technical limitations of <code>tf.Tensor</code>, DocArray's <code>TensorFlowTensor</code> is not a subclass of <code>tf.Tensor</code> but rather stores a <code>tf.Tensor</code> in its <code>.tensor</code> attribute. </p> <p>How does this affect you? Whenever you want to access the tensor data to, let's say, do operations with it or hand it to your ML model, instead of handing over your <code>TensorFlowTensor</code> instance, you need to access its <code>.tensor</code> attribute.</p> <p>This would look like the following:</p> <pre><code>from typing import Optional\nfrom docarray import DocList, BaseDoc\nimport tensorflow as tf\nclass Podcast(BaseDoc):\naudio_tensor: Optional[AudioTensorFlowTensor] = None\nembedding: Optional[AudioTensorFlowTensor] = None\nclass MyPodcastModel(tf.keras.Model):\ndef __init__(self):\nsuper().__init__()\nself.audio_encoder = AudioEncoder()\ndef call(self, inputs: DocList[Podcast]) -&gt; DocList[Podcast]:\ninputs.audio_tensor.embedding = self.audio_encoder(\ninputs.audio_tensor.tensor\n)  # access audio_tensor's .tensor attribute\nreturn inputs\n</code></pre>"},{"location":"#coming-from-fastapi","title":"Coming from FastAPI","text":"Click to expand <p>Documents are Pydantic Models (with a twist), and as such they are fully compatible with FastAPI!</p> <p>But why should you use them, and not the Pydantic models you already know and love? Good question!</p> <ul> <li>Because of the ML-first features, types and validations, here</li> <li>Because DocArray can act as an ORM for vector databases, similar to what SQLModel does for SQL databases</li> </ul> <p>And to seal the deal, let us show you how easily documents slot into your FastAPI app:</p> <pre><code>import numpy as np\nfrom fastapi import FastAPI\nfrom docarray.base_doc import DocArrayResponse\nfrom docarray import BaseDoc\nfrom docarray.documents import ImageDoc\nfrom docarray.typing import NdArray, ImageTensor\nclass InputDoc(BaseDoc):\nimg: ImageDoc\ntext: str\nclass OutputDoc(BaseDoc):\nembedding_clip: NdArray\nembedding_bert: NdArray\napp = FastAPI()\ndef model_img(img: ImageTensor) -&gt; NdArray:\nreturn np.zeros((100, 1))\ndef model_text(text: str) -&gt; NdArray:\nreturn np.zeros((100, 1))\n@app.post(\"/embed/\", response_model=OutputDoc, response_class=DocArrayResponse)\nasync def create_item(doc: InputDoc) -&gt; OutputDoc:\ndoc = OutputDoc(\nembedding_clip=model_img(doc.img.tensor), embedding_bert=model_text(doc.text)\n)\nreturn doc\ninput_doc = InputDoc(text='', img=ImageDoc(tensor=np.random.random((3, 224, 224))))\nasync with AsyncClient(app=app, base_url=\"http://test\") as ac:\nresponse = await ac.post(\"/embed/\", data=input_doc.json())\n</code></pre> <p>Just like a vanilla Pydantic model!</p>"},{"location":"#coming-from-jina","title":"Coming from Jina","text":"Click to expand <p>Jina has adopted docarray as their library for representing and serializing Documents.</p> <p>Jina allows to serve models and services that are built with DocArray allowing you to serve and scale these applications making full use of DocArray's serialization capabilites. </p> <pre><code>import numpy as np\nfrom jina import Deployment, Executor, requests\nfrom docarray import BaseDoc, DocList\nfrom docarray.documents import ImageDoc\nfrom docarray.typing import NdArray, ImageTensor\nclass InputDoc(BaseDoc):\nimg: ImageDoc\ntext: str\nclass OutputDoc(BaseDoc):\nembedding_clip: NdArray\nembedding_bert: NdArray\ndef model_img(img: ImageTensor) -&gt; NdArray:\nreturn np.zeros((100, 1))\ndef model_text(text: str) -&gt; NdArray:\nreturn np.zeros((100, 1))\nclass MyEmbeddingExecutor(Executor):\n@requests(on='/embed')\ndef encode(self, docs: DocList[InputDoc], **kwargs) -&gt; DocList[OutputDoc]:\nret = DocList[OutputDoc]()\nfor doc in docs:\noutput = OutputDoc(\nembedding_clip=model_img(doc.img.tensor),\nembedding_bert=model_text(doc.text),\n)\nret.append(output)\nreturn ret\nwith Deployment(\nprotocols=['grpc', 'http'], ports=[12345, 12346], uses=MyEmbeddingExecutor\n) as dep:\nresp = dep.post(\non='/embed',\ninputs=DocList[InputDoc](\n[InputDoc(text='', img=ImageDoc(tensor=np.random.random((3, 224, 224))))]\n),\nreturn_type=DocList[OutputDoc],\n)\nprint(resp)\n</code></pre>"},{"location":"#coming-from-a-vector-database","title":"Coming from a vector database","text":"Click to expand <p>If you came across DocArray as a universal vector database client, you can best think of it as a new kind of ORM for vector databases. DocArray's job is to take multimodal, nested and domain-specific data and to map it to a vector database, store it there, and thus make it searchable:</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.index import HnswDocumentIndex\nimport numpy as np\nfrom docarray.typing import ImageUrl, ImageTensor, NdArray\nclass ImageDoc(BaseDoc):\nurl: ImageUrl\ntensor: ImageTensor\nembedding: NdArray[128]\n# create some data\ndl = DocList[ImageDoc](\n[\nImageDoc(\nurl=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Alpamayo.jpg\",\ntensor=np.zeros((3, 224, 224)),\nembedding=np.random.random((128,)),\n)\nfor _ in range(100)\n]\n)\n# create a Document Index\nindex = HnswDocumentIndex[ImageDoc](work_dir='/tmp/test_index2')\n# index your data\nindex.index(dl)\n# find similar Documents\nquery = dl[0]\nresults, scores = index.find(query, limit=10, search_field='embedding')\n</code></pre> <p>Currently, DocArray supports the following vector databases:</p> <ul> <li>Weaviate</li> <li>Qdrant</li> <li>Elasticsearch v8 and v7</li> <li>Redis</li> <li>Milvus</li> <li>ExactNNMemorySearch as a local alternative with exact kNN search.</li> <li>HNSWlib as a local-first ANN alternative</li> </ul> <p>An integration of OpenSearch is currently in progress.</p> <p>Of course this is only one of the things that DocArray can do, so we encourage you to check out the rest of this readme!</p>"},{"location":"#coming-from-langchain","title":"Coming from Langchain","text":"Click to expand <p>With DocArray, you can connect external data to LLMs through Langchain. DocArray gives you the freedom to establish  flexible document schemas and choose from different backends for document storage. After creating your document index, you can connect it to your Langchain app using DocArrayRetriever.</p> <p>Install Langchain via: </p><pre><code>pip install langchain\n</code></pre> <ol> <li> <p>Define a schema and create documents: </p><pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\n# Define a document schema\nclass MovieDoc(BaseDoc):\ntitle: str\ndescription: str\nyear: int\nembedding: NdArray[1536]\nmovies = [\n{\"title\": \"#1 title\", \"description\": \"#1 description\", \"year\": 1999},\n{\"title\": \"#2 title\", \"description\": \"#2 description\", \"year\": 2001},\n]\n# Embed `description` and create documents\ndocs = DocList[MovieDoc](\nMovieDoc(embedding=embeddings.embed_query(movie[\"description\"]), **movie)\nfor movie in movies\n)\n</code></pre> </li> <li> <p>Initialize a document index using any supported backend: </p><pre><code>from docarray.index import (\nInMemoryExactNNIndex,\nHnswDocumentIndex,\nWeaviateDocumentIndex,\nQdrantDocumentIndex,\nElasticDocIndex,\nRedisDocumentIndex,\n)\n# Select a suitable backend and initialize it with data\ndb = InMemoryExactNNIndex[MovieDoc](docs)\n</code></pre> </li> <li> <p>Finally, initialize a retriever and integrate it into your chain! </p><pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.retrievers import DocArrayRetriever\n# Create a retriever\nretriever = DocArrayRetriever(\nindex=db,\nembeddings=embeddings,\nsearch_field=\"embedding\",\ncontent_field=\"description\",\n)\n# Use the retriever in your chain\nmodel = ChatOpenAI()\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\n</code></pre> </li> </ol> <p>Alternatively, you can use built-in vector stores. Langchain supports two vector stores: DocArrayInMemorySearch and DocArrayHnswSearch.  Both are user-friendly and are best suited to small to medium-sized datasets.</p>"},{"location":"#see-also","title":"See also","text":"<ul> <li>Documentation</li> <li>DocArray&lt;=0.21 documentation</li> <li>Join our Discord server</li> <li>Donation to Linux Foundation AI&amp;Data blog post</li> <li>Roadmap</li> </ul> <p>DocArray is a trademark of LF AI Projects, LLC </p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing-to-docarray","title":"Contributing to DocArray","text":"<p>Thanks for your interest in contributing to DocArray. We're grateful for your initiative! \u2764\ufe0f</p> <p>In this guide, we're going to go through the steps for each kind of contribution, and good and bad examples of what to do. We look forward to your contributions!</p> <ul> <li>\ud83d\udc1e Bugs and Issues</li> <li>\ud83e\udd47 Making Your First Submission</li> <li>\ud83d\udcdd Code style conventions</li> <li>\u2611\ufe0f Naming Conventions</li> <li>\u2795 Adding a dependency</li> <li>\ud83d\udca5 Testing DocArray Locally and on CI</li> <li>\ud83d\udcd6 Contributing Documentation</li> <li>\ud83d\ude4f Thank You</li> </ul> <p></p>"},{"location":"CONTRIBUTING/#bugs-and-issues","title":"\ud83d\udc1e Bugs and issues","text":""},{"location":"CONTRIBUTING/#submitting-issues","title":"Submitting issues","text":"<p>We love to get issue reports. But we love it even more if they're in the right format. For any bugs you encounter, we need you to:</p> <ul> <li>Describe your problem: What exactly is the bug. Be as clear and concise as possible</li> <li>Why do you think it's happening? If you have any insight, here's where to share it</li> </ul> <p>There are also a couple of nice to haves:</p> <ul> <li>Environment: Operating system, DocArray version, python version,...</li> <li>Screenshots: If they're relevant</li> </ul> <p></p>"},{"location":"CONTRIBUTING/#making-your-first-submission","title":"\ud83e\udd47 Making your first submission","text":"<ol> <li>Associate your local git config with your GitHub account. If this is your first time using git you can follow the steps.</li> <li>Fork the DocArray repo and clone onto your computer. </li> <li>Configure git pre-commit hooks. Please follow the steps</li> <li>Create a new branch, for example <code>fix-docarray-typo-1</code>.</li> <li>Work on this branch to do the fix/improvement.</li> <li>Commit the changes with the correct commit style.</li> <li>Make a pull request.</li> <li>Submit your pull request and wait for all checks to pass.</li> <li>Request reviews from one of the code owners.</li> <li>Get a LGTM \ud83d\udc4d and PR gets merged.</li> </ol> <p>Note: If you're just fixing a typo or grammatical issue, you can go straight to a pull request.</p>"},{"location":"CONTRIBUTING/#associate-with-your-github-account","title":"Associate with your GitHub account","text":"<ul> <li>Confirm username and email on your profile page.</li> <li>Set git config on your computer.</li> </ul> <pre><code>git config user.name \"YOUR GITHUB NAME\"\ngit config user.email \"YOUR GITHUB EMAIL\"\n</code></pre> <ul> <li>(Optional) Reset the commit author if you made commits before you set the git config.</li> </ul> <pre><code>git checkout YOUR-WORKED-BRANCH\ngit commit --amend --author=\"YOUR-GITHUB-NAME &lt;YOUR-GITHUB-EMAIL&gt;\" --no-edit\ngit log  # to confirm the change is effective\ngit push --force\n</code></pre>"},{"location":"CONTRIBUTING/#installing-dependencies-using-poetry","title":"Installing dependencies using Poetry","text":"<p>We use Poetry to manage our dependencies.</p> <p>To get stared with DocArray development you should do:</p> <pre><code>pip install poetry\npoetry install --all-extras # this will install all of the dependency needed for development\n</code></pre> <p>This will automatically create a virtual environment and install all the dependency from the <code>lockfile</code> of Poetry.</p> <p>To run your code you need to either activate the environment:</p> <p></p><pre><code>poetry shell\npython XYZ\n</code></pre> or use <code>poetry run</code>: <pre><code>poetry run python scratch.py\npoetry run pip xyz\npoetry run pytest\npoetry run XYZ\n</code></pre>"},{"location":"CONTRIBUTING/#install-pre-commit-hooks","title":"Install pre-commit hooks","text":"<p>In DocArray we use git's pre-commit hooks in order to make sure the code matches our standards of quality and documentation. It's easy to configure it:</p> <ol> <li><code>pip install pre-commit</code></li> <li><code>pre-commit install</code></li> </ol> <p>Now you will be automatically reminded to add docstrings to your code. <code>black</code> will take care that your code will match our style. Note that <code>black</code> will fail your commit but reformat your code, so you just need to add the files again and commit again.</p>"},{"location":"CONTRIBUTING/#restoring-correct-git-blame","title":"Restoring correct git blame","text":"<p>Run <code>git config blame.ignoreRevsFile .github/.git-blame-ignore-revs</code></p>"},{"location":"CONTRIBUTING/#code-style-conventions","title":"\ud83d\udcdd Code style conventions:","text":"<p>Most of our codebase is written in Python. </p>"},{"location":"CONTRIBUTING/#pep-compliance","title":"PEP compliance","text":"<p>We comply to the official PEP: E9, F63, F7, F82 code style and required every contribution to follow it. This is enforced by using ruff in our CI and in our pre-commit hooks.</p>"},{"location":"CONTRIBUTING/#python-version","title":"Python version","text":"<p>DocArray is compatible with Python 3.7 and above, therefore we can't accept contribution that used features from the newest Python versions without ensuring compatibility with python 3.7</p>"},{"location":"CONTRIBUTING/#code-formatting","title":"Code formatting","text":"<p>All of our Python codebase follows formatting standard. We are following the PEP8 standard, and we require that every code contribution is formatted using black with the default configurations. If you have installed the pre-commit hooks the formatting should be automatic on every commit. Moreover, our CI will block contributions that do not respect these conventions.</p>"},{"location":"CONTRIBUTING/#type-hints","title":"Type hints","text":"<p>Python is not a strongly typed programming language. Nevertheless, the use of type hints contributes to a better codebase, especially when reading, reviewing and refactoring. Therefore, we require every contribution to use type hints, unless there are strong reasons for not using them.</p> <p>Further, DocArray is type checked using mypy, and all contributions will have to pass this type check.</p> <p>Note: Example code in the documentation should also follow our code style conventions.</p> <p></p>"},{"location":"CONTRIBUTING/#naming-conventions","title":"\u2611\ufe0f Naming conventions","text":"<p>For branches, commits, and PRs we follow some basic naming conventions:</p> <ul> <li>Be descriptive</li> <li>Use all lower-case</li> <li>Limit punctuation</li> <li>Include one of our specified types</li> <li>Short (under 70 characters is best)</li> <li>In general, follow the Conventional Commit guidelines</li> </ul>"},{"location":"CONTRIBUTING/#specify-the-correct-types","title":"Specify the correct types","text":"<p>Type is an important prefix in PR, commit message. For each branch, commit, or PR, we need you to specify the type to help us keep things organized. For example,</p> <pre><code>feat: add hat wobble\n^--^  ^------------^\n|     |\n|     +-&gt; Summary in present tense.\n|\n+-------&gt; Type: build, ci, chore, docs, feat, fix, refactor, style, or test.\n</code></pre> <ul> <li><code>ci</code>: Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs)</li> <li><code>docs</code>: Documentation only changes</li> <li><code>feat</code>: A new feature</li> <li><code>fix</code>: A bug fix</li> <li><code>perf</code>: A code change that improves performance</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature</li> <li><code>test</code>: Adding missing tests or correcting existing tests</li> <li><code>chore</code>: updating grunt tasks etc.; no production code change</li> </ul>"},{"location":"CONTRIBUTING/#writing-your-commit-message","title":"Writing your commit message","text":"<p>A good commit message helps us track DocArray's development. A pull request with a bad commit message will be rejected automatically in the CI pipeline.</p> <p>Commit messages should stick to our naming conventions outlined above, and use the format <code>type(scope?): subject</code>:</p> <ul> <li><code>type</code> is one of the types above.</li> <li><code>scope</code> is optional, and represents the module your commit is working on.</li> <li><code>subject</code> explains the commit, without an ending period<code>.</code></li> </ul> <p>For example, a commit that fixes a bug in the executor module should be phrased as: <code>fix(executor): fix the bad naming in init function</code></p> <p>Good examples:</p> <pre><code>fix(elastic): fix batching in elastic document store\nfeat: add remote api\n</code></pre> <p>Bad examples: </p> Commit message Feedback <code>doc(101): improved 101 document</code> Should be <code>docs(101)</code> <code>tests(flow): add unit test to document array</code> Should be <code>test(array)</code> <code>DOC(101): Improved 101 Documentation</code> All letters should be in lowercase <code>fix(pea): i fix this issue and this looks really awesome and everything should be working now</code> Too long <code>fix(array):fix array serialization</code> Missing space after <code>:</code> <code>hello: add hello-world</code> Type <code>hello</code> is not allowed"},{"location":"CONTRIBUTING/#dco-and-signed-commit","title":"DCO and signed commit","text":"<p>Commits need to be signed. Indeed, the DocArray repo enforces the Developer Certificate of Origin via the DCO GitHub app.</p> <p>To sign your commits you need to use the <code>-s</code> argument when committing:</p> <pre><code>git commit -S -m 'feat: add a new feature'\n</code></pre>"},{"location":"CONTRIBUTING/#what-if-i-mess-up","title":"What if I mess up?","text":"<p>We all make mistakes. GitHub has a guide on rewriting commit messages so they can adhere to our standards.</p> <p>You can also install commitlint onto your own machine and check your commit message by running:</p> <pre><code>echo \"&lt;commit message&gt;\" | commitlint\n</code></pre>"},{"location":"CONTRIBUTING/#naming-your-pull-request","title":"Naming your pull request","text":"<p>We don't enforce naming of PRs and branches, but we recommend you follow the same style. It can simply be one of your commit messages, just copy/paste it, e.g. <code>fix(readme): improve the readability and move sections</code>.</p> <p></p>"},{"location":"CONTRIBUTING/#adding-a-dependency","title":"\u2795 Adding a dependency","text":"<p>To add a dependency to DocArray, edit <code>pyproject.toml</code> and add your dependency in the <code>[tool.poetry.dependencies]</code> section. Always overwrite poetry default version number (if you used <code>poetry add XYZ</code>): - Pick an appropriate version number. Don't pick the latest version, but rather the oldest that is still compatible. - Use the <code>&gt;=</code> notation instead of <code>~</code> to not lock upper limit.</p> <p>If appropriate, make the dependency optional. For example if it is a new library for a new modality or new vector database.</p> <p><code>mylib = {version = \"&gt;=X.y.z\", optional = true }</code></p> <p>You will also need to add an extra:</p> <pre><code>[tool.poetry.extras]\nnew_modalities = ['mylib']\n</code></pre> <p>Note: Manual editing of <code>pyproject.toml</code> is equivalent  to <code>poetry add \"mylib&gt;=3.9\"  -E new_modalities</code></p> <p></p>"},{"location":"CONTRIBUTING/#testing-docarray-locally-and-on-ci","title":"\ud83d\udca5 Testing DocArray Locally and on CI","text":"<p>Locally you can run the tests via:</p> <pre><code>poetry install --all-extras\npoetry run pip install protobuf==3.19.0\npoetry run pip install tensorflow\npoetry run pytest -v -s tests\n</code></pre> <p>For local development we suggest using the following command to run the tests:</p> <pre><code>poetry run pytest -v -s tests -m 'not tensorflow and not slow and not internet'\n</code></pre> <p>This only take a couple of seconds.</p>"},{"location":"CONTRIBUTING/#test-policy","title":"Test policy","text":"<p>Every contribution that adds or modifies the behavior of a feature must include a suite of tests that validates that the feature works as expected.</p> <p>This allows:</p> <ul> <li>the reviewer to be very confident that the feature does what it is supposed to do before merging it into the code base.</li> <li>the contributors to be sure that they don't break already-merged features when refactoring or modifying the code base.</li> </ul> <p></p>"},{"location":"CONTRIBUTING/#enable-logging","title":"Enable logging","text":"<p>If you need to monitor and debug your code, you can enable docarray logging: </p><pre><code>import logging\nlogging.getLogger('docarray').setLevel(logging.DEBUG)\n</code></pre> <p></p>"},{"location":"CONTRIBUTING/#compiling-protobuf","title":"Compiling protobuf","text":"<p>Some changes to the code base require also changing the <code>.proto</code> files that describe how DocArray serializes to and from protobuf messages.</p> <p>Changes to the <code>.proto</code> definitions should be kept to a minimum, in order to avoid breaking changes.</p> <p>If you do make modification in a <code>.proto</code> file, you need to recompile the protobuf definitions. In order to maintain compatibility with most of the Python ecosystem, in DocArray we compile to two different protobuf versions. Therefore, compilation is a two-step process:</p>"},{"location":"CONTRIBUTING/#step-1-compile-using-protoc-version-319","title":"Step 1: Compile using <code>protoc</code> version 3.19","text":"<ol> <li>Download protoc v3.19 as appropriate for your system, e.g. from here</li> <li>Unzip the file and make <code>protoc</code> executable: <code>chmod +x bin/protoc</code></li> <li>Compile the protobuf definitions in the <code>pb2</code> directory. From <code>docarray/proto/</code> run <code>path/to/v-3-19/bin/protoc -I . --python_out=\"pb2\" docarray.proto</code>.</li> </ol>"},{"location":"CONTRIBUTING/#step-2-compile-using-protoc-version-321","title":"Step 2: Compile using <code>protoc</code> version 3.21","text":"<ol> <li>Download protoc v3.21 as appropriate for your system, e.g. from here</li> <li>Same as above</li> <li>Compile the protobuf definitions in the <code>pb</code> directory. From <code>docarray/proto/</code> run <code>path/to/v-3-21/bin/protoc -I . --python_out=\"pb\" docarray.proto</code>.</li> </ol>"},{"location":"CONTRIBUTING/#contributing-documentation","title":"\ud83d\udcd6 Contributing documentation","text":"<p>Good docs make developers happy, and we love happy developers! We've got a few different types of docs:</p> <ul> <li>General documentation</li> <li>Tutorials/examples</li> <li>Docstrings in Python functions in RST format - generated by Sphinx</li> </ul>"},{"location":"CONTRIBUTING/#documentation-guidelines","title":"Documentation guidelines","text":"<ol> <li>Decide if your page is a user guide or a how-to, like in the <code>Data Types</code> section. Make sure it fits its section.</li> <li>Use \u201cyou\u201d instead of \u201cwe\u201d or \u201cI\u201d. It engages the reader more.</li> <li>Sentence case for headers. (Use https://convertcase.net/ to check)</li> <li>Keep sentences short. If possible, fewer than 13 words.</li> <li>Only use <code>backticks</code> for direct references to code elements.</li> <li>All acronyms should be UPPERCASE (Ex. YAML, JSON, HTTP, SSL).</li> <li>Think about the structure of the page beforehand. Split it into headers before writing the content.</li> <li>If relevant, include a \u201cSee also\u201d section at the end.</li> <li>Link to any existing explanations of the concepts you are using.</li> <li>Example code in the documentation should also follow our code style.</li> <li>Know when to break the rules. Documentation writing is as much art as it is science. Sometimes you will have to deviate from these rules in order to write good documentation.</li> </ol>"},{"location":"CONTRIBUTING/#building-documentation-on-your-local-machine","title":"Building documentation on your local machine","text":""},{"location":"CONTRIBUTING/#steps-to-build-locally","title":"Steps to build locally","text":"<p>First install the documentation dependency </p><pre><code>poetry install --with docs\n</code></pre> <p>Note: if you need to install extra (proto, database, ...) you need to specify those as well.</p> <p>Then build the documentation: </p><pre><code>cd docs\n./makedoc.sh\n</code></pre> <p>The docs website will be generated in <code>site</code>. To serve it, run:</p> <pre><code>cd ..\npoetry run mkdocs serve\n</code></pre> <p>You can now see docs website on http://localhost:8000 on your browser. Note: You may have to change the port from 8000 to something else if you already have a server running on that port.</p>"},{"location":"CONTRIBUTING/#thank-you","title":"\ud83d\ude4f Thank you","text":"<p>Once again, thanks so much for your interest in contributing to DocArray. We're excited to see your contributions!</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#glossary","title":"Glossary","text":"<p>DocArray's scope covers several fields, from AI to web apps. To make it easier to understand, we have created a glossary of terms used in the documentation. </p>"},{"location":"glossary/#concepts","title":"Concepts","text":""},{"location":"glossary/#multimodal-data","title":"<code>Multimodal Data</code>","text":"<p>Multimodal data is data that is composed of different modalities, like image, text, video, audio, etc.</p> <p>Actually, most of the data we have in the world is multimodal, for example:</p> <ul> <li>Newspaper pages are made up of headline, author byline, image, text, etc.</li> <li>YouTube videos are made up of a video, title, description, thumbnail, etc. </li> </ul>"},{"location":"glossary/#multimodal-ai","title":"<code>Multimodal AI</code>","text":"<p>Multimodal AI is the field of AI that focuses on multimodal data. </p> <p>Most of the recent breakthroughs in AI are multimodal AI. </p> <ul> <li>StableDiffusion, Midjourney and DALL-E 2 generate images from text.</li> <li>Whisper generates text from speech.</li> <li>GPT-4 and Flamingo are MLLMs (Multimodal Large Language Models) that understand both text and images.</li> </ul> <p>Many AI labs are focusing on multimodal AI because it can solve a lot of practical problems, and that it might actually be a requirement for strong AI systems (as argued by Yann Lecun in this article where he states that \"a system trained on language alone will never approximate human intelligence.\")</p>"},{"location":"glossary/#generative-ai","title":"<code>Generative AI</code>","text":"<p>Generative AI is also in the epicenter of the latest AI revolution. These tools allow us to generate data.</p> <ul> <li>StableDiffusion, MidJourney, and Dalle-2 generate images from text.</li> <li>LLMs: Large Language Models, (GPT, Flan, LLama, Bloom). These models generate text.</li> </ul>"},{"location":"glossary/#neural-search","title":"<code>Neural Search</code>","text":"<p>Neural search is search powered by neural networks. Unlike traditional keyword-based search methods, neural search understands the context and semantic meaning of a user's query, allowing it to find relevant results even when the exact keywords are not present.</p>"},{"location":"glossary/#vector-database","title":"<code>Vector Database</code>","text":"<p>A vector database is a specialized storage system designed to handle high-dimensional vectors, which are common representations of data in machine learning and AI applications. It enables efficient storage, indexing, and querying of these vectors, and typically supports operations like nearest neighbor search, similarity search, and clustering.</p>"},{"location":"glossary/#tools","title":"Tools","text":""},{"location":"glossary/#jina","title":"<code>Jina</code>","text":"<p>Jina is a framework for building multimodal applications. It relies heavily on DocArray to represent and send data.</p> <p>DocArray was originally part of Jina but it is now a standalone project independent of Jina.</p>"},{"location":"glossary/#pydantic","title":"<code>Pydantic</code>","text":"<p>Pydantic is a Python library that allows data validation using Python type hints.  DocArray relies on Pydantic.</p>"},{"location":"glossary/#fastapi","title":"<code>FastAPI</code>","text":"<p>FastAPI is a Python library that allows building API using Python type hints. It is built on top of Pydantic and nicely extends to DocArray.</p>"},{"location":"glossary/#weaviate","title":"<code>Weaviate</code>","text":"<p>Weaviate is an open-source vector database that is supported in DocArray.</p>"},{"location":"glossary/#qdrant","title":"<code>Qdrant</code>","text":"<p>Qdrant is an open-source vector database that is supported in DocArray.</p>"},{"location":"migration_guide/","title":"Migration guide","text":""},{"location":"migration_guide/#migration-guide","title":"Migration guide","text":"<p>If you are using DocArray v&lt;0.30.0, you will be familiar with its dataclass API.</p> <p>DocArray &gt;=0.30 is that idea, taken seriously. Every document is created through a dataclass-like interface, courtesy of Pydantic.</p> <p>This gives the following advantages:</p> <ul> <li>Flexibility: No need to conform to a fixed set of fields -- your data defines the schema.</li> <li>Multi-modality: Easily store multiple modalities and multiple embeddings in the same document.</li> <li>Language agnostic: At their core, documents are just dictionaries. This makes it easy to create and send them from any language, not just Python.</li> </ul> <p>You may also be familiar with our old Document Stores for vector DB integration. They are now called Document Indexes and offer the following improvements:</p> <ul> <li>Hybrid search: You can now combine vector search with text search, and even filter by arbitrary fields.</li> <li>Production-ready: The new Document Indexes are a much thinner wrapper around the various vector DB libraries, making them more robust and easier to maintain.</li> <li>Increased flexibility: We strive to support any configuration or setting that you could perform through the DB's first-party client.</li> </ul> <p>For now, Document Indexes support Weaviate, Qdrant, ElasticSearch, and HNSWLib, with more to come.</p>"},{"location":"migration_guide/#changes-to-document","title":"Changes to <code>Document</code>","text":"<ul> <li><code>Document</code> has been renamed to <code>BaseDoc</code>.</li> <li><code>BaseDoc</code> cannot be used directly, but instead has to be extended. Therefore, each document class is created through a dataclass-like interface.</li> <li>Following from the previous point, the extending of <code>BaseDoc</code> allows for a flexible schema while the  <code>Document</code> class in v1 only allowed for a fixed schema, with one of <code>tensor</code>, <code>text</code> and <code>blob</code>,  and additional <code>chunks</code> and <code>matches</code>.</li> <li>Due to the added flexibility, one can not know what fields your document class will provide.    Therefore, various methods from v1 (such as <code>.load_uri_to_image_tensor()</code>) are not supported in v2.   Instead, we provide some of those methods on the typing-level. </li> <li>In v2 we have the <code>LegacyDocument</code> class,    which extends <code>BaseDoc</code> while following the same schema as v1's <code>Document</code>.   The <code>LegacyDocument</code> can be useful to start migrating your codebase from v1 to v2.    Nevertheless, the API is not fully compatible with DocArray &lt;=0.21 <code>Document</code>.   Indeed, none of the methods associated with <code>Document</code> are present.    Only the schema of the data is similar.</li> </ul>"},{"location":"migration_guide/#changes-to-documentarray","title":"Changes to <code>DocumentArray</code>","text":""},{"location":"migration_guide/#doclist","title":"DocList","text":"<ul> <li>The <code>DocumentArray</code> class from v1 has been renamed to <code>DocList</code>,  to be more descriptive of its actual functionality, since it is a list of <code>BaseDoc</code>s.</li> </ul>"},{"location":"migration_guide/#docvec","title":"DocVec","text":"<ul> <li>Additionally, we have introduced the class <code>DocVec</code>, which is a column-based representation of <code>BaseDoc</code>s.  Both <code>DocVec</code> and <code>DocList</code> extend <code>AnyDocArray</code>.</li> <li><code>DocVec</code> is a container of Documents appropriate for performing computation that requires batches of data  (ex: matrix multiplication, distance calculation, deep learning forward pass).</li> <li>A <code>DocVec</code> has a similar interface as <code>DocList</code> but with an underlying implementation that is column-based instead of row-based. Each field of the schema of the <code>DocVec</code> (the <code>.doc_type</code> which is a <code>BaseDoc</code>) will be stored in a column. If the field is a tensor, the data from all Documents will be stored as a single <code>doc_vec</code> (torch/np/tf) tensor. If the tensor field is <code>AnyTensor</code> or a Union of tensor types, the <code>.tensor_type</code> will be used to determine the type of the <code>doc_vec</code> column. </li> </ul>"},{"location":"migration_guide/#parameterized-doclist","title":"Parameterized DocList","text":"<ul> <li>With the added flexibility of your document schema, and therefore endless options to design your document schema,  when initializing a <code>DocList</code> it does not necessarily have to be homogenous. </li> <li> <p>If you want a homogenous <code>DocList</code> you can parameterize it at initialization time: </p><pre><code>from docarray import DocList\nfrom docarray.documents import ImageDoc\ndocs = DocList[ImageDoc]()\n</code></pre> </li> <li> <p>Methods like <code>.from_csv()</code> or <code>.pull()</code> only work with parameterized <code>DocList</code>s. </p> </li> </ul>"},{"location":"migration_guide/#access-attributes-of-your-documentarray","title":"Access attributes of your DocumentArray","text":"<ul> <li>In v1 you could access an attribute of all Documents in your DocumentArray by calling the plural  of the attribute's name on your DocArray instance. </li> <li>In v2 you don't have to use the plural, but instead just use the document's attribute name,  since <code>AnyDocArray</code> will expose the same attributes as the <code>BaseDoc</code>s it contains. This will return a list of <code>type(attribute)</code>. However, this works if (and only if) all the <code>BaseDoc</code>s in the <code>AnyDocArray</code> have the same schema. Therfore only this works:</li> </ul> <pre><code>from docarray import BaseDoc, DocList\nclass Book(BaseDoc):\ntitle: str\nauthor: str = None\ndocs = DocList[Book]([Book(title=f'title {i}') for i in range(5)])\nbook_titles = docs.title  # returns a list[str]\n# this would fail\n# docs = DocList([Book(title=f'title {i}') for i in range(5)])\n# book_titles = docs.title\n</code></pre>"},{"location":"migration_guide/#changes-to-document-store","title":"Changes to Document Store","text":"<p>In v2 the <code>Document Store</code> has been renamed to <code>DocIndex</code> and can be used for fast retrieval using vector similarity.  DocArray &gt;=0.30 <code>DocIndex</code> supports:</p> <ul> <li>Weaviate</li> <li>Qdrant</li> <li>ElasticSearch</li> <li>HNSWLib</li> </ul> <p>Instead of creating a <code>DocumentArray</code> instance and setting the <code>storage</code> parameter to a vector database of your choice,  in v2 you can initialize a <code>DocIndex</code> object of your choice, such as: </p> <pre><code>db = HnswDocumentIndex[MyDoc](work_dir='/my/work/dir')\n</code></pre> <p>In contrast, <code>DocStore</code> in v2 can be used for simple long-term storage, such as with AWS S3 buckets or Jina AI Cloud.</p>"},{"location":"API_reference/array/any_da/","title":"AnyDocArray","text":""},{"location":"API_reference/array/any_da/#anydocarray","title":"AnyDocArray","text":""},{"location":"API_reference/array/any_da/#docarray.array.any_array.AnyDocArray","title":"<code>docarray.array.any_array.AnyDocArray</code>","text":"<p>             Bases: <code>Sequence[T_doc]</code>, <code>Generic[T_doc]</code>, <code>AbstractType</code></p> Source code in <code>docarray/array/any_array.py</code> <pre><code>class AnyDocArray(Sequence[T_doc], Generic[T_doc], AbstractType):\ndoc_type: Type[BaseDocWithoutId]\n__typed_da__: Dict[Type['AnyDocArray'], Dict[Type[BaseDocWithoutId], Type]] = {}\ndef __repr__(self):\nreturn f'&lt;{self.__class__.__name__} (length={len(self)})&gt;'\n@classmethod\ndef __class_getitem__(cls, item: Union[Type[BaseDocWithoutId], TypeVar, str]):\nif not isinstance(item, type):\nif sys.version_info &lt; (3, 12):\nreturn Generic.__class_getitem__.__func__(cls, item)  # type: ignore\n# this do nothing that checking that item is valid type var or str\n# Keep the approach in #1147 to be compatible with lower versions of Python.\nelse:\nreturn GenericAlias(cls, item)  # type: ignore\nif not safe_issubclass(item, BaseDocWithoutId):\nraise ValueError(\nf'{cls.__name__}[item] item should be a Document not a {item} '\n)\nif cls not in cls.__typed_da__:\ncls.__typed_da__[cls] = {}\nif item not in cls.__typed_da__[cls]:\n# Promote to global scope so multiprocessing can pickle it\nglobal _DocArrayTyped\nclass _DocArrayTyped(cls):  # type: ignore\ndoc_type: Type[BaseDocWithoutId] = cast(Type[BaseDocWithoutId], item)\nfor field in _DocArrayTyped.doc_type._docarray_fields().keys():\ndef _property_generator(val: str):\ndef _getter(self):\nif getattr(self, '_is_unusable', False):\nraise UnusableObjectError(\nUNUSABLE_ERROR_MSG.format(cls=cls.__name__)\n)\nreturn self._get_data_column(val)\ndef _setter(self, value):\nif getattr(self, '_is_unusable', False):\nraise UnusableObjectError(\nUNUSABLE_ERROR_MSG.format(cls=cls.__name__)\n)\nself._set_data_column(val, value)\n# need docstring for the property\nreturn property(fget=_getter, fset=_setter)\nsetattr(_DocArrayTyped, field, _property_generator(field))\n# this generates property on the fly based on the schema of the item\n# The global scope and qualname need to refer to this class a unique name.\n# Otherwise, creating another _DocArrayTyped will overwrite this one.\nchange_cls_name(\n_DocArrayTyped, f'{cls.__name__}[{item.__name__}]', globals()\n)\ncls.__typed_da__[cls][item] = _DocArrayTyped\nreturn cls.__typed_da__[cls][item]\n@overload\ndef __getitem__(self: T, item: int) -&gt; T_doc:\n...\n@overload\ndef __getitem__(self: T, item: IndexIterType) -&gt; T:\n...\n@abstractmethod\ndef __getitem__(self, item: Union[int, IndexIterType]) -&gt; Union[T_doc, T]:\n...\ndef __getattr__(self, item: str):\n# Needs to be explicitly defined here for the purpose to disable PyCharm's complaints\n# about not detected properties: https://youtrack.jetbrains.com/issue/PY-47991\nreturn super().__getattribute__(item)\n@abstractmethod\ndef _get_data_column(\nself: T,\nfield: str,\n) -&gt; Union[MutableSequence, T, 'AbstractTensor', None]:\n\"\"\"Return all values of the fields from all docs this array contains\n        :param field: name of the fields to extract\n        :return: Returns a list of the field value for each document\n        in the array like container\n        \"\"\"\n...\n@abstractmethod\ndef _set_data_column(\nself: T,\nfield: str,\nvalues: Union[List, T, 'AbstractTensor'],\n):\n\"\"\"Set all Documents in this [`DocList`][docarray.array.doc_list.doc_list.DocList] using the passed values\n        :param field: name of the fields to extract\n        :values: the values to set at the DocList level\n        \"\"\"\n...\n@classmethod\n@abstractmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\"\"\"\n...\n@abstractmethod\ndef to_protobuf(self) -&gt; 'DocListProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\ndef _to_node_protobuf(self) -&gt; 'NodeProto':\n\"\"\"Convert a [`DocList`][docarray.array.doc_list.doc_list.DocList] into a NodeProto\n        protobuf message.\n        This function should be called when a DocList is nested into\n        another Document that need to be converted into a protobuf.\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nreturn NodeProto(doc_array=self.to_protobuf())\n@abstractmethod\ndef traverse_flat(\nself: 'AnyDocArray',\naccess_path: str,\n) -&gt; Union[List[Any], 'AbstractTensor']:\n\"\"\"\n        Return a List of the accessed objects when applying the `access_path`. If this\n        results in a nested list or list of [`DocList`s][docarray.array.doc_list.doc_list.DocList], the list will be flattened\n        on the first level. The access path is a string that consists of attribute\n        names, concatenated and `\"__\"`-separated. It describes the path from the first\n        level to an arbitrary one, e.g. `'content__image__url'`.\n        ```python\n        from docarray import BaseDoc, DocList, Text\n        class Author(BaseDoc):\n            name: str\n        class Book(BaseDoc):\n            author: Author\n            content: Text\n        docs = DocList[Book](\n            Book(author=Author(name='Jenny'), content=Text(text=f'book_{i}'))\n            for i in range(10)  # noqa: E501\n        )\n        books = docs.traverse_flat(access_path='content')  # list of 10 Text objs\n        authors = docs.traverse_flat(access_path='author__name')  # list of 10 strings\n        ```\n        If the resulting list is a nested list, it will be flattened:\n        ```python\n        from docarray import BaseDoc, DocList\n        class Chapter(BaseDoc):\n            content: str\n        class Book(BaseDoc):\n            chapters: DocList[Chapter]\n        docs = DocList[Book](\n            Book(chapters=DocList[Chapter]([Chapter(content='some_content') for _ in range(3)]))\n            for _ in range(10)\n        )\n        chapters = docs.traverse_flat(access_path='chapters')  # list of 30 strings\n        ```\n        If your [`DocList`][docarray.array.doc_list.doc_list.DocList] is in doc_vec mode, and you want to access a field of\n        type `AnyTensor`, the doc_vec tensor will be returned instead of a list:\n        ```python\n        class Image(BaseDoc):\n            tensor: TorchTensor[3, 224, 224]\n        batch = DocList[Image](\n            [\n                Image(\n                    tensor=torch.zeros(3, 224, 224),\n                )\n                for _ in range(2)\n            ]\n        )\n        batch_stacked = batch.stack()\n        tensors = batch_stacked.traverse_flat(\n            access_path='tensor'\n        )  # tensor of shape (2, 3, 224, 224)\n        ```\n        :param access_path: a string that represents the access path (\"__\"-separated).\n        :return: list of the accessed objects, flattened if nested.\n        \"\"\"\n...\n@staticmethod\ndef _traverse(node: Any, access_path: str):\nif access_path:\ncurr_attr, _, path_attrs = access_path.partition('__')\nfrom docarray.array import DocList\nif isinstance(node, (DocList, list)):\nfor n in node:\nx = getattr(n, curr_attr)\nyield from AnyDocArray._traverse(x, path_attrs)\nelse:\nx = getattr(node, curr_attr)\nyield from AnyDocArray._traverse(x, path_attrs)\nelse:\nyield node\n@staticmethod\ndef _flatten_one_level(sequence: List[Any]) -&gt; List[Any]:\nfrom docarray import DocList\nif len(sequence) == 0 or not isinstance(sequence[0], (list, DocList)):\nreturn sequence\nelse:\nreturn [item for sublist in sequence for item in sublist]\ndef summary(self):\n\"\"\"\n        Print a summary of this [`DocList`][docarray.array.doc_list.doc_list.DocList] object and a summary of the schema of its\n        Document type.\n        \"\"\"\nDocArraySummary(self).summary()\ndef _batch(\nself: T,\nbatch_size: int,\nshuffle: bool = False,\nshow_progress: bool = False,\n) -&gt; Generator[T, None, None]:\n\"\"\"\n        Creates a `Generator` that yields [`DocList`][docarray.array.doc_list.doc_list.DocList] of size `batch_size`.\n        Note, that the last batch might be smaller than `batch_size`.\n        :param batch_size: Size of each generated batch.\n        :param shuffle: If set, shuffle the Documents before dividing into minibatches.\n        :param show_progress: if set, show a progress bar when batching documents.\n        :yield: a Generator of [`DocList`][docarray.array.doc_list.doc_list.DocList], each in the length of `batch_size`\n        \"\"\"\nfrom rich.progress import track\nif not (isinstance(batch_size, int) and batch_size &gt; 0):\nraise ValueError(\nf'`batch_size` should be a positive integer, received: {batch_size}'\n)\nN = len(self)\nindices = list(range(N))\nn_batches = int(np.ceil(N / batch_size))\nif shuffle:\nrandom.shuffle(indices)\nfor i in track(\nrange(n_batches),\ndescription='Batching documents',\ndisable=not show_progress,\n):\nyield self[indices[i * batch_size : (i + 1) * batch_size]]\n</code></pre>"},{"location":"API_reference/array/any_da/#docarray.array.any_array.AnyDocArray.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/array/any_da/#docarray.array.any_array.AnyDocArray.summary","title":"<code>summary()</code>","text":"<p>Print a summary of this <code>DocList</code> object and a summary of the schema of its Document type.</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>def summary(self):\n\"\"\"\n    Print a summary of this [`DocList`][docarray.array.doc_list.doc_list.DocList] object and a summary of the schema of its\n    Document type.\n    \"\"\"\nDocArraySummary(self).summary()\n</code></pre>"},{"location":"API_reference/array/any_da/#docarray.array.any_array.AnyDocArray.to_protobuf","title":"<code>to_protobuf()</code>  <code>abstractmethod</code>","text":"<p>Convert DocList into a Protobuf message</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>@abstractmethod\ndef to_protobuf(self) -&gt; 'DocListProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/array/any_da/#docarray.array.any_array.AnyDocArray.traverse_flat","title":"<code>traverse_flat(access_path)</code>  <code>abstractmethod</code>","text":"<p>Return a List of the accessed objects when applying the <code>access_path</code>. If this results in a nested list or list of <code>DocList</code>s, the list will be flattened on the first level. The access path is a string that consists of attribute names, concatenated and <code>\"__\"</code>-separated. It describes the path from the first level to an arbitrary one, e.g. <code>'content__image__url'</code>.</p> <pre><code>from docarray import BaseDoc, DocList, Text\nclass Author(BaseDoc):\nname: str\nclass Book(BaseDoc):\nauthor: Author\ncontent: Text\ndocs = DocList[Book](\nBook(author=Author(name='Jenny'), content=Text(text=f'book_{i}'))\nfor i in range(10)  # noqa: E501\n)\nbooks = docs.traverse_flat(access_path='content')  # list of 10 Text objs\nauthors = docs.traverse_flat(access_path='author__name')  # list of 10 strings\n</code></pre> <p>If the resulting list is a nested list, it will be flattened:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Chapter(BaseDoc):\ncontent: str\nclass Book(BaseDoc):\nchapters: DocList[Chapter]\ndocs = DocList[Book](\nBook(chapters=DocList[Chapter]([Chapter(content='some_content') for _ in range(3)]))\nfor _ in range(10)\n)\nchapters = docs.traverse_flat(access_path='chapters')  # list of 30 strings\n</code></pre> <p>If your <code>DocList</code> is in doc_vec mode, and you want to access a field of type <code>AnyTensor</code>, the doc_vec tensor will be returned instead of a list:</p> <pre><code>class Image(BaseDoc):\ntensor: TorchTensor[3, 224, 224]\nbatch = DocList[Image](\n[\nImage(\ntensor=torch.zeros(3, 224, 224),\n)\nfor _ in range(2)\n]\n)\nbatch_stacked = batch.stack()\ntensors = batch_stacked.traverse_flat(\naccess_path='tensor'\n)  # tensor of shape (2, 3, 224, 224)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>access_path</code> <code>str</code> <p>a string that represents the access path (\"__\"-separated).</p> required <p>Returns:</p> Type Description <code>Union[List[Any], AbstractTensor]</code> <p>list of the accessed objects, flattened if nested.</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>@abstractmethod\ndef traverse_flat(\nself: 'AnyDocArray',\naccess_path: str,\n) -&gt; Union[List[Any], 'AbstractTensor']:\n\"\"\"\n    Return a List of the accessed objects when applying the `access_path`. If this\n    results in a nested list or list of [`DocList`s][docarray.array.doc_list.doc_list.DocList], the list will be flattened\n    on the first level. The access path is a string that consists of attribute\n    names, concatenated and `\"__\"`-separated. It describes the path from the first\n    level to an arbitrary one, e.g. `'content__image__url'`.\n    ```python\n    from docarray import BaseDoc, DocList, Text\n    class Author(BaseDoc):\n        name: str\n    class Book(BaseDoc):\n        author: Author\n        content: Text\n    docs = DocList[Book](\n        Book(author=Author(name='Jenny'), content=Text(text=f'book_{i}'))\n        for i in range(10)  # noqa: E501\n    )\n    books = docs.traverse_flat(access_path='content')  # list of 10 Text objs\n    authors = docs.traverse_flat(access_path='author__name')  # list of 10 strings\n    ```\n    If the resulting list is a nested list, it will be flattened:\n    ```python\n    from docarray import BaseDoc, DocList\n    class Chapter(BaseDoc):\n        content: str\n    class Book(BaseDoc):\n        chapters: DocList[Chapter]\n    docs = DocList[Book](\n        Book(chapters=DocList[Chapter]([Chapter(content='some_content') for _ in range(3)]))\n        for _ in range(10)\n    )\n    chapters = docs.traverse_flat(access_path='chapters')  # list of 30 strings\n    ```\n    If your [`DocList`][docarray.array.doc_list.doc_list.DocList] is in doc_vec mode, and you want to access a field of\n    type `AnyTensor`, the doc_vec tensor will be returned instead of a list:\n    ```python\n    class Image(BaseDoc):\n        tensor: TorchTensor[3, 224, 224]\n    batch = DocList[Image](\n        [\n            Image(\n                tensor=torch.zeros(3, 224, 224),\n            )\n            for _ in range(2)\n        ]\n    )\n    batch_stacked = batch.stack()\n    tensors = batch_stacked.traverse_flat(\n        access_path='tensor'\n    )  # tensor of shape (2, 3, 224, 224)\n    ```\n    :param access_path: a string that represents the access path (\"__\"-separated).\n    :return: list of the accessed objects, flattened if nested.\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/array/da/","title":"DocList","text":""},{"location":"API_reference/array/da/#doclist","title":"DocList","text":""},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList","title":"<code>docarray.array.doc_list.doc_list.DocList</code>","text":"<p>             Bases: <code>ListAdvancedIndexing[T_doc]</code>, <code>PushPullMixin</code>, <code>IOMixinDocList</code>, <code>AnyDocArray[T_doc]</code></p> <p>DocList is a container of Documents.</p> <p>A DocList is a list of Documents of any schema. However, many DocList features are only available if these Documents are homogeneous and follow the same schema. To precise this schema you can use the <code>DocList[MyDocument]</code> syntax where MyDocument is a Document class (i.e. schema). This creates a DocList that can only contains Documents of the type <code>MyDocument</code>.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray, ImageUrl\nfrom typing import Optional\nclass Image(BaseDoc):\ntensor: Optional[NdArray[100]] = None\nurl: ImageUrl\ndocs = DocList[Image](\nImage(url='http://url.com/foo.png') for _ in range(10)\n)  # noqa: E510\n# If your DocList is homogeneous (i.e. follows the same schema), you can access\n# fields at the DocList level (for example `docs.tensor` or `docs.url`).\nprint(docs.url)\n# [ImageUrl('http://url.com/foo.png', host_type='domain'), ...]\n# You can also set fields, with `docs.tensor = np.random.random([10, 100])`:\nimport numpy as np\ndocs.tensor = np.random.random([10, 100])\nprint(docs.tensor)\n# [NdArray([0.11299577, 0.47206767, 0.481723  , 0.34754724, 0.15016037,\n#          0.88861321, 0.88317666, 0.93845579, 0.60486676, ... ]), ...]\n# You can index into a DocList like a numpy doc_list or torch tensor:\ndocs[0]  # index by position\ndocs[0:5:2]  # index by slice\ndocs[[0, 2, 3]]  # index by list of indices\ndocs[True, False, True, True, ...]  # index by boolean mask\n# You can delete items from a DocList like a Python List\ndel docs[0]  # remove first element from DocList\ndel docs[0:5]  # remove elements for 0 to 5 from DocList\n</code></pre> <p>Note</p> <p>If the DocList is homogeneous and its schema contains nested BaseDoc (i.e, BaseDoc inside a BaseDoc) where the nested Document is <code>Optional</code>, calling <code>docs.nested_doc</code> will return a List of the nested BaseDoc instead of DocList. This is because the nested field could be None and therefore could not fit into a DocList.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Optional[Iterable[T_doc]]</code> <p>iterable of Document</p> <code>None</code> Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>class DocList(\nListAdvancedIndexing[T_doc],\nPushPullMixin,\nIOMixinDocList,\nAnyDocArray[T_doc],\n):\n\"\"\"\n     DocList is a container of Documents.\n    A DocList is a list of Documents of any schema. However, many\n    DocList features are only available if these Documents are\n    homogeneous and follow the same schema. To precise this schema you can use\n    the `DocList[MyDocument]` syntax where MyDocument is a Document class\n    (i.e. schema). This creates a DocList that can only contains Documents of\n    the type `MyDocument`.\n    ```python\n    from docarray import BaseDoc, DocList\n    from docarray.typing import NdArray, ImageUrl\n    from typing import Optional\n    class Image(BaseDoc):\n        tensor: Optional[NdArray[100]] = None\n        url: ImageUrl\n    docs = DocList[Image](\n        Image(url='http://url.com/foo.png') for _ in range(10)\n    )  # noqa: E510\n    # If your DocList is homogeneous (i.e. follows the same schema), you can access\n    # fields at the DocList level (for example `docs.tensor` or `docs.url`).\n    print(docs.url)\n    # [ImageUrl('http://url.com/foo.png', host_type='domain'), ...]\n    # You can also set fields, with `docs.tensor = np.random.random([10, 100])`:\n    import numpy as np\n    docs.tensor = np.random.random([10, 100])\n    print(docs.tensor)\n    # [NdArray([0.11299577, 0.47206767, 0.481723  , 0.34754724, 0.15016037,\n    #          0.88861321, 0.88317666, 0.93845579, 0.60486676, ... ]), ...]\n    # You can index into a DocList like a numpy doc_list or torch tensor:\n    docs[0]  # index by position\n    docs[0:5:2]  # index by slice\n    docs[[0, 2, 3]]  # index by list of indices\n    docs[True, False, True, True, ...]  # index by boolean mask\n    # You can delete items from a DocList like a Python List\n    del docs[0]  # remove first element from DocList\n    del docs[0:5]  # remove elements for 0 to 5 from DocList\n    ```\n    !!! note\n        If the DocList is homogeneous and its schema contains nested BaseDoc\n        (i.e, BaseDoc inside a BaseDoc) where the nested Document is `Optional`, calling\n        `docs.nested_doc` will return a List of the nested BaseDoc instead of DocList.\n        This is because the nested field could be None and therefore could not fit into\n        a DocList.\n    :param docs: iterable of Document\n    \"\"\"\ndoc_type: Type[BaseDocWithoutId] = AnyDoc\ndef __init__(\nself,\ndocs: Optional[Iterable[T_doc]] = None,\nvalidate_input_docs: bool = True,\n):\nif validate_input_docs:\ndocs = self._validate_docs(docs) if docs else []\nelse:\ndocs = docs if docs else []\nsuper().__init__(docs)\n@classmethod\ndef construct(\ncls: Type[T],\ndocs: Sequence[T_doc],\n) -&gt; T:\n\"\"\"\n        Create a `DocList` without validation any data. The data must come from a\n        trusted source\n        :param docs: a Sequence (list) of Document with the same schema\n        :return: a `DocList` object\n        \"\"\"\nreturn cls(docs, False)\ndef __eq__(self, other: Any) -&gt; bool:\nif self.__len__() != other.__len__():\nreturn False\nfor doc_self, doc_other in zip(self, other):\nif doc_self != doc_other:\nreturn False\nreturn True\ndef _validate_docs(self, docs: Iterable[T_doc]) -&gt; Iterable[T_doc]:\n\"\"\"\n        Validate if an Iterable of Document are compatible with this `DocList`\n        \"\"\"\nfor doc in docs:\nyield self._validate_one_doc(doc)\ndef _validate_one_doc(self, doc: T_doc) -&gt; T_doc:\n\"\"\"Validate if a Document is compatible with this `DocList`\"\"\"\nif not safe_issubclass(self.doc_type, AnyDoc) and not isinstance(\ndoc, self.doc_type\n):\nraise ValueError(f'{doc} is not a {self.doc_type}')\nreturn doc\ndef __bytes__(self) -&gt; bytes:\nwith io.BytesIO() as bf:\nself._write_bytes(bf=bf)\nreturn bf.getvalue()\ndef append(self, doc: T_doc):\n\"\"\"\n        Append a Document to the `DocList`. The Document must be from the same class\n        as the `.doc_type` of this `DocList` otherwise it will fail.\n        :param doc: A Document\n        \"\"\"\nreturn super().append(self._validate_one_doc(doc))\ndef extend(self, docs: Iterable[T_doc]):\n\"\"\"\n        Extend a `DocList` with an Iterable of Document. The Documents must be from\n        the same class as the `.doc_type` of this `DocList` otherwise it will\n        fail.\n        :param docs: Iterable of Documents\n        \"\"\"\nit: Iterable[T_doc] = list()\nif self is docs:\n# see https://github.com/docarray/docarray/issues/1489\nit = list(docs)\nelse:\nit = self._validate_docs(docs)\nreturn super().extend(it)\ndef insert(self, i: SupportsIndex, doc: T_doc):\n\"\"\"\n        Insert a Document to the `DocList`. The Document must be from the same\n        class as the doc_type of this `DocList` otherwise it will fail.\n        :param i: index to insert\n        :param doc: A Document\n        \"\"\"\nsuper().insert(i, self._validate_one_doc(doc))\ndef _get_data_column(\nself: T,\nfield: str,\n) -&gt; Union[MutableSequence, T, 'TorchTensor', 'NdArray']:\n\"\"\"Return all v  @classmethod\n          def __class_getitem__(cls, item: Union[Type[BaseDoc], TypeVar, str]):alues of the fields from all docs this doc_list contains\n        @classmethod\n          def __class_getitem__(cls, item: Union[Type[BaseDoc], TypeVar, str]):\n              :param field: name of the fields to extract\n              :return: Returns a list of the field value for each document\n              in the doc_list like container\n        \"\"\"\nfield_type = self.__class__.doc_type._get_field_annotation(field)\nfield_info = self.__class__.doc_type._docarray_fields()[field]\nis_field_required = (\nfield_info.is_required() if is_pydantic_v2 else field_info.required\n)\nif (\nnot is_union_type(field_type)\nand is_field_required\nand isinstance(field_type, type)\nand safe_issubclass(field_type, BaseDocWithoutId)\n):\n# calling __class_getitem__ ourselves is a hack otherwise mypy complain\n# most likely a bug in mypy though\n# bug reported here https://github.com/python/mypy/issues/14111\nreturn DocList.__class_getitem__(field_type)(\n(getattr(doc, field) for doc in self),\n)\nelse:\nreturn [getattr(doc, field) for doc in self]\ndef _set_data_column(\nself: T,\nfield: str,\nvalues: Union[List, T, 'AbstractTensor'],\n):\n\"\"\"Set all Documents in this `DocList` using the passed values\n        :param field: name of the fields to set\n        :values: the values to set at the `DocList` level\n        \"\"\"\n...\nfor doc, value in zip(self, values):\nsetattr(doc, field, value)\ndef to_doc_vec(\nself,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; 'DocVec':\n\"\"\"\n        Convert the `DocList` into a `DocVec`. `Self` cannot be used\n        afterward\n        :param tensor_type: Tensor Class used to wrap the doc_vec tensors. This is useful\n        if the BaseDoc has some undefined tensor type like AnyTensor or Union of NdArray and TorchTensor\n        :return: A `DocVec` of the same document type as self\n        \"\"\"\nfrom docarray.array.doc_vec.doc_vec import DocVec\nreturn DocVec.__class_getitem__(self.doc_type)(self, tensor_type=tensor_type)\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, Iterable[BaseDocWithoutId]],\n):\nfrom docarray.array.doc_vec.doc_vec import DocVec\nif isinstance(value, cls):\nreturn value\nelif isinstance(value, DocVec):\nif (\nsafe_issubclass(value.doc_type, cls.doc_type)\nor value.doc_type == cls.doc_type\n):\nreturn cast(T, value.to_doc_list())\nelse:\nraise ValueError(\nf'DocList[value.doc_type] is not compatible with {cls}'\n)\nelif isinstance(value, cls):\nreturn cls(value)\nelif isinstance(value, Iterable):\ndocs = []\nfor doc in value:\ndocs.append(parse_obj_as(cls.doc_type, doc))\nreturn cls(docs)\nelse:\nraise TypeError(f'Expecting an Iterable of {cls.doc_type}')\ndef traverse_flat(\nself: 'DocList',\naccess_path: str,\n) -&gt; List[Any]:\nnodes = list(AnyDocArray._traverse(node=self, access_path=access_path))\nflattened = AnyDocArray._flatten_one_level(nodes)\nreturn flattened\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n        :param pb_msg: The protobuf message from where to construct the `DocList`\n        \"\"\"\nreturn super().from_protobuf(pb_msg)\n@classmethod\ndef _get_proto_class(cls: Type[T]):\nfrom docarray.proto import DocListProto\nreturn DocListProto\n@overload\ndef __getitem__(self, item: SupportsIndex) -&gt; T_doc:\n...\n@overload\ndef __getitem__(self: T, item: IndexIterType) -&gt; T:\n...\ndef __getitem__(self, item):\nreturn super().__getitem__(item)\n@classmethod\ndef __class_getitem__(cls, item: Union[Type[BaseDocWithoutId], TypeVar, str]):\nif cls.doc_type != AnyDoc:\nraise TypeError(f'{cls} object is not subscriptable')\nif isinstance(item, type) and safe_issubclass(item, BaseDocWithoutId):\nreturn AnyDocArray.__class_getitem__.__func__(cls, item)  # type: ignore\nif (\nisinstance(item, object)\nand not is_typevar(item)\nand not isinstance(item, str)\nand item is not Any\n):\nraise TypeError('Expecting a type, got object instead')\nreturn super().__class_getitem__(item)\ndef __repr__(self):\nreturn AnyDocArray.__repr__(self)  # type: ignore\nif is_pydantic_v2:\n@classmethod\ndef __get_pydantic_core_schema__(\ncls, _source_type: Any, _handler: GetCoreSchemaHandler\n) -&gt; core_schema.CoreSchema:\nreturn core_schema.general_plain_validator_function(\ncls.validate,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.append","title":"<code>append(doc)</code>","text":"<p>Append a Document to the <code>DocList</code>. The Document must be from the same class as the <code>.doc_type</code> of this <code>DocList</code> otherwise it will fail.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>T_doc</code> <p>A Document</p> required Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>def append(self, doc: T_doc):\n\"\"\"\n    Append a Document to the `DocList`. The Document must be from the same class\n    as the `.doc_type` of this `DocList` otherwise it will fail.\n    :param doc: A Document\n    \"\"\"\nreturn super().append(self._validate_one_doc(doc))\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.construct","title":"<code>construct(docs)</code>  <code>classmethod</code>","text":"<p>Create a <code>DocList</code> without validation any data. The data must come from a trusted source</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Sequence[T_doc]</code> <p>a Sequence (list) of Document with the same schema</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>@classmethod\ndef construct(\ncls: Type[T],\ndocs: Sequence[T_doc],\n) -&gt; T:\n\"\"\"\n    Create a `DocList` without validation any data. The data must come from a\n    trusted source\n    :param docs: a Sequence (list) of Document with the same schema\n    :return: a `DocList` object\n    \"\"\"\nreturn cls(docs, False)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.extend","title":"<code>extend(docs)</code>","text":"<p>Extend a <code>DocList</code> with an Iterable of Document. The Documents must be from the same class as the <code>.doc_type</code> of this <code>DocList</code> otherwise it will fail.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterable[T_doc]</code> <p>Iterable of Documents</p> required Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>def extend(self, docs: Iterable[T_doc]):\n\"\"\"\n    Extend a `DocList` with an Iterable of Document. The Documents must be from\n    the same class as the `.doc_type` of this `DocList` otherwise it will\n    fail.\n    :param docs: Iterable of Documents\n    \"\"\"\nit: Iterable[T_doc] = list()\nif self is docs:\n# see https://github.com/docarray/docarray/issues/1489\nit = list(docs)\nelse:\nit = self._validate_docs(docs)\nreturn super().extend(it)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_base64","title":"<code>from_base64(data, protocol='protobuf-array', compress=None, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Deserialize base64 strings into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Base64 string to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocList`.\n    :param data: Base64 string to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the deserialized `DocList`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_bytes","title":"<code>from_bytes(data, protocol='protobuf-array', compress=None, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Deserialize bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from which to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n    :param data: Bytes from which to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the deserialized `DocList`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_csv","title":"<code>from_csv(file_path, encoding='utf-8', dialect='excel')</code>  <code>classmethod</code>","text":"<p>Load a DocList from a csv file following the schema defined in the <code>.doc_type</code> attribute. Every row of the csv file will be mapped to one document in the doc_list. The column names (defined in the first row) have to match the field names of the Document type. For nested fields use \"__\"-separated access paths, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to csv file to load DocList from.</p> required <code>encoding</code> <code>str</code> <p>encoding used to read the csv file. Defaults to 'utf-8'.</p> <code>'utf-8'</code> <code>dialect</code> <code>Union[str, Dialect]</code> <p>defines separator and how to handle whitespaces etc. Can be a <code>csv.Dialect</code> instance or one string of: <code>'excel'</code> (for comma separated values), <code>'excel-tab'</code> (for tab separated values), <code>'unix'</code> (for csv file generated on UNIX systems).</p> <code>'excel'</code> <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, csv.Dialect] = 'excel',\n) -&gt; 'T':\n\"\"\"\n    Load a DocList from a csv file following the schema defined in the\n    [`.doc_type`][docarray.DocList] attribute.\n    Every row of the csv file will be mapped to one document in the doc_list.\n    The column names (defined in the first row) have to match the field names\n    of the Document type.\n    For nested fields use \"__\"-separated access paths, such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    :param file_path: path to csv file to load DocList from.\n    :param encoding: encoding used to read the csv file. Defaults to 'utf-8'.\n    :param dialect: defines separator and how to handle whitespaces etc.\n        Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n        instance or one string of:\n        `'excel'` (for comma separated values),\n        `'excel-tab'` (for tab separated values),\n        `'unix'` (for csv file generated on UNIX systems).\n    :return: `DocList` object\n    \"\"\"\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\nif file_path.startswith('http'):\nimport urllib.request\nwith urllib.request.urlopen(file_path) as f:\nfile = StringIO(f.read().decode(encoding))\nreturn cls._from_csv_file(file, dialect)\nelse:\nwith open(file_path, 'r', encoding=encoding) as fp:\nreturn cls._from_csv_file(fp, dialect)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_dataframe","title":"<code>from_dataframe(df)</code>  <code>classmethod</code>","text":"<p>Load a <code>DocList</code> from a <code>pandas.DataFrame</code> following the schema defined in the <code>.doc_type</code> attribute. Every row of the dataframe will be mapped to one Document in the doc_list. The column names of the dataframe have to match the field names of the Document type. For nested fields use \"__\"-separated access paths as column names, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <pre><code>import pandas as pd\nfrom docarray import BaseDoc, DocList\nclass Person(BaseDoc):\nname: str\nfollower: int\ndf = pd.DataFrame(\ndata=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n)\ndocs = DocList[Person].from_dataframe(df)\nassert docs.name == ['Maria', 'Jake']\nassert docs.follower == [12345, 54321]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p><code>pandas.DataFrame</code> to extract Document's information from</p> required <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> where each Document contains the information of one corresponding row of the <code>pandas.DataFrame</code>.</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_dataframe(cls: Type['T'], df: 'pd.DataFrame') -&gt; 'T':\n\"\"\"\n    Load a `DocList` from a `pandas.DataFrame` following the schema\n    defined in the [`.doc_type`][docarray.DocList] attribute.\n    Every row of the dataframe will be mapped to one Document in the doc_list.\n    The column names of the dataframe have to match the field names of the\n    Document type.\n    For nested fields use \"__\"-separated access paths as column names,\n    such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    ---\n    ```python\n    import pandas as pd\n    from docarray import BaseDoc, DocList\n    class Person(BaseDoc):\n        name: str\n        follower: int\n    df = pd.DataFrame(\n        data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n    )\n    docs = DocList[Person].from_dataframe(df)\n    assert docs.name == ['Maria', 'Jake']\n    assert docs.follower == [12345, 54321]\n    ```\n    ---\n    :param df: `pandas.DataFrame` to extract Document's information from\n    :return: `DocList` where each Document contains the information of one\n        corresponding row of the `pandas.DataFrame`.\n    \"\"\"\nfrom docarray import DocList\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\ndoc_type = cls.doc_type\ndocs = DocList.__class_getitem__(doc_type)()\nfield_names = df.columns.tolist()\nif field_names is None or len(field_names) == 0:\nraise TypeError(\"No field names are given.\")\nvalid_paths = _all_access_paths_valid(\ndoc_type=doc_type, access_paths=field_names\n)\nif not all(valid_paths):\nraise ValueError(\nf'Column names do not match the schema of the DocList\\'s '\nf'document type ({cls.doc_type.__name__}): '\nf'{list(compress(field_names, [not v for v in valid_paths]))}'\n)\nfor row in df.itertuples():\naccess_path2val = row._asdict()\naccess_path2val.pop('index', None)\ndoc_dict = _access_path_dict_to_nested_dict(access_path2val)\ndocs.append(doc_type.parse_obj(doc_dict))\nreturn docs\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_json","title":"<code>from_json(file)</code>  <code>classmethod</code>","text":"<p>Deserialize JSON strings or bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, bytearray]</code> <p>JSON object from where to deserialize a <code>DocList</code></p> required <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n    :param file: JSON object from where to deserialize a `DocList`\n    :return: the deserialized `DocList`\n    \"\"\"\njson_docs = orjson.loads(file)\nreturn cls([cls.doc_type(**v) for v in json_docs])\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocListProto</code> <p>The protobuf message from where to construct the <code>DocList</code></p> required Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: The protobuf message from where to construct the `DocList`\n    \"\"\"\nreturn super().from_protobuf(pb_msg)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.get_pushpull_backend","title":"<code>get_pushpull_backend(protocol)</code>  <code>classmethod</code>","text":"<p>Get the backend for the given protocol.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>PUSH_PULL_PROTOCOL</code> <p>the protocol to use, e.g. 'file', 's3'</p> required <p>Returns:</p> Type Description <code>Type[AbstractDocStore]</code> <p>the backend class</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef get_pushpull_backend(\ncls: Type[SelfPushPullMixin], protocol: PUSH_PULL_PROTOCOL\n) -&gt; Type['AbstractDocStore']:\n\"\"\"\n    Get the backend for the given protocol.\n    :param protocol: the protocol to use, e.g. 'file', 's3'\n    :return: the backend class\n    \"\"\"\nif protocol in cls.__backends__:\nreturn cls.__backends__[protocol]\nif protocol == 'file':\nfrom docarray.store.file import FileDocStore\ncls.__backends__[protocol] = FileDocStore\nlogging.debug('Loaded Local Filesystem backend')\nelif protocol == 's3':\nfrom docarray.store.s3 import S3DocStore\ncls.__backends__[protocol] = S3DocStore\nlogging.debug('Loaded S3 backend')\nelse:\nraise NotImplementedError(f'protocol {protocol} not supported')\nreturn cls.__backends__[protocol]\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.insert","title":"<code>insert(i, doc)</code>","text":"<p>Insert a Document to the <code>DocList</code>. The Document must be from the same class as the doc_type of this <code>DocList</code> otherwise it will fail.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>SupportsIndex</code> <p>index to insert</p> required <code>doc</code> <code>T_doc</code> <p>A Document</p> required Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>def insert(self, i: SupportsIndex, doc: T_doc):\n\"\"\"\n    Insert a Document to the `DocList`. The Document must be from the same\n    class as the doc_type of this `DocList` otherwise it will fail.\n    :param i: index to insert\n    :param doc: A Document\n    \"\"\"\nsuper().insert(i, self._validate_one_doc(doc))\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.load_binary","title":"<code>load_binary(file, protocol='protobuf-array', compress=None, show_progress=False, streaming=False)</code>  <code>classmethod</code>","text":"<p>Load doc_list elements from a compressed binary file.</p> <p>In case protocol is pickle the <code>Documents</code> are streamed from disk to save memory usage</p> <p>Note</p> <p>If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions. This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a string interpolation of the respective <code>protocol</code> and <code>compress</code> methods. For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be loaded assuming <code>protocol=protobuf</code> and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, Path, BufferedReader, _LazyRequestReader]</code> <p>File or filename or serialized bytes where the data is stored.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>streaming</code> <code>bool</code> <p>if <code>True</code> returns a generator over <code>Document</code> objects.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[T, Generator[T_doc, None, None]]</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_list elements from a compressed binary file.\n    In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n    !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename or serialized bytes where the data is stored.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param streaming: if `True` returns a generator over `Document` objects.\n    :return: a `DocList` object\n    \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx, load_protocol, load_compress, show_progress\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.pull","title":"<code>pull(url, show_progress=False, local_cache=True)</code>  <code>classmethod</code>","text":"<p>Pull a <code>DocList</code> from the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> <code>False</code> <code>local_cache</code> <code>bool</code> <p>store the downloaded <code>DocList</code> to local folder</p> <code>True</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef pull(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = True,\n) -&gt; 'DocList':\n\"\"\"Pull a `DocList` from the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded `DocList` to local folder\n    :return: a `DocList` object\n    \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull(\ncls, name, show_progress, local_cache  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.pull_stream","title":"<code>pull_stream(url, show_progress=False, local_cache=False)</code>  <code>classmethod</code>","text":"<p>Pull a stream of Documents from the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> <code>False</code> <code>local_cache</code> <code>bool</code> <p>store the downloaded <code>DocList</code> to local folder</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[BaseDoc]</code> <p>Iterator of Documents</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef pull_stream(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = False,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded `DocList` to local folder\n    :return: Iterator of Documents\n    \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling Document stream from {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull_stream(\ncls, name, show_progress, local_cache  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.push","title":"<code>push(url, show_progress=False, **kwargs)</code>","text":"<p>Push this <code>DocList</code> object to the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>def push(\nself,\nurl: str,\nshow_progress: bool = False,\n**kwargs,\n) -&gt; Dict:\n\"\"\"Push this `DocList` object to the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nlogging.info(f'Pushing {len(self)} docs to {url}')\nprotocol, name = self.__class__.resolve_url(url)\nreturn self.__class__.get_pushpull_backend(protocol).push(\nself, name, show_progress  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.push_stream","title":"<code>push_stream(docs, url, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Push a stream of documents to the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[BaseDoc]</code> <p>a stream of documents</p> required <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef push_stream(\ncls: Type[SelfPushPullMixin],\ndocs: Iterator['BaseDoc'],\nurl: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified url.\n    :param docs: a stream of documents\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nlogging.info(f'Pushing stream to {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).push_stream(docs, name, show_progress)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.resolve_url","title":"<code>resolve_url(url)</code>  <code>staticmethod</code>","text":"<p>Resolve the URL to the correct protocol and name.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url to resolve</p> required Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@staticmethod\ndef resolve_url(url: str) -&gt; Tuple[PUSH_PULL_PROTOCOL, str]:\n\"\"\"Resolve the URL to the correct protocol and name.\n    :param url: url to resolve\n    \"\"\"\nprotocol, name = url.split('://', 2)\nif protocol in SUPPORTED_PUSH_PULL_PROTOCOLS:\nprotocol = cast(PUSH_PULL_PROTOCOL, protocol)\nreturn protocol, name\nelse:\nraise ValueError(f'Unsupported protocol {protocol}')\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.save_binary","title":"<code>save_binary(file, protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Save DocList into a binary file.</p> <p>It will use the protocol to pick how to save the DocList. If used <code>picke-doc_list</code> and <code>protobuf-array</code> the DocList will be stored and compressed at complete level using <code>pickle</code> or <code>protobuf</code>. When using <code>protobuf</code> or <code>pickle</code> as protocol each Document in DocList will be stored individually and this would make it available for streaming.</p> <p>!!! note     If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions.     This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a     string interpolation of the respective <code>protocol</code> and <code>compress</code> methods.     For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be created using <code>protocol=protobuf</code>     and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>File or filename to which the data is saved.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def save_binary(\nself,\nfile: Union[str, pathlib.Path],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\n\"\"\"Save DocList into a binary file.\n    It will use the protocol to pick how to save the DocList.\n    If used `picke-doc_list` and `protobuf-array` the DocList will be stored\n    and compressed at complete level using `pickle` or `protobuf`.\n    When using `protobuf` or `pickle` as protocol each Document in DocList\n    will be stored individually and this would make it available for streaming.\n     !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be created using `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename to which the data is saved.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    \"\"\"\nif isinstance(file, io.BufferedWriter):\nfile_ctx = nullcontext(file)\nelse:\n_protocol, _compress = _protocol_and_compress_from_file_path(file)\nif _protocol is not None:\nprotocol = _protocol\nif _compress is not None:\ncompress = _compress\nfile_ctx = open(file, 'wb')\nself.to_bytes(\nprotocol=protocol,\ncompress=compress,\nfile_ctx=file_ctx,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.summary","title":"<code>summary()</code>","text":"<p>Print a summary of this <code>DocList</code> object and a summary of the schema of its Document type.</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>def summary(self):\n\"\"\"\n    Print a summary of this [`DocList`][docarray.array.doc_list.doc_list.DocList] object and a summary of the schema of its\n    Document type.\n    \"\"\"\nDocArraySummary(self).summary()\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_base64","title":"<code>to_base64(protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Serialize itself into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_base64(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; str:\n\"\"\"Serialize itself into base64 encoded string.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\ncompress=compress,\nprotocol=protocol,\nshow_progress=show_progress,\n)\nreturn base64.b64encode(bf.getvalue()).decode('utf-8')\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_bytes","title":"<code>to_bytes(protocol='protobuf-array', compress=None, file_ctx=None, show_progress=False)</code>","text":"<p>Serialize itself into <code>bytes</code>.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between : <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>file_ctx</code> <code>Optional[BinaryIO]</code> <p>File or filename or serialized bytes where the data is stored.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_bytes(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nfile_ctx: Optional[BinaryIO] = None,\nshow_progress: bool = False,\n) -&gt; Optional[bytes]:\n\"\"\"Serialize itself into `bytes`.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between : `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param file_ctx: File or filename or serialized bytes where the data is stored.\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith file_ctx or io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\nif isinstance(bf, io.BytesIO):\nreturn bf.getvalue()\nreturn None\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_csv","title":"<code>to_csv(file_path, dialect='excel')</code>","text":"<p>Save a <code>DocList</code> to a csv file. The field names will be stored in the first row. Each row corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a csv file.</p> required <code>dialect</code> <code>Union[str, Dialect]</code> <p>defines separator and how to handle whitespaces etc. Can be a <code>csv.Dialect</code> instance or one string of: <code>'excel'</code> (for comma separated values), <code>'excel-tab'</code> (for tab separated values), <code>'unix'</code> (for csv file generated on UNIX systems).</p> <code>'excel'</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_csv(\nself, file_path: str, dialect: Union[str, csv.Dialect] = 'excel'\n) -&gt; None:\n\"\"\"\n    Save a `DocList` to a csv file.\n    The field names will be stored in the first row. Each row corresponds to the\n    information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :param file_path: path to a csv file.\n    :param dialect: defines separator and how to handle whitespaces etc.\n        Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n        instance or one string of:\n        `'excel'` (for comma separated values),\n        `'excel-tab'` (for tab separated values),\n        `'unix'` (for csv file generated on UNIX systems).\n    \"\"\"\nif self.doc_type == AnyDoc or self.doc_type == BaseDoc:\nraise TypeError(\nf'{type(self)} must be homogeneous to be converted to a csv.'\n'There is no document schema defined. '\nf'Please specify the {type(self)}\\'s Document type using `{type(self)}[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\nwith open(file_path, 'w') as csv_file:\nwriter = csv.DictWriter(csv_file, fieldnames=fields, dialect=dialect)\nwriter.writeheader()\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\nwriter.writerow(doc_dict)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Save a DocList to a <code>pandas.DataFrame</code>. The field names will be stored as column names. Each row of the dataframe corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas.DataFrame</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_dataframe(self) -&gt; 'pd.DataFrame':\n\"\"\"\n    Save a DocList to a `pandas.DataFrame`.\n    The field names will be stored as column names. Each row of the dataframe corresponds\n    to the information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :return: `pandas.DataFrame`\n    \"\"\"\nif TYPE_CHECKING:\nimport pandas as pd\nelse:\npd = import_library('pandas', raise_error=True)\nif self.doc_type == AnyDoc:\nraise TypeError(\n'DocList must be homogeneous to be converted to a DataFrame.'\n'There is no document schema defined. '\n'Please specify the DocList\\'s Document type using `DocList[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\ndf = pd.DataFrame(columns=fields)\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\ndoc_dict = {k: [v] for k, v in doc_dict.items()}\ndf = pd.concat([df, pd.DataFrame.from_dict(doc_dict)], ignore_index=True)\nreturn df\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_doc_vec","title":"<code>to_doc_vec(tensor_type=NdArray)</code>","text":"<p>Convert the <code>DocList</code> into a <code>DocVec</code>. <code>Self</code> cannot be used afterward</p> <p>Parameters:</p> Name Type Description Default <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>Tensor Class used to wrap the doc_vec tensors. This is useful if the BaseDoc has some undefined tensor type like AnyTensor or Union of NdArray and TorchTensor</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>DocVec</code> <p>A <code>DocVec</code> of the same document type as self</p> Source code in <code>docarray/array/doc_list/doc_list.py</code> <pre><code>def to_doc_vec(\nself,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; 'DocVec':\n\"\"\"\n    Convert the `DocList` into a `DocVec`. `Self` cannot be used\n    afterward\n    :param tensor_type: Tensor Class used to wrap the doc_vec tensors. This is useful\n    if the BaseDoc has some undefined tensor type like AnyTensor or Union of NdArray and TorchTensor\n    :return: A `DocVec` of the same document type as self\n    \"\"\"\nfrom docarray.array.doc_vec.doc_vec import DocVec\nreturn DocVec.__class_getitem__(self.doc_type)(self, tensor_type=tensor_type)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_json","title":"<code>to_json()</code>","text":"<p>Convert the object into JSON bytes. Can be loaded via <code>.from_json</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON serialization of <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Convert the object into JSON bytes. Can be loaded via `.from_json`.\n    :return: JSON serialization of `DocList`\n    \"\"\"\nreturn orjson_dumps(self).decode('UTF-8')\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.doc_list.DocList.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert <code>DocList</code> into a Protobuf message</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_protobuf(self) -&gt; 'DocListProto':\n\"\"\"Convert `DocList` into a Protobuf message\"\"\"\nfrom docarray.proto import DocListProto\nda_proto = DocListProto()\nfor doc in self:\nda_proto.docs.append(doc.to_protobuf())\nreturn da_proto\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList","title":"<code>docarray.array.doc_list.io.IOMixinDocList</code>","text":"<p>             Bases: <code>Iterable[T_doc]</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>class IOMixinDocList(Iterable[T_doc]):\ndoc_type: Type[T_doc]\n@abstractmethod\ndef __len__(self):\n...\n@abstractmethod\ndef __init__(\nself,\ndocs: Optional[Iterable[BaseDoc]] = None,\n):\n...\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n        :param pb_msg: The protobuf message from where to construct the DocList\n        \"\"\"\nreturn cls(cls.doc_type.from_protobuf(doc_proto) for doc_proto in pb_msg.docs)\ndef to_protobuf(self) -&gt; 'DocListProto':\n\"\"\"Convert `DocList` into a Protobuf message\"\"\"\nfrom docarray.proto import DocListProto\nda_proto = DocListProto()\nfor doc in self:\nda_proto.docs.append(doc.to_protobuf())\nreturn da_proto\n@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n        :param data: Bytes from which to deserialize\n        :param protocol: protocol that was used to serialize\n        :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :return: the deserialized `DocList`\n        \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\ndef _write_bytes(\nself,\nbf: BinaryIO,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\nif protocol in ARRAY_PROTOCOLS:\ncompress_ctx = _get_compress_ctx(compress)\nelse:\n# delegate the compression to per-doc compression\ncompress_ctx = None\nfc: ContextManager\nif compress_ctx is None:\n# if compress do not support streaming then postpone the compress\n# into the for-loop\nf, fc = bf, nullcontext()\nelse:\nf = compress_ctx(bf)\nfc = f\ncompress = None\nwith fc:\nif protocol == 'protobuf-array':\nf.write(self.to_protobuf().SerializePartialToString())\nelif protocol == 'pickle-array':\nf.write(pickle.dumps(self))\nelif protocol == 'json-array':\nf.write(self.to_json().encode())\nelif protocol in SINGLE_PROTOCOLS:\nf.write(\nb''.join(\nself._to_binary_stream(\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\n)\n)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only {ALLOWED_PROTOCOLS}.'\n)\ndef _to_binary_stream(\nself,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; Iterator[bytes]:\nfrom rich import filesize\nif show_progress:\nfrom docarray.utils._internal.progress_bar import _get_progressbar\npbar, t = _get_progressbar(\n'Serializing', disable=not show_progress, total=len(self)\n)\nelse:\nfrom contextlib import nullcontext\npbar = nullcontext()\nyield self._stream_header\nwith pbar:\nif show_progress:\n_total_size = 0\npbar.start_task(t)\nfor doc in self:\ndoc_bytes = doc.to_bytes(protocol=protocol, compress=compress)\nlen_doc_as_bytes = len(doc_bytes).to_bytes(4, 'big', signed=False)\nall_bytes = len_doc_as_bytes + doc_bytes\nyield all_bytes\nif show_progress:\n_total_size += len(all_bytes)\npbar.update(\nt,\nadvance=1,\ntotal_size=str(filesize.decimal(_total_size)),\n)\ndef to_bytes(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nfile_ctx: Optional[BinaryIO] = None,\nshow_progress: bool = False,\n) -&gt; Optional[bytes]:\n\"\"\"Serialize itself into `bytes`.\n        For more Pythonic code, please use ``bytes(...)``.\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between : `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param file_ctx: File or filename or serialized bytes where the data is stored.\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :return: the binary serialization in bytes or None if file_ctx is passed where to store\n        \"\"\"\nwith file_ctx or io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\nif isinstance(bf, io.BytesIO):\nreturn bf.getvalue()\nreturn None\n@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocList`.\n        :param data: Base64 string to deserialize\n        :param protocol: protocol that was used to serialize\n        :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :return: the deserialized `DocList`\n        \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\ndef to_base64(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; str:\n\"\"\"Serialize itself into base64 encoded string.\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :return: the binary serialization in bytes or None if file_ctx is passed where to store\n        \"\"\"\nwith io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\ncompress=compress,\nprotocol=protocol,\nshow_progress=show_progress,\n)\nreturn base64.b64encode(bf.getvalue()).decode('utf-8')\n@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n        :param file: JSON object from where to deserialize a `DocList`\n        :return: the deserialized `DocList`\n        \"\"\"\njson_docs = orjson.loads(file)\nreturn cls([cls.doc_type(**v) for v in json_docs])\ndef to_json(self) -&gt; str:\n\"\"\"Convert the object into JSON bytes. Can be loaded via `.from_json`.\n        :return: JSON serialization of `DocList`\n        \"\"\"\nreturn orjson_dumps(self).decode('UTF-8')\n@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, csv.Dialect] = 'excel',\n) -&gt; 'T':\n\"\"\"\n        Load a DocList from a csv file following the schema defined in the\n        [`.doc_type`][docarray.DocList] attribute.\n        Every row of the csv file will be mapped to one document in the doc_list.\n        The column names (defined in the first row) have to match the field names\n        of the Document type.\n        For nested fields use \"__\"-separated access paths, such as `'image__url'`.\n        List-like fields (including field of type DocList) are not supported.\n        :param file_path: path to csv file to load DocList from.\n        :param encoding: encoding used to read the csv file. Defaults to 'utf-8'.\n        :param dialect: defines separator and how to handle whitespaces etc.\n            Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n            instance or one string of:\n            `'excel'` (for comma separated values),\n            `'excel-tab'` (for tab separated values),\n            `'unix'` (for csv file generated on UNIX systems).\n        :return: `DocList` object\n        \"\"\"\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\nif file_path.startswith('http'):\nimport urllib.request\nwith urllib.request.urlopen(file_path) as f:\nfile = StringIO(f.read().decode(encoding))\nreturn cls._from_csv_file(file, dialect)\nelse:\nwith open(file_path, 'r', encoding=encoding) as fp:\nreturn cls._from_csv_file(fp, dialect)\n@classmethod\ndef _from_csv_file(\ncls: Type['T'],\nfile: Union[StringIO, TextIOWrapper],\ndialect: Union[str, csv.Dialect],\n) -&gt; 'T':\nrows = csv.DictReader(file, dialect=dialect)\ndoc_type = cls.doc_type\ndocs = []\nfield_names: List[str] = (\n[] if rows.fieldnames is None else [str(f) for f in rows.fieldnames]\n)\nif field_names is None or len(field_names) == 0:\nraise TypeError(\"No field names are given.\")\nvalid_paths = _all_access_paths_valid(\ndoc_type=doc_type, access_paths=field_names\n)\nif not all(valid_paths):\nraise ValueError(\nf'Column names do not match the schema of the DocList\\'s '\nf'document type ({cls.doc_type.__name__}): '\nf'{list(compress(field_names, [not v for v in valid_paths]))}'\n)\nfor access_path2val in rows:\ndoc_dict: Dict[Any, Any] = _access_path_dict_to_nested_dict(access_path2val)\ndocs.append(doc_type.parse_obj(doc_dict))\nreturn cls(docs)\ndef to_csv(\nself, file_path: str, dialect: Union[str, csv.Dialect] = 'excel'\n) -&gt; None:\n\"\"\"\n        Save a `DocList` to a csv file.\n        The field names will be stored in the first row. Each row corresponds to the\n        information of one Document.\n        Columns for nested fields will be named after the \"__\"-seperated access paths,\n        such as `'image__url'` for `image.url`.\n        :param file_path: path to a csv file.\n        :param dialect: defines separator and how to handle whitespaces etc.\n            Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n            instance or one string of:\n            `'excel'` (for comma separated values),\n            `'excel-tab'` (for tab separated values),\n            `'unix'` (for csv file generated on UNIX systems).\n        \"\"\"\nif self.doc_type == AnyDoc or self.doc_type == BaseDoc:\nraise TypeError(\nf'{type(self)} must be homogeneous to be converted to a csv.'\n'There is no document schema defined. '\nf'Please specify the {type(self)}\\'s Document type using `{type(self)}[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\nwith open(file_path, 'w') as csv_file:\nwriter = csv.DictWriter(csv_file, fieldnames=fields, dialect=dialect)\nwriter.writeheader()\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\nwriter.writerow(doc_dict)\n@classmethod\ndef from_dataframe(cls: Type['T'], df: 'pd.DataFrame') -&gt; 'T':\n\"\"\"\n        Load a `DocList` from a `pandas.DataFrame` following the schema\n        defined in the [`.doc_type`][docarray.DocList] attribute.\n        Every row of the dataframe will be mapped to one Document in the doc_list.\n        The column names of the dataframe have to match the field names of the\n        Document type.\n        For nested fields use \"__\"-separated access paths as column names,\n        such as `'image__url'`.\n        List-like fields (including field of type DocList) are not supported.\n        ---\n        ```python\n        import pandas as pd\n        from docarray import BaseDoc, DocList\n        class Person(BaseDoc):\n            name: str\n            follower: int\n        df = pd.DataFrame(\n            data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n        )\n        docs = DocList[Person].from_dataframe(df)\n        assert docs.name == ['Maria', 'Jake']\n        assert docs.follower == [12345, 54321]\n        ```\n        ---\n        :param df: `pandas.DataFrame` to extract Document's information from\n        :return: `DocList` where each Document contains the information of one\n            corresponding row of the `pandas.DataFrame`.\n        \"\"\"\nfrom docarray import DocList\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\ndoc_type = cls.doc_type\ndocs = DocList.__class_getitem__(doc_type)()\nfield_names = df.columns.tolist()\nif field_names is None or len(field_names) == 0:\nraise TypeError(\"No field names are given.\")\nvalid_paths = _all_access_paths_valid(\ndoc_type=doc_type, access_paths=field_names\n)\nif not all(valid_paths):\nraise ValueError(\nf'Column names do not match the schema of the DocList\\'s '\nf'document type ({cls.doc_type.__name__}): '\nf'{list(compress(field_names, [not v for v in valid_paths]))}'\n)\nfor row in df.itertuples():\naccess_path2val = row._asdict()\naccess_path2val.pop('index', None)\ndoc_dict = _access_path_dict_to_nested_dict(access_path2val)\ndocs.append(doc_type.parse_obj(doc_dict))\nreturn docs\ndef to_dataframe(self) -&gt; 'pd.DataFrame':\n\"\"\"\n        Save a DocList to a `pandas.DataFrame`.\n        The field names will be stored as column names. Each row of the dataframe corresponds\n        to the information of one Document.\n        Columns for nested fields will be named after the \"__\"-seperated access paths,\n        such as `'image__url'` for `image.url`.\n        :return: `pandas.DataFrame`\n        \"\"\"\nif TYPE_CHECKING:\nimport pandas as pd\nelse:\npd = import_library('pandas', raise_error=True)\nif self.doc_type == AnyDoc:\nraise TypeError(\n'DocList must be homogeneous to be converted to a DataFrame.'\n'There is no document schema defined. '\n'Please specify the DocList\\'s Document type using `DocList[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\ndf = pd.DataFrame(columns=fields)\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\ndoc_dict = {k: [v] for k, v in doc_dict.items()}\ndf = pd.concat([df, pd.DataFrame.from_dict(doc_dict)], ignore_index=True)\nreturn df\n# Methods to load from/to files in different formats\n@property\ndef _stream_header(self) -&gt; bytes:\n# Binary format for streaming case\n# V2 DocList streaming serialization format\n# | 1 byte | 8 bytes | 4 bytes | variable(DocArray &gt;=0.30) | 4 bytes | variable(DocArray &gt;=0.30) ...\n# 1 byte (uint8)\nversion_byte = b'\\x02'\n# 8 bytes (uint64)\nnum_docs_as_bytes = len(self).to_bytes(8, 'big', signed=False)\nreturn version_byte + num_docs_as_bytes\n@classmethod\n@abstractmethod\ndef _get_proto_class(cls: Type[T]):\n...\n@classmethod\ndef _load_binary_all(\ncls: Type[T],\nfile_ctx: Union[ContextManager[io.BufferedReader], ContextManager[bytes]],\nprotocol: Optional[ProtocolType],\ncompress: Optional[str],\nshow_progress: bool,\ntensor_type: Optional[Type['AbstractTensor']] = None,\n):\n\"\"\"Read a `DocList` object from a binary file\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :param tensor_type: only relevant for DocVec; tensor_type of the DocVec\n        :return: a `DocList`\n        \"\"\"\nwith file_ctx as fp:\nif isinstance(fp, bytes):\nd = fp\nelse:\nd = fp.read()\nif protocol is not None and protocol in (\n'pickle-array',\n'protobuf-array',\n'json-array',\n):\nif _get_compress_ctx(algorithm=compress) is not None:\nd = _decompress_bytes(d, algorithm=compress)\ncompress = None\nif protocol is not None and protocol == 'protobuf-array':\nproto = cls._get_proto_class()()\nproto.ParseFromString(d)\nif tensor_type is not None:\ncls_ = cast('IOMixinDocVec', cls)\nreturn cls_.from_protobuf(proto, tensor_type=tensor_type)\nelse:\nreturn cls.from_protobuf(proto)\nelif protocol is not None and protocol == 'pickle-array':\nreturn pickle.loads(d)\nelif protocol is not None and protocol == 'json-array':\nif tensor_type is not None:\ncls_ = cast('IOMixinDocVec', cls)\nreturn cls_.from_json(d, tensor_type=tensor_type)\nelse:\nreturn cls.from_json(d)\n# Binary format for streaming case\nelse:\nfrom rich import filesize\nfrom docarray.utils._internal.progress_bar import _get_progressbar\n# 1 byte (uint8)\nversion_num = int.from_bytes(d[0:1], 'big', signed=False)\nif version_num != 2:\nraise ValueError(\nf'Unsupported version number {version_num} in binary format, expected 2'\n)\n# 8 bytes (uint64)\nnum_docs = int.from_bytes(d[1:9], 'big', signed=False)\npbar, t = _get_progressbar(\n'Deserializing', disable=not show_progress, total=num_docs\n)\n# this 9 is version + num_docs bytes used\nstart_pos = 9\ndocs = []\nwith pbar:\n_total_size = 0\npbar.start_task(t)\nfor _ in range(num_docs):\n# 4 bytes (uint32)\nlen_current_doc_in_bytes = int.from_bytes(\nd[start_pos : start_pos + 4], 'big', signed=False\n)\nstart_doc_pos = start_pos + 4\nend_doc_pos = start_doc_pos + len_current_doc_in_bytes\nstart_pos = end_doc_pos\n# variable length bytes doc\nload_protocol: ProtocolType = protocol or cast(\nProtocolType, 'protobuf'\n)\ndoc = cls.doc_type.from_bytes(\nd[start_doc_pos:end_doc_pos],\nprotocol=load_protocol,\ncompress=compress,\n)\ndocs.append(doc)\n_total_size += len_current_doc_in_bytes\npbar.update(\nt, advance=1, total_size=str(filesize.decimal(_total_size))\n)\nif tensor_type is not None:\ncls__ = cast(Type['DocVec'], cls)\n# mypy doesn't realize that cls_ is callable\nreturn cls__(docs, tensor_type=tensor_type)  # type: ignore\nreturn cls(docs)\n@classmethod\ndef _load_binary_stream(\ncls: Type[T],\nfile_ctx: ContextManager[io.BufferedReader],\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; Generator['T_doc', None, None]:\n\"\"\"Yield `Document` objects from a binary file\n        :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :return: a generator of `Document` objects\n        \"\"\"\nfrom rich import filesize\nwith file_ctx as f:\nversion_numdocs_lendoc0 = f.read(9)\n# 1 byte (uint8)\nversion_num = int.from_bytes(\nversion_numdocs_lendoc0[0:1], 'big', signed=False\n)\nif version_num != 2:\nraise ValueError(\nf'Unsupported version number {version_num} in binary format, expected 2'\n)\n# 8 bytes (uint64)\nnum_docs = int.from_bytes(version_numdocs_lendoc0[1:9], 'big', signed=False)\nif show_progress:\nfrom docarray.utils._internal.progress_bar import _get_progressbar\npbar, t = _get_progressbar(\n'Deserializing', disable=not show_progress, total=num_docs\n)\nelse:\nfrom contextlib import nullcontext\npbar = nullcontext()\nwith pbar:\nif show_progress:\n_total_size = 0\npbar.start_task(t)\nfor _ in range(num_docs):\n# 4 bytes (uint32)\nlen_current_doc_in_bytes = int.from_bytes(\nf.read(4), 'big', signed=False\n)\nload_protocol: ProtocolType = protocol\nyield cls.doc_type.from_bytes(\nf.read(len_current_doc_in_bytes),\nprotocol=load_protocol,\ncompress=compress,\n)\nif show_progress:\n_total_size += len_current_doc_in_bytes\npbar.update(\nt, advance=1, total_size=str(filesize.decimal(_total_size))\n)\n@staticmethod\ndef _get_file_context(\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType,\ncompress: Optional[str] = None,\n) -&gt; Tuple[\nUnion[nullcontext, io.BufferedReader], Optional[ProtocolType], Optional[str]\n]:\nload_protocol: Optional[ProtocolType] = protocol\nload_compress: Optional[str] = compress\nfile_ctx: Union[nullcontext, io.BufferedReader]\nif isinstance(file, (io.BufferedReader, _LazyRequestReader, bytes)):\nfile_ctx = nullcontext(file)\n# by checking path existence we allow file to be of type Path, LocalPath, PurePath and str\nelif isinstance(file, (str, pathlib.Path)) and os.path.exists(file):\nload_protocol, load_compress = _protocol_and_compress_from_file_path(\nfile, protocol, compress\n)\nfile_ctx = open(file, 'rb')\nelse:\nraise FileNotFoundError(f'cannot find file {file}')\nreturn file_ctx, load_protocol, load_compress\n@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_list elements from a compressed binary file.\n        In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n        !!! note\n            If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n            This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n            string interpolation of the respective `protocol` and `compress` methods.\n            For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n            and `compress=lz4`.\n        :param file: File or filename or serialized bytes where the data is stored.\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :param streaming: if `True` returns a generator over `Document` objects.\n        :return: a `DocList` object\n        \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx, load_protocol, load_compress, show_progress\n)\ndef save_binary(\nself,\nfile: Union[str, pathlib.Path],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\n\"\"\"Save DocList into a binary file.\n        It will use the protocol to pick how to save the DocList.\n        If used `picke-doc_list` and `protobuf-array` the DocList will be stored\n        and compressed at complete level using `pickle` or `protobuf`.\n        When using `protobuf` or `pickle` as protocol each Document in DocList\n        will be stored individually and this would make it available for streaming.\n         !!! note\n            If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n            This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n            string interpolation of the respective `protocol` and `compress` methods.\n            For example if `file=my_docarray.protobuf.lz4` then the binary data will be created using `protocol=protobuf`\n            and `compress=lz4`.\n        :param file: File or filename to which the data is saved.\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        \"\"\"\nif isinstance(file, io.BufferedWriter):\nfile_ctx = nullcontext(file)\nelse:\n_protocol, _compress = _protocol_and_compress_from_file_path(file)\nif _protocol is not None:\nprotocol = _protocol\nif _compress is not None:\ncompress = _compress\nfile_ctx = open(file, 'wb')\nself.to_bytes(\nprotocol=protocol,\ncompress=compress,\nfile_ctx=file_ctx,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_base64","title":"<code>from_base64(data, protocol='protobuf-array', compress=None, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Deserialize base64 strings into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Base64 string to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocList`.\n    :param data: Base64 string to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the deserialized `DocList`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_bytes","title":"<code>from_bytes(data, protocol='protobuf-array', compress=None, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Deserialize bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from which to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n    :param data: Bytes from which to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the deserialized `DocList`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_csv","title":"<code>from_csv(file_path, encoding='utf-8', dialect='excel')</code>  <code>classmethod</code>","text":"<p>Load a DocList from a csv file following the schema defined in the <code>.doc_type</code> attribute. Every row of the csv file will be mapped to one document in the doc_list. The column names (defined in the first row) have to match the field names of the Document type. For nested fields use \"__\"-separated access paths, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to csv file to load DocList from.</p> required <code>encoding</code> <code>str</code> <p>encoding used to read the csv file. Defaults to 'utf-8'.</p> <code>'utf-8'</code> <code>dialect</code> <code>Union[str, Dialect]</code> <p>defines separator and how to handle whitespaces etc. Can be a <code>csv.Dialect</code> instance or one string of: <code>'excel'</code> (for comma separated values), <code>'excel-tab'</code> (for tab separated values), <code>'unix'</code> (for csv file generated on UNIX systems).</p> <code>'excel'</code> <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, csv.Dialect] = 'excel',\n) -&gt; 'T':\n\"\"\"\n    Load a DocList from a csv file following the schema defined in the\n    [`.doc_type`][docarray.DocList] attribute.\n    Every row of the csv file will be mapped to one document in the doc_list.\n    The column names (defined in the first row) have to match the field names\n    of the Document type.\n    For nested fields use \"__\"-separated access paths, such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    :param file_path: path to csv file to load DocList from.\n    :param encoding: encoding used to read the csv file. Defaults to 'utf-8'.\n    :param dialect: defines separator and how to handle whitespaces etc.\n        Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n        instance or one string of:\n        `'excel'` (for comma separated values),\n        `'excel-tab'` (for tab separated values),\n        `'unix'` (for csv file generated on UNIX systems).\n    :return: `DocList` object\n    \"\"\"\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\nif file_path.startswith('http'):\nimport urllib.request\nwith urllib.request.urlopen(file_path) as f:\nfile = StringIO(f.read().decode(encoding))\nreturn cls._from_csv_file(file, dialect)\nelse:\nwith open(file_path, 'r', encoding=encoding) as fp:\nreturn cls._from_csv_file(fp, dialect)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_dataframe","title":"<code>from_dataframe(df)</code>  <code>classmethod</code>","text":"<p>Load a <code>DocList</code> from a <code>pandas.DataFrame</code> following the schema defined in the <code>.doc_type</code> attribute. Every row of the dataframe will be mapped to one Document in the doc_list. The column names of the dataframe have to match the field names of the Document type. For nested fields use \"__\"-separated access paths as column names, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <pre><code>import pandas as pd\nfrom docarray import BaseDoc, DocList\nclass Person(BaseDoc):\nname: str\nfollower: int\ndf = pd.DataFrame(\ndata=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n)\ndocs = DocList[Person].from_dataframe(df)\nassert docs.name == ['Maria', 'Jake']\nassert docs.follower == [12345, 54321]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p><code>pandas.DataFrame</code> to extract Document's information from</p> required <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> where each Document contains the information of one corresponding row of the <code>pandas.DataFrame</code>.</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_dataframe(cls: Type['T'], df: 'pd.DataFrame') -&gt; 'T':\n\"\"\"\n    Load a `DocList` from a `pandas.DataFrame` following the schema\n    defined in the [`.doc_type`][docarray.DocList] attribute.\n    Every row of the dataframe will be mapped to one Document in the doc_list.\n    The column names of the dataframe have to match the field names of the\n    Document type.\n    For nested fields use \"__\"-separated access paths as column names,\n    such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    ---\n    ```python\n    import pandas as pd\n    from docarray import BaseDoc, DocList\n    class Person(BaseDoc):\n        name: str\n        follower: int\n    df = pd.DataFrame(\n        data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n    )\n    docs = DocList[Person].from_dataframe(df)\n    assert docs.name == ['Maria', 'Jake']\n    assert docs.follower == [12345, 54321]\n    ```\n    ---\n    :param df: `pandas.DataFrame` to extract Document's information from\n    :return: `DocList` where each Document contains the information of one\n        corresponding row of the `pandas.DataFrame`.\n    \"\"\"\nfrom docarray import DocList\nif cls.doc_type == AnyDoc or cls.doc_type == BaseDoc:\nraise TypeError(\n'There is no document schema defined. '\nf'Please specify the {cls}\\'s Document type using `{cls}[MyDoc]`.'\n)\ndoc_type = cls.doc_type\ndocs = DocList.__class_getitem__(doc_type)()\nfield_names = df.columns.tolist()\nif field_names is None or len(field_names) == 0:\nraise TypeError(\"No field names are given.\")\nvalid_paths = _all_access_paths_valid(\ndoc_type=doc_type, access_paths=field_names\n)\nif not all(valid_paths):\nraise ValueError(\nf'Column names do not match the schema of the DocList\\'s '\nf'document type ({cls.doc_type.__name__}): '\nf'{list(compress(field_names, [not v for v in valid_paths]))}'\n)\nfor row in df.itertuples():\naccess_path2val = row._asdict()\naccess_path2val.pop('index', None)\ndoc_dict = _access_path_dict_to_nested_dict(access_path2val)\ndocs.append(doc_type.parse_obj(doc_dict))\nreturn docs\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_json","title":"<code>from_json(file)</code>  <code>classmethod</code>","text":"<p>Deserialize JSON strings or bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, bytearray]</code> <p>JSON object from where to deserialize a <code>DocList</code></p> required <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n    :param file: JSON object from where to deserialize a `DocList`\n    :return: the deserialized `DocList`\n    \"\"\"\njson_docs = orjson.loads(file)\nreturn cls([cls.doc_type(**v) for v in json_docs])\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocListProto</code> <p>The protobuf message from where to construct the DocList</p> required Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocListProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: The protobuf message from where to construct the DocList\n    \"\"\"\nreturn cls(cls.doc_type.from_protobuf(doc_proto) for doc_proto in pb_msg.docs)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.load_binary","title":"<code>load_binary(file, protocol='protobuf-array', compress=None, show_progress=False, streaming=False)</code>  <code>classmethod</code>","text":"<p>Load doc_list elements from a compressed binary file.</p> <p>In case protocol is pickle the <code>Documents</code> are streamed from disk to save memory usage</p> <p>Note</p> <p>If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions. This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a string interpolation of the respective <code>protocol</code> and <code>compress</code> methods. For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be loaded assuming <code>protocol=protobuf</code> and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, Path, BufferedReader, _LazyRequestReader]</code> <p>File or filename or serialized bytes where the data is stored.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>streaming</code> <code>bool</code> <p>if <code>True</code> returns a generator over <code>Document</code> objects.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[T, Generator[T_doc, None, None]]</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_list elements from a compressed binary file.\n    In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n    !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename or serialized bytes where the data is stored.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param streaming: if `True` returns a generator over `Document` objects.\n    :return: a `DocList` object\n    \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx, load_protocol, load_compress, show_progress\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.save_binary","title":"<code>save_binary(file, protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Save DocList into a binary file.</p> <p>It will use the protocol to pick how to save the DocList. If used <code>picke-doc_list</code> and <code>protobuf-array</code> the DocList will be stored and compressed at complete level using <code>pickle</code> or <code>protobuf</code>. When using <code>protobuf</code> or <code>pickle</code> as protocol each Document in DocList will be stored individually and this would make it available for streaming.</p> <p>!!! note     If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions.     This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a     string interpolation of the respective <code>protocol</code> and <code>compress</code> methods.     For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be created using <code>protocol=protobuf</code>     and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>File or filename to which the data is saved.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def save_binary(\nself,\nfile: Union[str, pathlib.Path],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\n\"\"\"Save DocList into a binary file.\n    It will use the protocol to pick how to save the DocList.\n    If used `picke-doc_list` and `protobuf-array` the DocList will be stored\n    and compressed at complete level using `pickle` or `protobuf`.\n    When using `protobuf` or `pickle` as protocol each Document in DocList\n    will be stored individually and this would make it available for streaming.\n     !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be created using `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename to which the data is saved.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    \"\"\"\nif isinstance(file, io.BufferedWriter):\nfile_ctx = nullcontext(file)\nelse:\n_protocol, _compress = _protocol_and_compress_from_file_path(file)\nif _protocol is not None:\nprotocol = _protocol\nif _compress is not None:\ncompress = _compress\nfile_ctx = open(file, 'wb')\nself.to_bytes(\nprotocol=protocol,\ncompress=compress,\nfile_ctx=file_ctx,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_base64","title":"<code>to_base64(protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Serialize itself into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_base64(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; str:\n\"\"\"Serialize itself into base64 encoded string.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\ncompress=compress,\nprotocol=protocol,\nshow_progress=show_progress,\n)\nreturn base64.b64encode(bf.getvalue()).decode('utf-8')\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_bytes","title":"<code>to_bytes(protocol='protobuf-array', compress=None, file_ctx=None, show_progress=False)</code>","text":"<p>Serialize itself into <code>bytes</code>.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between : <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>file_ctx</code> <code>Optional[BinaryIO]</code> <p>File or filename or serialized bytes where the data is stored.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_bytes(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nfile_ctx: Optional[BinaryIO] = None,\nshow_progress: bool = False,\n) -&gt; Optional[bytes]:\n\"\"\"Serialize itself into `bytes`.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between : `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param file_ctx: File or filename or serialized bytes where the data is stored.\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith file_ctx or io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\nif isinstance(bf, io.BytesIO):\nreturn bf.getvalue()\nreturn None\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_csv","title":"<code>to_csv(file_path, dialect='excel')</code>","text":"<p>Save a <code>DocList</code> to a csv file. The field names will be stored in the first row. Each row corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a csv file.</p> required <code>dialect</code> <code>Union[str, Dialect]</code> <p>defines separator and how to handle whitespaces etc. Can be a <code>csv.Dialect</code> instance or one string of: <code>'excel'</code> (for comma separated values), <code>'excel-tab'</code> (for tab separated values), <code>'unix'</code> (for csv file generated on UNIX systems).</p> <code>'excel'</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_csv(\nself, file_path: str, dialect: Union[str, csv.Dialect] = 'excel'\n) -&gt; None:\n\"\"\"\n    Save a `DocList` to a csv file.\n    The field names will be stored in the first row. Each row corresponds to the\n    information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :param file_path: path to a csv file.\n    :param dialect: defines separator and how to handle whitespaces etc.\n        Can be a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect)\n        instance or one string of:\n        `'excel'` (for comma separated values),\n        `'excel-tab'` (for tab separated values),\n        `'unix'` (for csv file generated on UNIX systems).\n    \"\"\"\nif self.doc_type == AnyDoc or self.doc_type == BaseDoc:\nraise TypeError(\nf'{type(self)} must be homogeneous to be converted to a csv.'\n'There is no document schema defined. '\nf'Please specify the {type(self)}\\'s Document type using `{type(self)}[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\nwith open(file_path, 'w') as csv_file:\nwriter = csv.DictWriter(csv_file, fieldnames=fields, dialect=dialect)\nwriter.writeheader()\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\nwriter.writerow(doc_dict)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Save a DocList to a <code>pandas.DataFrame</code>. The field names will be stored as column names. Each row of the dataframe corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas.DataFrame</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_dataframe(self) -&gt; 'pd.DataFrame':\n\"\"\"\n    Save a DocList to a `pandas.DataFrame`.\n    The field names will be stored as column names. Each row of the dataframe corresponds\n    to the information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :return: `pandas.DataFrame`\n    \"\"\"\nif TYPE_CHECKING:\nimport pandas as pd\nelse:\npd = import_library('pandas', raise_error=True)\nif self.doc_type == AnyDoc:\nraise TypeError(\n'DocList must be homogeneous to be converted to a DataFrame.'\n'There is no document schema defined. '\n'Please specify the DocList\\'s Document type using `DocList[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\ndf = pd.DataFrame(columns=fields)\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\ndoc_dict = {k: [v] for k, v in doc_dict.items()}\ndf = pd.concat([df, pd.DataFrame.from_dict(doc_dict)], ignore_index=True)\nreturn df\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_json","title":"<code>to_json()</code>","text":"<p>Convert the object into JSON bytes. Can be loaded via <code>.from_json</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON serialization of <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Convert the object into JSON bytes. Can be loaded via `.from_json`.\n    :return: JSON serialization of `DocList`\n    \"\"\"\nreturn orjson_dumps(self).decode('UTF-8')\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.io.IOMixinDocList.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert <code>DocList</code> into a Protobuf message</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_protobuf(self) -&gt; 'DocListProto':\n\"\"\"Convert `DocList` into a Protobuf message\"\"\"\nfrom docarray.proto import DocListProto\nda_proto = DocListProto()\nfor doc in self:\nda_proto.docs.append(doc.to_protobuf())\nreturn da_proto\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin","title":"<code>docarray.array.doc_list.pushpull.PushPullMixin</code>","text":"<p>             Bases: <code>Iterable['BaseDoc']</code></p> <p>Mixin class for push/pull functionality.</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>class PushPullMixin(Iterable['BaseDoc']):\n\"\"\"Mixin class for push/pull functionality.\"\"\"\n__backends__: Dict[str, Type['AbstractDocStore']] = {}\ndoc_type: Type['BaseDoc']\n@abstractmethod\ndef __len__(self) -&gt; int:\n...\n@staticmethod\ndef resolve_url(url: str) -&gt; Tuple[PUSH_PULL_PROTOCOL, str]:\n\"\"\"Resolve the URL to the correct protocol and name.\n        :param url: url to resolve\n        \"\"\"\nprotocol, name = url.split('://', 2)\nif protocol in SUPPORTED_PUSH_PULL_PROTOCOLS:\nprotocol = cast(PUSH_PULL_PROTOCOL, protocol)\nreturn protocol, name\nelse:\nraise ValueError(f'Unsupported protocol {protocol}')\n@classmethod\ndef get_pushpull_backend(\ncls: Type[SelfPushPullMixin], protocol: PUSH_PULL_PROTOCOL\n) -&gt; Type['AbstractDocStore']:\n\"\"\"\n        Get the backend for the given protocol.\n        :param protocol: the protocol to use, e.g. 'file', 's3'\n        :return: the backend class\n        \"\"\"\nif protocol in cls.__backends__:\nreturn cls.__backends__[protocol]\nif protocol == 'file':\nfrom docarray.store.file import FileDocStore\ncls.__backends__[protocol] = FileDocStore\nlogging.debug('Loaded Local Filesystem backend')\nelif protocol == 's3':\nfrom docarray.store.s3 import S3DocStore\ncls.__backends__[protocol] = S3DocStore\nlogging.debug('Loaded S3 backend')\nelse:\nraise NotImplementedError(f'protocol {protocol} not supported')\nreturn cls.__backends__[protocol]\ndef push(\nself,\nurl: str,\nshow_progress: bool = False,\n**kwargs,\n) -&gt; Dict:\n\"\"\"Push this `DocList` object to the specified url.\n        :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nlogging.info(f'Pushing {len(self)} docs to {url}')\nprotocol, name = self.__class__.resolve_url(url)\nreturn self.__class__.get_pushpull_backend(protocol).push(\nself, name, show_progress  # type: ignore\n)\n@classmethod\ndef push_stream(\ncls: Type[SelfPushPullMixin],\ndocs: Iterator['BaseDoc'],\nurl: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified url.\n        :param docs: a stream of documents\n        :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nlogging.info(f'Pushing stream to {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).push_stream(docs, name, show_progress)\n@classmethod\ndef pull(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = True,\n) -&gt; 'DocList':\n\"\"\"Pull a `DocList` from the specified url.\n        :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: store the downloaded `DocList` to local folder\n        :return: a `DocList` object\n        \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull(\ncls, name, show_progress, local_cache  # type: ignore\n)\n@classmethod\ndef pull_stream(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = False,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified url.\n        :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: store the downloaded `DocList` to local folder\n        :return: Iterator of Documents\n        \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling Document stream from {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull_stream(\ncls, name, show_progress, local_cache  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.get_pushpull_backend","title":"<code>get_pushpull_backend(protocol)</code>  <code>classmethod</code>","text":"<p>Get the backend for the given protocol.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>PUSH_PULL_PROTOCOL</code> <p>the protocol to use, e.g. 'file', 's3'</p> required <p>Returns:</p> Type Description <code>Type[AbstractDocStore]</code> <p>the backend class</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef get_pushpull_backend(\ncls: Type[SelfPushPullMixin], protocol: PUSH_PULL_PROTOCOL\n) -&gt; Type['AbstractDocStore']:\n\"\"\"\n    Get the backend for the given protocol.\n    :param protocol: the protocol to use, e.g. 'file', 's3'\n    :return: the backend class\n    \"\"\"\nif protocol in cls.__backends__:\nreturn cls.__backends__[protocol]\nif protocol == 'file':\nfrom docarray.store.file import FileDocStore\ncls.__backends__[protocol] = FileDocStore\nlogging.debug('Loaded Local Filesystem backend')\nelif protocol == 's3':\nfrom docarray.store.s3 import S3DocStore\ncls.__backends__[protocol] = S3DocStore\nlogging.debug('Loaded S3 backend')\nelse:\nraise NotImplementedError(f'protocol {protocol} not supported')\nreturn cls.__backends__[protocol]\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.pull","title":"<code>pull(url, show_progress=False, local_cache=True)</code>  <code>classmethod</code>","text":"<p>Pull a <code>DocList</code> from the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> <code>False</code> <code>local_cache</code> <code>bool</code> <p>store the downloaded <code>DocList</code> to local folder</p> <code>True</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef pull(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = True,\n) -&gt; 'DocList':\n\"\"\"Pull a `DocList` from the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded `DocList` to local folder\n    :return: a `DocList` object\n    \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull(\ncls, name, show_progress, local_cache  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.pull_stream","title":"<code>pull_stream(url, show_progress=False, local_cache=False)</code>  <code>classmethod</code>","text":"<p>Pull a stream of Documents from the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> <code>False</code> <code>local_cache</code> <code>bool</code> <p>store the downloaded <code>DocList</code> to local folder</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[BaseDoc]</code> <p>Iterator of Documents</p> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef pull_stream(\ncls: Type[SelfPushPullMixin],\nurl: str,\nshow_progress: bool = False,\nlocal_cache: bool = False,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded `DocList` to local folder\n    :return: Iterator of Documents\n    \"\"\"\nfrom docarray.base_doc import AnyDoc\nif cls.doc_type == AnyDoc:\nraise TypeError(\n'There is no document schema defined. '\n'Please specify the `DocList`\\'s Document type using `DocList[MyDoc]`.'\n)\nlogging.info(f'Pulling Document stream from {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).pull_stream(\ncls, name, show_progress, local_cache  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.push","title":"<code>push(url, show_progress=False, **kwargs)</code>","text":"<p>Push this <code>DocList</code> object to the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>def push(\nself,\nurl: str,\nshow_progress: bool = False,\n**kwargs,\n) -&gt; Dict:\n\"\"\"Push this `DocList` object to the specified url.\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nlogging.info(f'Pushing {len(self)} docs to {url}')\nprotocol, name = self.__class__.resolve_url(url)\nreturn self.__class__.get_pushpull_backend(protocol).push(\nself, name, show_progress  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.push_stream","title":"<code>push_stream(docs, url, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Push a stream of documents to the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[BaseDoc]</code> <p>a stream of documents</p> required <code>url</code> <code>str</code> <p>url specifying the protocol and save name of the <code>DocList</code>. Should be of the form <code>protocol://namespace/name</code>. e.g. <code>s3://bucket/path/to/namespace/name</code>, <code>file:///path/to/folder/name</code></p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@classmethod\ndef push_stream(\ncls: Type[SelfPushPullMixin],\ndocs: Iterator['BaseDoc'],\nurl: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified url.\n    :param docs: a stream of documents\n    :param url: url specifying the protocol and save name of the `DocList`. Should be of the form ``protocol://namespace/name``. e.g. ``s3://bucket/path/to/namespace/name``, ``file:///path/to/folder/name``\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nlogging.info(f'Pushing stream to {url}')\nprotocol, name = cls.resolve_url(url)\nreturn cls.get_pushpull_backend(protocol).push_stream(docs, name, show_progress)\n</code></pre>"},{"location":"API_reference/array/da/#docarray.array.doc_list.pushpull.PushPullMixin.resolve_url","title":"<code>resolve_url(url)</code>  <code>staticmethod</code>","text":"<p>Resolve the URL to the correct protocol and name.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url to resolve</p> required Source code in <code>docarray/array/doc_list/pushpull.py</code> <pre><code>@staticmethod\ndef resolve_url(url: str) -&gt; Tuple[PUSH_PULL_PROTOCOL, str]:\n\"\"\"Resolve the URL to the correct protocol and name.\n    :param url: url to resolve\n    \"\"\"\nprotocol, name = url.split('://', 2)\nif protocol in SUPPORTED_PUSH_PULL_PROTOCOLS:\nprotocol = cast(PUSH_PULL_PROTOCOL, protocol)\nreturn protocol, name\nelse:\nraise ValueError(f'Unsupported protocol {protocol}')\n</code></pre>"},{"location":"API_reference/array/da_stack/","title":"DocVec","text":""},{"location":"API_reference/array/da_stack/#docvec","title":"DocVec","text":""},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec","title":"<code>docarray.array.doc_vec.doc_vec.DocVec</code>","text":"<p>             Bases: <code>IOMixinDocVec</code>, <code>AnyDocArray[T_doc]</code></p> <p>DocVec is a container of Documents appropriates to perform computation that require batches of data (ex: matrix multiplication, distance calculation, deep learning forward pass)</p> <p>A DocVec has a similar interface as <code>DocList</code> but with an underlying implementation that is column based instead of row based. Each field of the schema of the <code>DocVec</code> (the <code>.doc_type</code> which is a <code>BaseDoc</code>) will be stored in a column.</p> <p>If the field is a tensor, the data from all Documents will be stored as a single (torch/np/tf) tensor.</p> <p>If the tensor field is <code>AnyTensor</code> or a Union of tensor types, the <code>.tensor_type</code> will be used to determine the type of the column.</p> <p>If the field is another <code>BaseDoc</code> the column will be another <code>DocVec</code> that follows the schema of the nested Document.</p> <p>If the field is a <code>DocList</code> or <code>DocVec</code> then the column will be a list of <code>DocVec</code>.</p> <p>For any other type the column is a Python list.</p> <p>Every <code>Document</code> inside a <code>DocVec</code> is a view into the data columns stored at the <code>DocVec</code> level. The <code>BaseDoc</code> does not hold any data itself. The behavior of this Document \"view\" is similar to the behavior of <code>view = tensor[i]</code> in numpy/PyTorch.</p> <p>Note</p> <p>DocVec supports optional fields. Nevertheless if a field is optional it needs to be homogeneous. This means that if the first document has a None value all of the other documents should have a None value as well.</p> <p>Note</p> <p>If one field is Optional the column will be stored * as None if the first doc is as the field as None * as a normal column otherwise that cannot contain None value</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Sequence[T_doc]</code> <p>a homogeneous sequence of <code>BaseDoc</code></p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>Tensor Class used to wrap the doc_vec tensors. This is useful if the BaseDoc of this DocVec has some undefined tensor type like AnyTensor or Union of NdArray and TorchTensor</p> <code>NdArray</code> Source code in <code>docarray/array/doc_vec/doc_vec.py</code> <pre><code>class DocVec(IOMixinDocVec, AnyDocArray[T_doc]):  # type: ignore\n\"\"\"\n    DocVec is a container of Documents appropriates to perform\n    computation that require batches of data (ex: matrix multiplication, distance\n    calculation, deep learning forward pass)\n    A DocVec has a similar interface as [`DocList`][docarray.array.DocList]\n    but with an underlying implementation that is column based instead of row based.\n    Each field of the schema of the `DocVec` (the `.doc_type` which is a\n    [`BaseDoc`][docarray.BaseDoc]) will be stored in a column.\n    If the field is a tensor, the data from all Documents will be stored as a single\n    (torch/np/tf) tensor.\n    If the tensor field is `AnyTensor` or a Union of tensor types, the\n    `.tensor_type` will be used to determine the type of the column.\n    If the field is another [`BaseDoc`][docarray.BaseDoc] the column will be another\n    `DocVec` that follows the schema of the nested Document.\n    If the field is a [`DocList`][docarray.DocList] or `DocVec` then the column will\n    be a list of `DocVec`.\n    For any other type the column is a Python list.\n    Every `Document` inside a `DocVec` is a view into the data columns stored at the\n    `DocVec` level. The `BaseDoc` does not hold any data itself. The behavior of\n    this Document \"view\" is similar to the behavior of `view = tensor[i]` in\n    numpy/PyTorch.\n    !!! note\n        DocVec supports optional fields. Nevertheless if a field is optional it needs to\n        be homogeneous. This means that if the first document has a None value all of the\n        other documents should have a None value as well.\n    !!! note\n        If one field is Optional the column will be stored\n        * as None if the first doc is as the field as None\n        * as a normal column otherwise that cannot contain None value\n    :param docs: a homogeneous sequence of `BaseDoc`\n    :param tensor_type: Tensor Class used to wrap the doc_vec tensors. This is useful\n        if the BaseDoc of this DocVec has some undefined tensor type like\n        AnyTensor or Union of NdArray and TorchTensor\n    \"\"\"\ndoc_type: Type[T_doc] = BaseDoc  # type: ignore\ndef __init__(\nself: T,\ndocs: Sequence[T_doc],\ntensor_type: Type['AbstractTensor'] = NdArray,\n):\nif (\nnot hasattr(self, 'doc_type')\nor self.doc_type == AnyDoc\nor self.doc_type == BaseDoc\n):\nraise TypeError(\nf'{self.__class__.__name__} does not precise a doc_type. You probably should do'\nf'docs = DocVec[MyDoc](docs) instead of DocVec(docs)'\n)\nself.tensor_type = tensor_type\nself._is_unusable = False\ntensor_columns: Dict[str, Optional[AbstractTensor]] = dict()\ndoc_columns: Dict[str, Optional['DocVec']] = dict()\ndocs_vec_columns: Dict[str, Optional[ListAdvancedIndexing['DocVec']]] = dict()\nany_columns: Dict[str, ListAdvancedIndexing] = dict()\nif len(docs) == 0:\nraise ValueError(f'docs {docs}: should not be empty')\ndocs = (\ndocs\nif isinstance(docs, DocList)\nelse DocList.__class_getitem__(self.doc_type)(docs)\n)\nfor field_name, field in self.doc_type._docarray_fields().items():\n# here we iterate over the field of the docs schema, and we collect the data\n# from each document and put them in the corresponding column\nfield_type: Type = self.doc_type._get_field_annotation(field_name)\nfield_info = self.doc_type._docarray_fields()[field_name]\nis_field_required = (\nfield_info.is_required() if is_pydantic_v2 else field_info.required\n)\nfirst_doc_is_none = getattr(docs[0], field_name) is None\ndef _verify_optional_field_of_docs(docs):\nif is_field_required:\nif first_doc_is_none:\nraise ValueError(\nf'Field {field_name} is None for {docs[0]} even though it is required'\n)\nif first_doc_is_none:\nfor i, doc in enumerate(docs):\nif getattr(doc, field_name) is not None:\nraise ValueError(\nf'Field {field_name} is put to None for the first doc. This mean that '\nf'all of the other docs should have this field set to None as well. '\nf'This is not the case for {doc} at index {i}'\n)\ndef _check_doc_field_not_none(field_name, doc):\nif getattr(doc, field_name) is None:\nraise ValueError(\nf'Field {field_name} is None for {doc} even though it is not None for the first doc'\n)\nif is_tensor_union(field_type):\nfield_type = tensor_type\n# all generic tensor types such as AnyTensor, ImageTensor, etc. are subclasses of AbstractTensor.\n# Perform check only if the field_type is not an alias and is a subclass of AbstractTensor\nelif not isinstance(field_type, typingGenericAlias) and safe_issubclass(\nfield_type, AbstractTensor\n):\n# check if the tensor associated with the field_name in the document is a subclass of the tensor_type\n# e.g. if the field_type is AnyTensor but the type(docs[0][field_name]) is ImageTensor,\n# then we change the field_type to ImageTensor, since AnyTensor is a union of all the tensor types\n# and does not override any methods of specific tensor types\ntensor = getattr(docs[0], field_name)\nif safe_issubclass(tensor.__class__, tensor_type):\nfield_type = tensor_type\nif isinstance(field_type, type):\nif tf_available and safe_issubclass(field_type, TensorFlowTensor):\n# tf.Tensor does not allow item assignment, therefore the\n# optimized way\n# of initializing an empty array and assigning values to it\n# iteratively\n# does not work here, therefore handle separately.\nif first_doc_is_none:\n_verify_optional_field_of_docs(docs)\ntensor_columns[field_name] = None\nelse:\ntf_stack = []\nfor i, doc in enumerate(docs):\nval = getattr(doc, field_name)\n_check_doc_field_not_none(field_name, doc)\ntf_stack.append(val.tensor)\nstacked: tf.Tensor = tf.stack(tf_stack)\ntensor_columns[field_name] = TensorFlowTensor(stacked)\nelif jnp_available and safe_issubclass(field_type, JaxArray):\nif first_doc_is_none:\n_verify_optional_field_of_docs(docs)\ntensor_columns[field_name] = None\nelse:\ntf_stack = []\nfor i, doc in enumerate(docs):\nval = getattr(doc, field_name)\n_check_doc_field_not_none(field_name, doc)\ntf_stack.append(val.tensor)\njax_stacked: jnp.ndarray = jnp.stack(tf_stack)\ntensor_columns[field_name] = JaxArray(jax_stacked)\nelif safe_issubclass(field_type, AbstractTensor):\nif first_doc_is_none:\n_verify_optional_field_of_docs(docs)\ntensor_columns[field_name] = None\nelse:\ntensor = getattr(docs[0], field_name)\ncolumn_shape = (\n(len(docs), *tensor.shape)\nif tensor is not None\nelse (len(docs),)\n)\ntensor_columns[field_name] = field_type._docarray_from_native(\nfield_type.get_comp_backend().empty(\ncolumn_shape,\ndtype=tensor.dtype\nif hasattr(tensor, 'dtype')\nelse None,\ndevice=tensor.device\nif hasattr(tensor, 'device')\nelse None,\n)\n)\nfor i, doc in enumerate(docs):\n_check_doc_field_not_none(field_name, doc)\nval = getattr(doc, field_name)\ncast(AbstractTensor, tensor_columns[field_name])[i] = val\nelif safe_issubclass(field_type, BaseDoc):\nif first_doc_is_none:\n_verify_optional_field_of_docs(docs)\ndoc_columns[field_name] = None\nelse:\nif is_field_required:\ndoc_columns[field_name] = getattr(\ndocs, field_name\n).to_doc_vec(tensor_type=self.tensor_type)\nelse:\ndoc_columns[field_name] = DocList.__class_getitem__(\nfield_type\n)(getattr(docs, field_name)).to_doc_vec(\ntensor_type=self.tensor_type\n)\nelif safe_issubclass(field_type, AnyDocArray):\nif first_doc_is_none:\n_verify_optional_field_of_docs(docs)\ndocs_vec_columns[field_name] = None\nelse:\ndocs_list = list()\nfor doc in docs:\ndocs_nested = getattr(doc, field_name)\n_check_doc_field_not_none(field_name, doc)\nif isinstance(docs_nested, DocList):\ndocs_nested = docs_nested.to_doc_vec(\ntensor_type=self.tensor_type\n)\ndocs_list.append(docs_nested)\ndocs_vec_columns[field_name] = ListAdvancedIndexing(docs_list)\nelse:\nany_columns[field_name] = ListAdvancedIndexing(\ngetattr(docs, field_name)\n)\nelse:\nany_columns[field_name] = ListAdvancedIndexing(\ngetattr(docs, field_name)\n)\nself._storage = ColumnStorage(\ntensor_columns,\ndoc_columns,\ndocs_vec_columns,\nany_columns,\ntensor_type,\n)\n@classmethod\ndef from_columns_storage(cls: Type[T], storage: ColumnStorage) -&gt; T:\n\"\"\"\n        Create a DocVec directly from a storage object\n        :param storage: the underlying storage.\n        :return: a DocVec\n        \"\"\"\ndocs = cls.__new__(cls)\ndocs.tensor_type = storage.tensor_type\ndocs._storage = storage\nreturn docs\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, Iterable[T_doc]],\n) -&gt; T:\nif isinstance(value, cls):\nreturn value\nelif isinstance(value, DocList):\nif (\nsafe_issubclass(value.doc_type, cls.doc_type)\nor value.doc_type == cls.doc_type\n):\nreturn cast(T, value.to_doc_vec())\nelse:\nraise ValueError(f'DocVec[value.doc_type] is not compatible with {cls}')\nelif isinstance(value, DocList.__class_getitem__(cls.doc_type)):\nreturn cast(T, value.to_doc_vec())\nelif isinstance(value, Sequence):\nreturn cls(value)\nelif isinstance(value, Iterable):\nreturn cls(list(value))\nelse:\nraise TypeError(f'Expecting an Iterable of {cls.doc_type}')\ndef to(self: T, device: str) -&gt; T:\n\"\"\"Move all tensors of this DocVec to the given device\n        :param device: the device to move the data to\n        \"\"\"\nfor field, col_tens in self._storage.tensor_columns.items():\nif col_tens is not None:\nself._storage.tensor_columns[\nfield\n] = col_tens.get_comp_backend().to_device(col_tens, device)\nfor field, col_doc in self._storage.doc_columns.items():\nif col_doc is not None:\nself._storage.doc_columns[field] = col_doc.to(device)\nfor _, col_da in self._storage.docs_vec_columns.items():\nif col_da is not None:\nfor docs in col_da:\ndocs.to(device)\nreturn self\n################################################\n# Accessing data : Indexing / Getitem related  #\n################################################\n@overload\ndef __getitem__(self: T, item: int) -&gt; T_doc:\n...\n@overload\ndef __getitem__(self: T, item: IndexIterType) -&gt; T:\n...\ndef __getitem__(self: T, item: Union[int, IndexIterType]) -&gt; Union[T_doc, T]:\nif item is None:\nreturn self  # PyTorch behaviour\n# multiple docs case\nif isinstance(item, (slice, Iterable)):\nreturn self.__class__.from_columns_storage(self._storage[item])\n# single doc case\nreturn self.doc_type.from_view(ColumnStorageView(item, self._storage))\ndef _get_data_column(\nself: T,\nfield: str,\n) -&gt; Union[MutableSequence, 'DocVec', AbstractTensor, None]:\n\"\"\"Return one column of the data\n        :param field: name of the fields to extract\n        :return: Returns a list of the field value for each document\n        in the array like container\n        \"\"\"\nif field in self._storage.any_columns.keys():\nreturn self._storage.any_columns[field]\nelif field in self._storage.docs_vec_columns.keys():\nreturn self._storage.docs_vec_columns[field]\nelif field in self._storage.columns.keys():\nreturn self._storage.columns[field]\nelse:\nraise ValueError(f'{field} does not exist in {self}')\n####################################\n# Updating data : Setitem related  #\n####################################\n@overload\ndef __setitem__(self: T, key: int, value: T_doc):\n...\n@overload\ndef __setitem__(self: T, key: IndexIterType, value: T):\n...\n@no_type_check\ndef __setitem__(self: T, key, value):\n# single doc case\nif not isinstance(key, (slice, Iterable)):\nif not isinstance(value, self.doc_type):\nraise ValueError(f'{value} is not a {self.doc_type}')\nfor field, value in value.dict().items():\nself._storage.columns[field][key] = value  # todo we might want to\n# define a safety mechanism in someone put a wrong value\nelse:\n# multiple docs case\nself._set_data_and_columns(key, value)\ndef _set_data_and_columns(\nself: T,\nindex_item: Union[Tuple, Iterable, slice],\nvalue: Union[T, DocList[T_doc]],\n) -&gt; None:\n\"\"\"Delegates the setting to the data and the columns.\n        :param index_item: the key used as index. Needs to be a valid index for both\n            DocList (data) and column types (torch/tensorflow/numpy tensors)\n        :value: the value to set at the `key` location\n        \"\"\"\nif isinstance(index_item, tuple):\nindex_item = list(index_item)\n# set data and prepare columns\nprocessed_value: T\nif isinstance(value, DocList):\nif not safe_issubclass(value.doc_type, self.doc_type):\nraise TypeError(\nf'{value} schema : {value.doc_type} is not compatible with '\nf'this DocVec schema : {self.doc_type}'\n)\nprocessed_value = cast(\nT, value.to_doc_vec(tensor_type=self.tensor_type)\n)  # we need to copy data here\nelif isinstance(value, DocVec):\nif not safe_issubclass(value.doc_type, self.doc_type):\nraise TypeError(\nf'{value} schema : {value.doc_type} is not compatible with '\nf'this DocVec schema : {self.doc_type}'\n)\nprocessed_value = value\nelse:\nraise TypeError(f'Can not set a DocVec with {type(value)}')\nfor field, col in self._storage.columns.items():\ncol[index_item] = processed_value._storage.columns[field]\ndef _set_data_column(\nself: T,\nfield: str,\nvalues: Union[\nSequence[DocList[T_doc]],\nSequence[Any],\nT,\nDocList,\nAbstractTensor,\nNone,\n],\n) -&gt; None:\n\"\"\"Set all Documents in this DocList using the passed values\n        :param field: name of the fields to set\n        :values: the values to set at the DocList level\n        \"\"\"\nif values is None:\nif field in self._storage.tensor_columns.keys():\nself._storage.tensor_columns[field] = values\nelif field in self._storage.doc_columns.keys():\nself._storage.doc_columns[field] = values\nelif field in self._storage.docs_vec_columns.keys():\nself._storage.docs_vec_columns[field] = values\nelif field in self._storage.any_columns.keys():\nraise ValueError(\nf'column {field} cannot be set to None, try to pass '\nf'a list of None instead'\n)\nelse:\nraise ValueError(f'{field} does not exist in {self}')\nelse:\nif len(values) != len(self._storage):\nraise ValueError(\nf'{values} has not the right length, expected '\nf'{len(self._storage)} , got {len(values)}'\n)\nif field in self._storage.tensor_columns.keys():\ncol = self._storage.tensor_columns[field]\nif col is not None:\nvalidation_class = col.__unparametrizedcls__ or col.__class__\nelse:\nvalidation_class = self.doc_type._get_field_annotation(field)\n# TODO shape check should be handle by the tensor validation\nvalues = parse_obj_as(validation_class, values)\nself._storage.tensor_columns[field] = values\nelif field in self._storage.doc_columns.keys():\nvalues_ = parse_obj_as(\nDocVec.__class_getitem__(\nself.doc_type._get_field_annotation(field)\n),\nvalues,\n)\nself._storage.doc_columns[field] = values_\nelif field in self._storage.docs_vec_columns.keys():\nvalues_ = cast(Sequence[DocList[T_doc]], values)\n# TODO here we should actually check if this is correct\nself._storage.docs_vec_columns[field] = values_\nelif field in self._storage.any_columns.keys():\n# TODO here we should actually check if this is correct\nvalues_ = cast(Sequence, values)\nself._storage.any_columns[field] = values_\nelse:\nraise KeyError(f'{field} is not a valid field for this DocList')\n####################\n# Deleting data    #\n####################\ndef __delitem__(self, key: Union[int, IndexIterType]) -&gt; None:\nraise NotImplementedError(\nf'{self.__class__.__name__} does not implement '\nf'__del_item__. You are trying to delete an element'\nf'from {self.__class__.__name__} which is not '\nf'designed for this operation. Please `unstack`'\nf' before doing the deletion'\n)\n####################\n# Sequence related #\n####################\ndef __iter__(self):\nfor i in range(len(self)):\nyield self[i]\ndef __len__(self):\nreturn len(self._storage)\ndef __eq__(self, other: Any) -&gt; bool:\nif not isinstance(other, DocVec):\nreturn False\nif self.doc_type != other.doc_type:\nreturn False\nif self.tensor_type != other.tensor_type:\nreturn False\nif self._storage != other._storage:\nreturn False\nreturn True\n####################\n# IO related       #\n####################\n@classmethod\ndef _get_proto_class(cls: Type[T]):\nfrom docarray.proto import DocVecProto\nreturn DocVecProto\ndef _docarray_to_json_compatible(self) -&gt; Dict[str, Dict[str, Any]]:\ntup = self._storage.columns_json_compatible()\nreturn tup._asdict()\ndef to_doc_list(self: T) -&gt; DocList[T_doc]:\n\"\"\"Convert DocVec into a DocList.\n        Note this destroys the arguments and returns a new DocList\n        \"\"\"\nunstacked_doc_column: Dict[str, Optional[DocList]] = dict()\nunstacked_da_column: Dict[str, Optional[List[DocList]]] = dict()\nunstacked_tensor_column: Dict[str, Optional[List[AbstractTensor]]] = dict()\nunstacked_any_column = self._storage.any_columns\nfor field, doc_col in self._storage.doc_columns.items():\nunstacked_doc_column[field] = doc_col.to_doc_list() if doc_col else None\nfor field, da_col in self._storage.docs_vec_columns.items():\nunstacked_da_column[field] = (\n[docs.to_doc_list() for docs in da_col] if da_col else None\n)\nfor field, tensor_col in list(self._storage.tensor_columns.items()):\n# list is needed here otherwise we cannot delete the column\nif tensor_col is not None:\ntensors = list()\nfor tensor in tensor_col:\ntensor_copy = tensor.get_comp_backend().copy(tensor)\ntensors.append(tensor_copy)\nunstacked_tensor_column[field] = tensors\ndel self._storage.tensor_columns[field]\nunstacked_column = ChainMap(  # type: ignore\nunstacked_any_column,  # type: ignore\nunstacked_tensor_column,  # type: ignore\nunstacked_da_column,  # type: ignore\nunstacked_doc_column,  # type: ignore\n)  # type: ignore\ndocs = []\nfor i in range(len(self)):\ndata = {field: col[i] for field, col in unstacked_column.items()}\ndocs.append(self.doc_type.construct(**data))\ndel self._storage\ndoc_type = self.doc_type\n# Setting _is_unusable will raise an Exception if someone interacts with this instance from hereon out.\n# I don't like relying on this state, but we can't override the getattr/setattr directly:\n# https://stackoverflow.com/questions/10376604/overriding-special-methods-on-an-instance\nself._is_unusable = True\nreturn DocList.__class_getitem__(doc_type).construct(docs)\ndef traverse_flat(\nself,\naccess_path: str,\n) -&gt; Union[List[Any], 'TorchTensor', 'NdArray']:\nnodes = list(AnyDocArray._traverse(node=self, access_path=access_path))\nflattened = AnyDocArray._flatten_one_level(nodes)\ncls_to_check = (NdArray, TorchTensor) if TorchTensor is not None else (NdArray,)\nif len(flattened) == 1 and isinstance(flattened[0], cls_to_check):\nreturn flattened[0]\nelse:\nreturn flattened\n@classmethod\ndef __class_getitem__(cls, item: Union[Type[BaseDoc], TypeVar, str]):\n# call implementation in AnyDocArray\nreturn super(IOMixinDocVec, cls).__class_getitem__(item)\nif is_pydantic_v2:\n@classmethod\ndef __get_pydantic_core_schema__(\ncls, _source_type: Any, _handler: GetCoreSchemaHandler\n) -&gt; core_schema.CoreSchema:\nreturn core_schema.general_plain_validator_function(\ncls.validate,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_base64","title":"<code>from_base64(data, protocol='protobuf-array', compress=None, show_progress=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize base64 strings into a <code>DocVec</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Base64 string to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVEc</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocVec</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocVec`.\n    :param data: Base64 string to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param tensor_type: the tensor type of the resulting DocVEc\n    :return: the deserialized `DocVec`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_bytes","title":"<code>from_bytes(data, protocol='protobuf-array', compress=None, show_progress=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from which to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVec</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocVec</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n    :param data: Bytes from which to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param tensor_type: the tensor type of the resulting DocVec\n    :return: the deserialized `DocVec`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_columns_storage","title":"<code>from_columns_storage(storage)</code>  <code>classmethod</code>","text":"<p>Create a DocVec directly from a storage object</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>ColumnStorage</code> <p>the underlying storage.</p> required <p>Returns:</p> Type Description <code>T</code> <p>a DocVec</p> Source code in <code>docarray/array/doc_vec/doc_vec.py</code> <pre><code>@classmethod\ndef from_columns_storage(cls: Type[T], storage: ColumnStorage) -&gt; T:\n\"\"\"\n    Create a DocVec directly from a storage object\n    :param storage: the underlying storage.\n    :return: a DocVec\n    \"\"\"\ndocs = cls.__new__(cls)\ndocs.tensor_type = storage.tensor_type\ndocs._storage = storage\nreturn docs\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_csv","title":"<code>from_csv(file_path, encoding='utf-8', dialect='excel')</code>  <code>classmethod</code>","text":"<p>DocVec does not support <code>.from_csv()</code>. This is because CSV is a row-based format while DocVec has a column-based data layout. To overcome this, do: <code>DocList[MyDoc].from_csv(...).to_doc_vec()</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, 'csv.Dialect'] = 'excel',\n) -&gt; 'T':\n\"\"\"\n    DocVec does not support `.from_csv()`. This is because CSV is a row-based format\n    while DocVec has a column-based data layout.\n    To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.\n    \"\"\"\nraise NotImplementedError(\nf'{cls} does not support `.from_csv()`. This is because CSV is a row-based format while'\nf'{cls} has a column-based data layout. '\nf'To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.'\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_dataframe","title":"<code>from_dataframe(df, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Load a <code>DocVec</code> from a <code>pandas.DataFrame</code> following the schema defined in the <code>.doc_type</code> attribute. Every row of the dataframe will be mapped to one Document in the doc_vec. The column names of the dataframe have to match the field names of the Document type. For nested fields use \"__\"-separated access paths as column names, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <pre><code>import pandas as pd\nfrom docarray import BaseDoc, DocVec\nclass Person(BaseDoc):\nname: str\nfollower: int\ndf = pd.DataFrame(\ndata=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n)\ndocs = DocVec[Person].from_dataframe(df)\nassert docs.name == ['Maria', 'Jake']\nassert docs.follower == [12345, 54321]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p><code>pandas.DataFrame</code> to extract Document's information from</p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVec</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> where each Document contains the information of one corresponding row of the <code>pandas.DataFrame</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_dataframe(\ncls: Type['T'],\ndf: 'pd.DataFrame',\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; 'T':\n\"\"\"\n    Load a `DocVec` from a `pandas.DataFrame` following the schema\n    defined in the [`.doc_type`][docarray.DocVec] attribute.\n    Every row of the dataframe will be mapped to one Document in the doc_vec.\n    The column names of the dataframe have to match the field names of the\n    Document type.\n    For nested fields use \"__\"-separated access paths as column names,\n    such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    ---\n    ```python\n    import pandas as pd\n    from docarray import BaseDoc, DocVec\n    class Person(BaseDoc):\n        name: str\n        follower: int\n    df = pd.DataFrame(\n        data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n    )\n    docs = DocVec[Person].from_dataframe(df)\n    assert docs.name == ['Maria', 'Jake']\n    assert docs.follower == [12345, 54321]\n    ```\n    ---\n    :param df: `pandas.DataFrame` to extract Document's information from\n    :param tensor_type: the tensor type of the resulting DocVec\n    :return: `DocList` where each Document contains the information of one\n        corresponding row of the `pandas.DataFrame`.\n    \"\"\"\n# type ignore could be avoided by simply putting this implementation in the DocVec class\n# but leaving it here for code separation\nreturn cls(super().from_dataframe(df), tensor_type=tensor_type)  # type: ignore\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_json","title":"<code>from_json(file, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize JSON strings or bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, bytearray]</code> <p>JSON object from where to deserialize a <code>DocList</code></p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type to use for the tensor columns. Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray. All tensors of the output DocVec will be of this type.</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\ntensor_type: Type[AbstractTensor] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n    :param file: JSON object from where to deserialize a `DocList`\n    :param tensor_type: the tensor type to use for the tensor columns.\n        Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n        All tensors of the output DocVec will be of this type.\n    :return: the deserialized `DocList`\n    \"\"\"\njson_columns = orjson.loads(file)\nreturn cls._from_json_col_dict(json_columns, tensor_type=tensor_type)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.from_protobuf","title":"<code>from_protobuf(pb_msg, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>create a DocVec from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocVecProto</code> <p>the protobuf message to deserialize</p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type to use for the tensor columns. Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray. All tensors of the output DocVec will be of this type.</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>The deserialized DocVec</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_protobuf(\ncls: Type[T], pb_msg: 'DocVecProto', tensor_type: Type[AbstractTensor] = NdArray\n) -&gt; T:\n\"\"\"create a DocVec from a protobuf message\n    :param pb_msg: the protobuf message to deserialize\n    :param tensor_type: the tensor type to use for the tensor columns.\n        Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n        All tensors of the output DocVec will be of this type.\n    :return: The deserialized DocVec\n    \"\"\"\ntensor_columns: Dict[str, Optional[AbstractTensor]] = {}\ndoc_columns: Dict[str, Optional['DocVec']] = {}\ndocs_vec_columns: Dict[str, Optional[ListAdvancedIndexing['DocVec']]] = {}\nany_columns: Dict[str, ListAdvancedIndexing] = {}\nfor tens_col_name, tens_col_proto in pb_msg.tensor_columns.items():\nif _is_none_ndarray_proto(tens_col_proto):\n# handle values that were None before serialization\ntensor_columns[tens_col_name] = None\nelse:\ntensor_columns[tens_col_name] = tensor_type.from_protobuf(\ntens_col_proto\n)\nfor doc_col_name, doc_col_proto in pb_msg.doc_columns.items():\nif _is_none_docvec_proto(doc_col_proto):\n# handle values that were None before serialization\ndoc_columns[doc_col_name] = None\nelse:\ncol_doc_type: Type = cls.doc_type._get_field_annotation(doc_col_name)\ndoc_columns[doc_col_name] = cls.__class_getitem__(\ncol_doc_type\n).from_protobuf(doc_col_proto, tensor_type=tensor_type)\nfor docs_vec_col_name, docs_vec_col_proto in pb_msg.docs_vec_columns.items():\nvec_list: Optional[ListAdvancedIndexing]\nif _is_none_list_of_docvec_proto(docs_vec_col_proto):\n# handle values that were None before serialization\nvec_list = None\nelse:\nvec_list = ListAdvancedIndexing()\nfor doc_list_proto in docs_vec_col_proto.data:\ncol_doc_type = cls.doc_type._get_field_annotation(\ndocs_vec_col_name\n).doc_type\nvec_list.append(\ncls.__class_getitem__(col_doc_type).from_protobuf(\ndoc_list_proto, tensor_type=tensor_type\n)\n)\ndocs_vec_columns[docs_vec_col_name] = vec_list\nfor any_col_name, any_col_proto in pb_msg.any_columns.items():\nany_column: ListAdvancedIndexing = ListAdvancedIndexing()\nfor node_proto in any_col_proto.data:\ncontent = cls.doc_type._get_content_from_node_proto(\nnode_proto, any_col_name\n)\nany_column.append(content)\nany_columns[any_col_name] = any_column\nstorage = ColumnStorage(\ntensor_columns=tensor_columns,\ndoc_columns=doc_columns,\ndocs_vec_columns=docs_vec_columns,\nany_columns=any_columns,\ntensor_type=tensor_type,\n)\nreturn cls.from_columns_storage(storage)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.load_binary","title":"<code>load_binary(file, protocol='protobuf-array', compress=None, show_progress=False, streaming=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Load doc_vec elements from a compressed binary file.</p> <p>In case protocol is pickle the <code>Documents</code> are streamed from disk to save memory usage</p> <p>Note</p> <p>If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions. This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a string interpolation of the respective <code>protocol</code> and <code>compress</code> methods. For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be loaded assuming <code>protocol=protobuf</code> and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, Path, BufferedReader, _LazyRequestReader]</code> <p>File or filename or serialized bytes where the data is stored.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>streaming</code> <code>bool</code> <p>if <code>True</code> returns a generator over <code>Document</code> objects.</p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVEc</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>Union[T, Generator[T_doc, None, None]]</code> <p>a <code>DocVec</code> object</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_vec elements from a compressed binary file.\n    In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n    !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename or serialized bytes where the data is stored.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param streaming: if `True` returns a generator over `Document` objects.\n    :param tensor_type: the tensor type of the resulting DocVEc\n    :return: a `DocVec` object\n    \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx,\nload_protocol,\nload_compress,\nshow_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.save_binary","title":"<code>save_binary(file, protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Save DocList into a binary file.</p> <p>It will use the protocol to pick how to save the DocList. If used <code>picke-doc_list</code> and <code>protobuf-array</code> the DocList will be stored and compressed at complete level using <code>pickle</code> or <code>protobuf</code>. When using <code>protobuf</code> or <code>pickle</code> as protocol each Document in DocList will be stored individually and this would make it available for streaming.</p> <p>!!! note     If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions.     This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a     string interpolation of the respective <code>protocol</code> and <code>compress</code> methods.     For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be created using <code>protocol=protobuf</code>     and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>File or filename to which the data is saved.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def save_binary(\nself,\nfile: Union[str, pathlib.Path],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\n\"\"\"Save DocList into a binary file.\n    It will use the protocol to pick how to save the DocList.\n    If used `picke-doc_list` and `protobuf-array` the DocList will be stored\n    and compressed at complete level using `pickle` or `protobuf`.\n    When using `protobuf` or `pickle` as protocol each Document in DocList\n    will be stored individually and this would make it available for streaming.\n     !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be created using `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename to which the data is saved.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    \"\"\"\nif isinstance(file, io.BufferedWriter):\nfile_ctx = nullcontext(file)\nelse:\n_protocol, _compress = _protocol_and_compress_from_file_path(file)\nif _protocol is not None:\nprotocol = _protocol\nif _compress is not None:\ncompress = _compress\nfile_ctx = open(file, 'wb')\nself.to_bytes(\nprotocol=protocol,\ncompress=compress,\nfile_ctx=file_ctx,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.summary","title":"<code>summary()</code>","text":"<p>Print a summary of this <code>DocList</code> object and a summary of the schema of its Document type.</p> Source code in <code>docarray/array/any_array.py</code> <pre><code>def summary(self):\n\"\"\"\n    Print a summary of this [`DocList`][docarray.array.doc_list.doc_list.DocList] object and a summary of the schema of its\n    Document type.\n    \"\"\"\nDocArraySummary(self).summary()\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to","title":"<code>to(device)</code>","text":"<p>Move all tensors of this DocVec to the given device</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>the device to move the data to</p> required Source code in <code>docarray/array/doc_vec/doc_vec.py</code> <pre><code>def to(self: T, device: str) -&gt; T:\n\"\"\"Move all tensors of this DocVec to the given device\n    :param device: the device to move the data to\n    \"\"\"\nfor field, col_tens in self._storage.tensor_columns.items():\nif col_tens is not None:\nself._storage.tensor_columns[\nfield\n] = col_tens.get_comp_backend().to_device(col_tens, device)\nfor field, col_doc in self._storage.doc_columns.items():\nif col_doc is not None:\nself._storage.doc_columns[field] = col_doc.to(device)\nfor _, col_da in self._storage.docs_vec_columns.items():\nif col_da is not None:\nfor docs in col_da:\ndocs.to(device)\nreturn self\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_base64","title":"<code>to_base64(protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Serialize itself into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_base64(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; str:\n\"\"\"Serialize itself into base64 encoded string.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\ncompress=compress,\nprotocol=protocol,\nshow_progress=show_progress,\n)\nreturn base64.b64encode(bf.getvalue()).decode('utf-8')\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_bytes","title":"<code>to_bytes(protocol='protobuf-array', compress=None, file_ctx=None, show_progress=False)</code>","text":"<p>Serialize itself into <code>bytes</code>.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between : <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>file_ctx</code> <code>Optional[BinaryIO]</code> <p>File or filename or serialized bytes where the data is stored.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_bytes(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nfile_ctx: Optional[BinaryIO] = None,\nshow_progress: bool = False,\n) -&gt; Optional[bytes]:\n\"\"\"Serialize itself into `bytes`.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between : `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param file_ctx: File or filename or serialized bytes where the data is stored.\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith file_ctx or io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\nif isinstance(bf, io.BytesIO):\nreturn bf.getvalue()\nreturn None\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_csv","title":"<code>to_csv(file_path, dialect='excel')</code>","text":"<p>DocVec does not support <code>.to_csv()</code>. This is because CSV is a row-based format while DocVec has a column-based data layout. To overcome this, do: <code>doc_vec.to_doc_list().to_csv(...)</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>def to_csv(\nself, file_path: str, dialect: Union[str, 'csv.Dialect'] = 'excel'\n) -&gt; None:\n\"\"\"\n    DocVec does not support `.to_csv()`. This is because CSV is a row-based format\n    while DocVec has a column-based data layout.\n    To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.\n    \"\"\"\nraise NotImplementedError(\nf'{type(self)} does not support `.to_csv()`. This is because CSV is a row-based format'\nf'while {type(self)} has a column-based data layout. '\nf'To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.'\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Save a DocList to a <code>pandas.DataFrame</code>. The field names will be stored as column names. Each row of the dataframe corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas.DataFrame</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_dataframe(self) -&gt; 'pd.DataFrame':\n\"\"\"\n    Save a DocList to a `pandas.DataFrame`.\n    The field names will be stored as column names. Each row of the dataframe corresponds\n    to the information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :return: `pandas.DataFrame`\n    \"\"\"\nif TYPE_CHECKING:\nimport pandas as pd\nelse:\npd = import_library('pandas', raise_error=True)\nif self.doc_type == AnyDoc:\nraise TypeError(\n'DocList must be homogeneous to be converted to a DataFrame.'\n'There is no document schema defined. '\n'Please specify the DocList\\'s Document type using `DocList[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\ndf = pd.DataFrame(columns=fields)\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\ndoc_dict = {k: [v] for k, v in doc_dict.items()}\ndf = pd.concat([df, pd.DataFrame.from_dict(doc_dict)], ignore_index=True)\nreturn df\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_doc_list","title":"<code>to_doc_list()</code>","text":"<p>Convert DocVec into a DocList.</p> <p>Note this destroys the arguments and returns a new DocList</p> Source code in <code>docarray/array/doc_vec/doc_vec.py</code> <pre><code>def to_doc_list(self: T) -&gt; DocList[T_doc]:\n\"\"\"Convert DocVec into a DocList.\n    Note this destroys the arguments and returns a new DocList\n    \"\"\"\nunstacked_doc_column: Dict[str, Optional[DocList]] = dict()\nunstacked_da_column: Dict[str, Optional[List[DocList]]] = dict()\nunstacked_tensor_column: Dict[str, Optional[List[AbstractTensor]]] = dict()\nunstacked_any_column = self._storage.any_columns\nfor field, doc_col in self._storage.doc_columns.items():\nunstacked_doc_column[field] = doc_col.to_doc_list() if doc_col else None\nfor field, da_col in self._storage.docs_vec_columns.items():\nunstacked_da_column[field] = (\n[docs.to_doc_list() for docs in da_col] if da_col else None\n)\nfor field, tensor_col in list(self._storage.tensor_columns.items()):\n# list is needed here otherwise we cannot delete the column\nif tensor_col is not None:\ntensors = list()\nfor tensor in tensor_col:\ntensor_copy = tensor.get_comp_backend().copy(tensor)\ntensors.append(tensor_copy)\nunstacked_tensor_column[field] = tensors\ndel self._storage.tensor_columns[field]\nunstacked_column = ChainMap(  # type: ignore\nunstacked_any_column,  # type: ignore\nunstacked_tensor_column,  # type: ignore\nunstacked_da_column,  # type: ignore\nunstacked_doc_column,  # type: ignore\n)  # type: ignore\ndocs = []\nfor i in range(len(self)):\ndata = {field: col[i] for field, col in unstacked_column.items()}\ndocs.append(self.doc_type.construct(**data))\ndel self._storage\ndoc_type = self.doc_type\n# Setting _is_unusable will raise an Exception if someone interacts with this instance from hereon out.\n# I don't like relying on this state, but we can't override the getattr/setattr directly:\n# https://stackoverflow.com/questions/10376604/overriding-special-methods-on-an-instance\nself._is_unusable = True\nreturn DocList.__class_getitem__(doc_type).construct(docs)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_json","title":"<code>to_json()</code>","text":"<p>Convert the object into JSON bytes. Can be loaded via <code>.from_json</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON serialization of <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Convert the object into JSON bytes. Can be loaded via `.from_json`.\n    :return: JSON serialization of `DocList`\n    \"\"\"\nreturn orjson_dumps(self).decode('UTF-8')\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.doc_vec.DocVec.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert DocVec into a Protobuf message</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>def to_protobuf(self) -&gt; 'DocVecProto':\n\"\"\"Convert DocVec into a Protobuf message\"\"\"\nfrom docarray.proto import (\nDocVecProto,\nListOfAnyProto,\nListOfDocArrayProto,\nListOfDocVecProto,\nNdArrayProto,\n)\nself_ = cast('DocVec', self)\ndoc_columns_proto: Dict[str, DocVecProto] = dict()\ntensor_columns_proto: Dict[str, NdArrayProto] = dict()\nda_columns_proto: Dict[str, ListOfDocArrayProto] = dict()\nany_columns_proto: Dict[str, ListOfAnyProto] = dict()\nfor field, col_doc in self_._storage.doc_columns.items():\nif col_doc is None:\n# put dummy empty DocVecProto for serialization\ndoc_columns_proto[field] = _none_docvec_proto()\nelse:\ndoc_columns_proto[field] = col_doc.to_protobuf()\nfor field, col_tens in self_._storage.tensor_columns.items():\nif col_tens is None:\n# put dummy empty NdArrayProto for serialization\ntensor_columns_proto[field] = _none_ndarray_proto()\nelse:\ntensor_columns_proto[field] = (\ncol_tens.to_protobuf() if col_tens is not None else None\n)\nfor field, col_da in self_._storage.docs_vec_columns.items():\nlist_proto = ListOfDocVecProto()\nif col_da:\nfor docs in col_da:\nlist_proto.data.append(docs.to_protobuf())\nelse:\n# put dummy empty ListOfDocVecProto for serialization\nlist_proto = _none_list_of_docvec_proto()\nda_columns_proto[field] = list_proto\nfor field, col_any in self_._storage.any_columns.items():\nlist_proto = ListOfAnyProto()\nfor data in col_any:\nlist_proto.data.append(_type_to_protobuf(data))\nany_columns_proto[field] = list_proto\nreturn DocVecProto(\ndoc_columns=doc_columns_proto,\ntensor_columns=tensor_columns_proto,\ndocs_vec_columns=da_columns_proto,\nany_columns=any_columns_proto,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec","title":"<code>docarray.array.doc_vec.io.IOMixinDocVec</code>","text":"<p>             Bases: <code>IOMixinDocList</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>class IOMixinDocVec(IOMixinDocList):\n@classmethod\n@abstractmethod\ndef from_columns_storage(cls: Type[T], storage: ColumnStorage) -&gt; T:\n...\n@classmethod\n@abstractmethod\ndef __class_getitem__(cls, item: Union[Type[BaseDoc], TypeVar, str]):\n...\n@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\ntensor_type: Type[AbstractTensor] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n        :param file: JSON object from where to deserialize a `DocList`\n        :param tensor_type: the tensor type to use for the tensor columns.\n            Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n            All tensors of the output DocVec will be of this type.\n        :return: the deserialized `DocList`\n        \"\"\"\njson_columns = orjson.loads(file)\nreturn cls._from_json_col_dict(json_columns, tensor_type=tensor_type)\n@classmethod\ndef _from_json_col_dict(\ncls: Type[T],\njson_columns: Dict[str, Any],\ntensor_type: Type[AbstractTensor] = NdArray,\n) -&gt; T:\ntensor_cols = json_columns['tensor_columns']\ndoc_cols = json_columns['doc_columns']\ndocs_vec_cols = json_columns['docs_vec_columns']\nany_cols = json_columns['any_columns']\nfor key, col in tensor_cols.items():\nif col is not None:\ntensor_cols[key] = parse_obj_as(tensor_type, col)\nelse:\ntensor_cols[key] = None\nfor key, col in doc_cols.items():\nif col is not None:\ncol_doc_type = cls.doc_type._get_field_annotation(key)\ndoc_cols[key] = cls.__class_getitem__(col_doc_type)._from_json_col_dict(\ncol, tensor_type=tensor_type\n)\nelse:\ndoc_cols[key] = None\nfor key, col in docs_vec_cols.items():\nif col is not None:\ncol_doc_type = cls.doc_type._get_field_annotation(key).doc_type\ncol_ = ListAdvancedIndexing(\ncls.__class_getitem__(col_doc_type)._from_json_col_dict(\nvec, tensor_type=tensor_type\n)\nfor vec in col\n)\ndocs_vec_cols[key] = col_\nelse:\ndocs_vec_cols[key] = None\nfor key, col in any_cols.items():\nif col is not None:\ncol_type = cls.doc_type._get_field_annotation(key)\nfield_required = (\ncls.doc_type._docarray_fields()[key].is_required()\nif is_pydantic_v2\nelse cls.doc_type._docarray_fields()[key].required\n)\ncol_type = col_type if field_required else Optional[col_type]\ncol_ = ListAdvancedIndexing(parse_obj_as(col_type, val) for val in col)\nany_cols[key] = col_\nelse:\nany_cols[key] = None\nreturn cls.from_columns_storage(\nColumnStorage(\ntensor_cols, doc_cols, docs_vec_cols, any_cols, tensor_type=tensor_type\n)\n)\n@classmethod\ndef from_protobuf(\ncls: Type[T], pb_msg: 'DocVecProto', tensor_type: Type[AbstractTensor] = NdArray\n) -&gt; T:\n\"\"\"create a DocVec from a protobuf message\n        :param pb_msg: the protobuf message to deserialize\n        :param tensor_type: the tensor type to use for the tensor columns.\n            Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n            All tensors of the output DocVec will be of this type.\n        :return: The deserialized DocVec\n        \"\"\"\ntensor_columns: Dict[str, Optional[AbstractTensor]] = {}\ndoc_columns: Dict[str, Optional['DocVec']] = {}\ndocs_vec_columns: Dict[str, Optional[ListAdvancedIndexing['DocVec']]] = {}\nany_columns: Dict[str, ListAdvancedIndexing] = {}\nfor tens_col_name, tens_col_proto in pb_msg.tensor_columns.items():\nif _is_none_ndarray_proto(tens_col_proto):\n# handle values that were None before serialization\ntensor_columns[tens_col_name] = None\nelse:\ntensor_columns[tens_col_name] = tensor_type.from_protobuf(\ntens_col_proto\n)\nfor doc_col_name, doc_col_proto in pb_msg.doc_columns.items():\nif _is_none_docvec_proto(doc_col_proto):\n# handle values that were None before serialization\ndoc_columns[doc_col_name] = None\nelse:\ncol_doc_type: Type = cls.doc_type._get_field_annotation(doc_col_name)\ndoc_columns[doc_col_name] = cls.__class_getitem__(\ncol_doc_type\n).from_protobuf(doc_col_proto, tensor_type=tensor_type)\nfor docs_vec_col_name, docs_vec_col_proto in pb_msg.docs_vec_columns.items():\nvec_list: Optional[ListAdvancedIndexing]\nif _is_none_list_of_docvec_proto(docs_vec_col_proto):\n# handle values that were None before serialization\nvec_list = None\nelse:\nvec_list = ListAdvancedIndexing()\nfor doc_list_proto in docs_vec_col_proto.data:\ncol_doc_type = cls.doc_type._get_field_annotation(\ndocs_vec_col_name\n).doc_type\nvec_list.append(\ncls.__class_getitem__(col_doc_type).from_protobuf(\ndoc_list_proto, tensor_type=tensor_type\n)\n)\ndocs_vec_columns[docs_vec_col_name] = vec_list\nfor any_col_name, any_col_proto in pb_msg.any_columns.items():\nany_column: ListAdvancedIndexing = ListAdvancedIndexing()\nfor node_proto in any_col_proto.data:\ncontent = cls.doc_type._get_content_from_node_proto(\nnode_proto, any_col_name\n)\nany_column.append(content)\nany_columns[any_col_name] = any_column\nstorage = ColumnStorage(\ntensor_columns=tensor_columns,\ndoc_columns=doc_columns,\ndocs_vec_columns=docs_vec_columns,\nany_columns=any_columns,\ntensor_type=tensor_type,\n)\nreturn cls.from_columns_storage(storage)\ndef to_protobuf(self) -&gt; 'DocVecProto':\n\"\"\"Convert DocVec into a Protobuf message\"\"\"\nfrom docarray.proto import (\nDocVecProto,\nListOfAnyProto,\nListOfDocArrayProto,\nListOfDocVecProto,\nNdArrayProto,\n)\nself_ = cast('DocVec', self)\ndoc_columns_proto: Dict[str, DocVecProto] = dict()\ntensor_columns_proto: Dict[str, NdArrayProto] = dict()\nda_columns_proto: Dict[str, ListOfDocArrayProto] = dict()\nany_columns_proto: Dict[str, ListOfAnyProto] = dict()\nfor field, col_doc in self_._storage.doc_columns.items():\nif col_doc is None:\n# put dummy empty DocVecProto for serialization\ndoc_columns_proto[field] = _none_docvec_proto()\nelse:\ndoc_columns_proto[field] = col_doc.to_protobuf()\nfor field, col_tens in self_._storage.tensor_columns.items():\nif col_tens is None:\n# put dummy empty NdArrayProto for serialization\ntensor_columns_proto[field] = _none_ndarray_proto()\nelse:\ntensor_columns_proto[field] = (\ncol_tens.to_protobuf() if col_tens is not None else None\n)\nfor field, col_da in self_._storage.docs_vec_columns.items():\nlist_proto = ListOfDocVecProto()\nif col_da:\nfor docs in col_da:\nlist_proto.data.append(docs.to_protobuf())\nelse:\n# put dummy empty ListOfDocVecProto for serialization\nlist_proto = _none_list_of_docvec_proto()\nda_columns_proto[field] = list_proto\nfor field, col_any in self_._storage.any_columns.items():\nlist_proto = ListOfAnyProto()\nfor data in col_any:\nlist_proto.data.append(_type_to_protobuf(data))\nany_columns_proto[field] = list_proto\nreturn DocVecProto(\ndoc_columns=doc_columns_proto,\ntensor_columns=tensor_columns_proto,\ndocs_vec_columns=da_columns_proto,\nany_columns=any_columns_proto,\n)\ndef to_csv(\nself, file_path: str, dialect: Union[str, 'csv.Dialect'] = 'excel'\n) -&gt; None:\n\"\"\"\n        DocVec does not support `.to_csv()`. This is because CSV is a row-based format\n        while DocVec has a column-based data layout.\n        To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.\n        \"\"\"\nraise NotImplementedError(\nf'{type(self)} does not support `.to_csv()`. This is because CSV is a row-based format'\nf'while {type(self)} has a column-based data layout. '\nf'To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.'\n)\n@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, 'csv.Dialect'] = 'excel',\n) -&gt; 'T':\n\"\"\"\n        DocVec does not support `.from_csv()`. This is because CSV is a row-based format\n        while DocVec has a column-based data layout.\n        To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.\n        \"\"\"\nraise NotImplementedError(\nf'{cls} does not support `.from_csv()`. This is because CSV is a row-based format while'\nf'{cls} has a column-based data layout. '\nf'To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.'\n)\n@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocVec`.\n        :param data: Base64 string to deserialize\n        :param protocol: protocol that was used to serialize\n        :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :param tensor_type: the tensor type of the resulting DocVEc\n        :return: the deserialized `DocVec`\n        \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n        :param data: Bytes from which to deserialize\n        :param protocol: protocol that was used to serialize\n        :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :param tensor_type: the tensor type of the resulting DocVec\n        :return: the deserialized `DocVec`\n        \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n@classmethod\ndef from_dataframe(\ncls: Type['T'],\ndf: 'pd.DataFrame',\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; 'T':\n\"\"\"\n        Load a `DocVec` from a `pandas.DataFrame` following the schema\n        defined in the [`.doc_type`][docarray.DocVec] attribute.\n        Every row of the dataframe will be mapped to one Document in the doc_vec.\n        The column names of the dataframe have to match the field names of the\n        Document type.\n        For nested fields use \"__\"-separated access paths as column names,\n        such as `'image__url'`.\n        List-like fields (including field of type DocList) are not supported.\n        ---\n        ```python\n        import pandas as pd\n        from docarray import BaseDoc, DocVec\n        class Person(BaseDoc):\n            name: str\n            follower: int\n        df = pd.DataFrame(\n            data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n        )\n        docs = DocVec[Person].from_dataframe(df)\n        assert docs.name == ['Maria', 'Jake']\n        assert docs.follower == [12345, 54321]\n        ```\n        ---\n        :param df: `pandas.DataFrame` to extract Document's information from\n        :param tensor_type: the tensor type of the resulting DocVec\n        :return: `DocList` where each Document contains the information of one\n            corresponding row of the `pandas.DataFrame`.\n        \"\"\"\n# type ignore could be avoided by simply putting this implementation in the DocVec class\n# but leaving it here for code separation\nreturn cls(super().from_dataframe(df), tensor_type=tensor_type)  # type: ignore\n@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_vec elements from a compressed binary file.\n        In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n        !!! note\n            If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n            This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n            string interpolation of the respective `protocol` and `compress` methods.\n            For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n            and `compress=lz4`.\n        :param file: File or filename or serialized bytes where the data is stored.\n        :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n        :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n        :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n        :param streaming: if `True` returns a generator over `Document` objects.\n        :param tensor_type: the tensor type of the resulting DocVEc\n        :return: a `DocVec` object\n        \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx,\nload_protocol,\nload_compress,\nshow_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_base64","title":"<code>from_base64(data, protocol='protobuf-array', compress=None, show_progress=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize base64 strings into a <code>DocVec</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Base64 string to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVEc</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocVec</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize base64 strings into a `DocVec`.\n    :param data: Base64 string to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compress algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param tensor_type: the tensor type of the resulting DocVEc\n    :return: the deserialized `DocVec`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(base64.b64decode(data)),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_bytes","title":"<code>from_bytes(data, protocol='protobuf-array', compress=None, show_progress=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Bytes from which to deserialize</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol that was used to serialize</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm that was used to serialize between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVec</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocVec</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize bytes into a `DocList`.\n    :param data: Bytes from which to deserialize\n    :param protocol: protocol that was used to serialize\n    :param compress: compression algorithm that was used to serialize between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param tensor_type: the tensor type of the resulting DocVec\n    :return: the deserialized `DocVec`\n    \"\"\"\nreturn cls._load_binary_all(\nfile_ctx=nullcontext(data),\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_csv","title":"<code>from_csv(file_path, encoding='utf-8', dialect='excel')</code>  <code>classmethod</code>","text":"<p>DocVec does not support <code>.from_csv()</code>. This is because CSV is a row-based format while DocVec has a column-based data layout. To overcome this, do: <code>DocList[MyDoc].from_csv(...).to_doc_vec()</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_csv(\ncls: Type['T'],\nfile_path: str,\nencoding: str = 'utf-8',\ndialect: Union[str, 'csv.Dialect'] = 'excel',\n) -&gt; 'T':\n\"\"\"\n    DocVec does not support `.from_csv()`. This is because CSV is a row-based format\n    while DocVec has a column-based data layout.\n    To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.\n    \"\"\"\nraise NotImplementedError(\nf'{cls} does not support `.from_csv()`. This is because CSV is a row-based format while'\nf'{cls} has a column-based data layout. '\nf'To overcome this, do: `DocList[MyDoc].from_csv(...).to_doc_vec()`.'\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_dataframe","title":"<code>from_dataframe(df, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Load a <code>DocVec</code> from a <code>pandas.DataFrame</code> following the schema defined in the <code>.doc_type</code> attribute. Every row of the dataframe will be mapped to one Document in the doc_vec. The column names of the dataframe have to match the field names of the Document type. For nested fields use \"__\"-separated access paths as column names, such as <code>'image__url'</code>.</p> <p>List-like fields (including field of type DocList) are not supported.</p> <pre><code>import pandas as pd\nfrom docarray import BaseDoc, DocVec\nclass Person(BaseDoc):\nname: str\nfollower: int\ndf = pd.DataFrame(\ndata=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n)\ndocs = DocVec[Person].from_dataframe(df)\nassert docs.name == ['Maria', 'Jake']\nassert docs.follower == [12345, 54321]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p><code>pandas.DataFrame</code> to extract Document's information from</p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVec</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p><code>DocList</code> where each Document contains the information of one corresponding row of the <code>pandas.DataFrame</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_dataframe(\ncls: Type['T'],\ndf: 'pd.DataFrame',\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; 'T':\n\"\"\"\n    Load a `DocVec` from a `pandas.DataFrame` following the schema\n    defined in the [`.doc_type`][docarray.DocVec] attribute.\n    Every row of the dataframe will be mapped to one Document in the doc_vec.\n    The column names of the dataframe have to match the field names of the\n    Document type.\n    For nested fields use \"__\"-separated access paths as column names,\n    such as `'image__url'`.\n    List-like fields (including field of type DocList) are not supported.\n    ---\n    ```python\n    import pandas as pd\n    from docarray import BaseDoc, DocVec\n    class Person(BaseDoc):\n        name: str\n        follower: int\n    df = pd.DataFrame(\n        data=[['Maria', 12345], ['Jake', 54321]], columns=['name', 'follower']\n    )\n    docs = DocVec[Person].from_dataframe(df)\n    assert docs.name == ['Maria', 'Jake']\n    assert docs.follower == [12345, 54321]\n    ```\n    ---\n    :param df: `pandas.DataFrame` to extract Document's information from\n    :param tensor_type: the tensor type of the resulting DocVec\n    :return: `DocList` where each Document contains the information of one\n        corresponding row of the `pandas.DataFrame`.\n    \"\"\"\n# type ignore could be avoided by simply putting this implementation in the DocVec class\n# but leaving it here for code separation\nreturn cls(super().from_dataframe(df), tensor_type=tensor_type)  # type: ignore\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_json","title":"<code>from_json(file, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Deserialize JSON strings or bytes into a <code>DocList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, bytearray]</code> <p>JSON object from where to deserialize a <code>DocList</code></p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type to use for the tensor columns. Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray. All tensors of the output DocVec will be of this type.</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>the deserialized <code>DocList</code></p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\nfile: Union[str, bytes, bytearray],\ntensor_type: Type[AbstractTensor] = NdArray,\n) -&gt; T:\n\"\"\"Deserialize JSON strings or bytes into a `DocList`.\n    :param file: JSON object from where to deserialize a `DocList`\n    :param tensor_type: the tensor type to use for the tensor columns.\n        Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n        All tensors of the output DocVec will be of this type.\n    :return: the deserialized `DocList`\n    \"\"\"\njson_columns = orjson.loads(file)\nreturn cls._from_json_col_dict(json_columns, tensor_type=tensor_type)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.from_protobuf","title":"<code>from_protobuf(pb_msg, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>create a DocVec from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocVecProto</code> <p>the protobuf message to deserialize</p> required <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type to use for the tensor columns. Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray. All tensors of the output DocVec will be of this type.</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>T</code> <p>The deserialized DocVec</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef from_protobuf(\ncls: Type[T], pb_msg: 'DocVecProto', tensor_type: Type[AbstractTensor] = NdArray\n) -&gt; T:\n\"\"\"create a DocVec from a protobuf message\n    :param pb_msg: the protobuf message to deserialize\n    :param tensor_type: the tensor type to use for the tensor columns.\n        Could be NdArray, TorchTensor, or TensorFlowTensor. Defaults to NdArray.\n        All tensors of the output DocVec will be of this type.\n    :return: The deserialized DocVec\n    \"\"\"\ntensor_columns: Dict[str, Optional[AbstractTensor]] = {}\ndoc_columns: Dict[str, Optional['DocVec']] = {}\ndocs_vec_columns: Dict[str, Optional[ListAdvancedIndexing['DocVec']]] = {}\nany_columns: Dict[str, ListAdvancedIndexing] = {}\nfor tens_col_name, tens_col_proto in pb_msg.tensor_columns.items():\nif _is_none_ndarray_proto(tens_col_proto):\n# handle values that were None before serialization\ntensor_columns[tens_col_name] = None\nelse:\ntensor_columns[tens_col_name] = tensor_type.from_protobuf(\ntens_col_proto\n)\nfor doc_col_name, doc_col_proto in pb_msg.doc_columns.items():\nif _is_none_docvec_proto(doc_col_proto):\n# handle values that were None before serialization\ndoc_columns[doc_col_name] = None\nelse:\ncol_doc_type: Type = cls.doc_type._get_field_annotation(doc_col_name)\ndoc_columns[doc_col_name] = cls.__class_getitem__(\ncol_doc_type\n).from_protobuf(doc_col_proto, tensor_type=tensor_type)\nfor docs_vec_col_name, docs_vec_col_proto in pb_msg.docs_vec_columns.items():\nvec_list: Optional[ListAdvancedIndexing]\nif _is_none_list_of_docvec_proto(docs_vec_col_proto):\n# handle values that were None before serialization\nvec_list = None\nelse:\nvec_list = ListAdvancedIndexing()\nfor doc_list_proto in docs_vec_col_proto.data:\ncol_doc_type = cls.doc_type._get_field_annotation(\ndocs_vec_col_name\n).doc_type\nvec_list.append(\ncls.__class_getitem__(col_doc_type).from_protobuf(\ndoc_list_proto, tensor_type=tensor_type\n)\n)\ndocs_vec_columns[docs_vec_col_name] = vec_list\nfor any_col_name, any_col_proto in pb_msg.any_columns.items():\nany_column: ListAdvancedIndexing = ListAdvancedIndexing()\nfor node_proto in any_col_proto.data:\ncontent = cls.doc_type._get_content_from_node_proto(\nnode_proto, any_col_name\n)\nany_column.append(content)\nany_columns[any_col_name] = any_column\nstorage = ColumnStorage(\ntensor_columns=tensor_columns,\ndoc_columns=doc_columns,\ndocs_vec_columns=docs_vec_columns,\nany_columns=any_columns,\ntensor_type=tensor_type,\n)\nreturn cls.from_columns_storage(storage)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.load_binary","title":"<code>load_binary(file, protocol='protobuf-array', compress=None, show_progress=False, streaming=False, tensor_type=NdArray)</code>  <code>classmethod</code>","text":"<p>Load doc_vec elements from a compressed binary file.</p> <p>In case protocol is pickle the <code>Documents</code> are streamed from disk to save memory usage</p> <p>Note</p> <p>If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions. This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a string interpolation of the respective <code>protocol</code> and <code>compress</code> methods. For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be loaded assuming <code>protocol=protobuf</code> and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, bytes, Path, BufferedReader, _LazyRequestReader]</code> <p>File or filename or serialized bytes where the data is stored.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <code>streaming</code> <code>bool</code> <p>if <code>True</code> returns a generator over <code>Document</code> objects.</p> <code>False</code> <code>tensor_type</code> <code>Type[AbstractTensor]</code> <p>the tensor type of the resulting DocVEc</p> <code>NdArray</code> <p>Returns:</p> Type Description <code>Union[T, Generator[T_doc, None, None]]</code> <p>a <code>DocVec</code> object</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>@classmethod\ndef load_binary(\ncls: Type[T],\nfile: Union[str, bytes, pathlib.Path, io.BufferedReader, _LazyRequestReader],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\nstreaming: bool = False,\ntensor_type: Type['AbstractTensor'] = NdArray,\n) -&gt; Union[T, Generator['T_doc', None, None]]:\n\"\"\"Load doc_vec elements from a compressed binary file.\n    In case protocol is pickle the `Documents` are streamed from disk to save memory usage\n    !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be loaded assuming `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename or serialized bytes where the data is stored.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :param streaming: if `True` returns a generator over `Document` objects.\n    :param tensor_type: the tensor type of the resulting DocVEc\n    :return: a `DocVec` object\n    \"\"\"\nfile_ctx, load_protocol, load_compress = cls._get_file_context(\nfile, protocol, compress\n)\nif streaming:\nif load_protocol not in SINGLE_PROTOCOLS:\nraise ValueError(\nf'`streaming` is only available when using {\" or \".join(map(lambda x: f\"`{x}`\", SINGLE_PROTOCOLS))} as protocol, '\nf'got {load_protocol}'\n)\nelse:\nreturn cls._load_binary_stream(\nfile_ctx,\nprotocol=load_protocol,\ncompress=load_compress,\nshow_progress=show_progress,\n)\nelse:\nreturn cls._load_binary_all(\nfile_ctx,\nload_protocol,\nload_compress,\nshow_progress,\ntensor_type=tensor_type,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.save_binary","title":"<code>save_binary(file, protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Save DocList into a binary file.</p> <p>It will use the protocol to pick how to save the DocList. If used <code>picke-doc_list</code> and <code>protobuf-array</code> the DocList will be stored and compressed at complete level using <code>pickle</code> or <code>protobuf</code>. When using <code>protobuf</code> or <code>pickle</code> as protocol each Document in DocList will be stored individually and this would make it available for streaming.</p> <p>!!! note     If <code>file</code> is <code>str</code> it can specify <code>protocol</code> and <code>compress</code> as file extensions.     This functionality assumes <code>file=file_name.$protocol.$compress</code> where <code>$protocol</code> and <code>$compress</code> refer to a     string interpolation of the respective <code>protocol</code> and <code>compress</code> methods.     For example if <code>file=my_docarray.protobuf.lz4</code> then the binary data will be created using <code>protocol=protobuf</code>     and <code>compress=lz4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path]</code> <p>File or filename to which the data is saved.</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def save_binary(\nself,\nfile: Union[str, pathlib.Path],\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; None:\n\"\"\"Save DocList into a binary file.\n    It will use the protocol to pick how to save the DocList.\n    If used `picke-doc_list` and `protobuf-array` the DocList will be stored\n    and compressed at complete level using `pickle` or `protobuf`.\n    When using `protobuf` or `pickle` as protocol each Document in DocList\n    will be stored individually and this would make it available for streaming.\n     !!! note\n        If `file` is `str` it can specify `protocol` and `compress` as file extensions.\n        This functionality assumes `file=file_name.$protocol.$compress` where `$protocol` and `$compress` refer to a\n        string interpolation of the respective `protocol` and `compress` methods.\n        For example if `file=my_docarray.protobuf.lz4` then the binary data will be created using `protocol=protobuf`\n        and `compress=lz4`.\n    :param file: File or filename to which the data is saved.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    \"\"\"\nif isinstance(file, io.BufferedWriter):\nfile_ctx = nullcontext(file)\nelse:\n_protocol, _compress = _protocol_and_compress_from_file_path(file)\nif _protocol is not None:\nprotocol = _protocol\nif _compress is not None:\ncompress = _compress\nfile_ctx = open(file, 'wb')\nself.to_bytes(\nprotocol=protocol,\ncompress=compress,\nfile_ctx=file_ctx,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_base64","title":"<code>to_base64(protocol='protobuf-array', compress=None, show_progress=False)</code>","text":"<p>Serialize itself into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_base64(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nshow_progress: bool = False,\n) -&gt; str:\n\"\"\"Serialize itself into base64 encoded string.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\ncompress=compress,\nprotocol=protocol,\nshow_progress=show_progress,\n)\nreturn base64.b64encode(bf.getvalue()).decode('utf-8')\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_bytes","title":"<code>to_bytes(protocol='protobuf-array', compress=None, file_ctx=None, show_progress=False)</code>","text":"<p>Serialize itself into <code>bytes</code>.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'</p> <code>'protobuf-array'</code> <code>compress</code> <code>Optional[str]</code> <p>compress algorithm to use between : <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, <code>gzip</code></p> <code>None</code> <code>file_ctx</code> <code>Optional[BinaryIO]</code> <p>File or filename or serialized bytes where the data is stored.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show progress bar, only works when protocol is <code>pickle</code> or <code>protobuf</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[bytes]</code> <p>the binary serialization in bytes or None if file_ctx is passed where to store</p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_bytes(\nself,\nprotocol: ProtocolType = 'protobuf-array',\ncompress: Optional[str] = None,\nfile_ctx: Optional[BinaryIO] = None,\nshow_progress: bool = False,\n) -&gt; Optional[bytes]:\n\"\"\"Serialize itself into `bytes`.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle-array', 'protobuf-array', 'pickle' or 'protobuf'\n    :param compress: compress algorithm to use between : `lz4`, `bz2`, `lzma`, `zlib`, `gzip`\n    :param file_ctx: File or filename or serialized bytes where the data is stored.\n    :param show_progress: show progress bar, only works when protocol is `pickle` or `protobuf`\n    :return: the binary serialization in bytes or None if file_ctx is passed where to store\n    \"\"\"\nwith file_ctx or io.BytesIO() as bf:\nself._write_bytes(\nbf=bf,\nprotocol=protocol,\ncompress=compress,\nshow_progress=show_progress,\n)\nif isinstance(bf, io.BytesIO):\nreturn bf.getvalue()\nreturn None\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_csv","title":"<code>to_csv(file_path, dialect='excel')</code>","text":"<p>DocVec does not support <code>.to_csv()</code>. This is because CSV is a row-based format while DocVec has a column-based data layout. To overcome this, do: <code>doc_vec.to_doc_list().to_csv(...)</code>.</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>def to_csv(\nself, file_path: str, dialect: Union[str, 'csv.Dialect'] = 'excel'\n) -&gt; None:\n\"\"\"\n    DocVec does not support `.to_csv()`. This is because CSV is a row-based format\n    while DocVec has a column-based data layout.\n    To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.\n    \"\"\"\nraise NotImplementedError(\nf'{type(self)} does not support `.to_csv()`. This is because CSV is a row-based format'\nf'while {type(self)} has a column-based data layout. '\nf'To overcome this, do: `doc_vec.to_doc_list().to_csv(...)`.'\n)\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Save a DocList to a <code>pandas.DataFrame</code>. The field names will be stored as column names. Each row of the dataframe corresponds to the information of one Document. Columns for nested fields will be named after the \"__\"-seperated access paths, such as <code>'image__url'</code> for <code>image.url</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas.DataFrame</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_dataframe(self) -&gt; 'pd.DataFrame':\n\"\"\"\n    Save a DocList to a `pandas.DataFrame`.\n    The field names will be stored as column names. Each row of the dataframe corresponds\n    to the information of one Document.\n    Columns for nested fields will be named after the \"__\"-seperated access paths,\n    such as `'image__url'` for `image.url`.\n    :return: `pandas.DataFrame`\n    \"\"\"\nif TYPE_CHECKING:\nimport pandas as pd\nelse:\npd = import_library('pandas', raise_error=True)\nif self.doc_type == AnyDoc:\nraise TypeError(\n'DocList must be homogeneous to be converted to a DataFrame.'\n'There is no document schema defined. '\n'Please specify the DocList\\'s Document type using `DocList[MyDoc]`.'\n)\nfields = self.doc_type._get_access_paths()\ndf = pd.DataFrame(columns=fields)\nfor doc in self:\ndoc_dict = _dict_to_access_paths(doc.dict())\ndoc_dict = {k: [v] for k, v in doc_dict.items()}\ndf = pd.concat([df, pd.DataFrame.from_dict(doc_dict)], ignore_index=True)\nreturn df\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_json","title":"<code>to_json()</code>","text":"<p>Convert the object into JSON bytes. Can be loaded via <code>.from_json</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON serialization of <code>DocList</code></p> Source code in <code>docarray/array/doc_list/io.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Convert the object into JSON bytes. Can be loaded via `.from_json`.\n    :return: JSON serialization of `DocList`\n    \"\"\"\nreturn orjson_dumps(self).decode('UTF-8')\n</code></pre>"},{"location":"API_reference/array/da_stack/#docarray.array.doc_vec.io.IOMixinDocVec.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert DocVec into a Protobuf message</p> Source code in <code>docarray/array/doc_vec/io.py</code> <pre><code>def to_protobuf(self) -&gt; 'DocVecProto':\n\"\"\"Convert DocVec into a Protobuf message\"\"\"\nfrom docarray.proto import (\nDocVecProto,\nListOfAnyProto,\nListOfDocArrayProto,\nListOfDocVecProto,\nNdArrayProto,\n)\nself_ = cast('DocVec', self)\ndoc_columns_proto: Dict[str, DocVecProto] = dict()\ntensor_columns_proto: Dict[str, NdArrayProto] = dict()\nda_columns_proto: Dict[str, ListOfDocArrayProto] = dict()\nany_columns_proto: Dict[str, ListOfAnyProto] = dict()\nfor field, col_doc in self_._storage.doc_columns.items():\nif col_doc is None:\n# put dummy empty DocVecProto for serialization\ndoc_columns_proto[field] = _none_docvec_proto()\nelse:\ndoc_columns_proto[field] = col_doc.to_protobuf()\nfor field, col_tens in self_._storage.tensor_columns.items():\nif col_tens is None:\n# put dummy empty NdArrayProto for serialization\ntensor_columns_proto[field] = _none_ndarray_proto()\nelse:\ntensor_columns_proto[field] = (\ncol_tens.to_protobuf() if col_tens is not None else None\n)\nfor field, col_da in self_._storage.docs_vec_columns.items():\nlist_proto = ListOfDocVecProto()\nif col_da:\nfor docs in col_da:\nlist_proto.data.append(docs.to_protobuf())\nelse:\n# put dummy empty ListOfDocVecProto for serialization\nlist_proto = _none_list_of_docvec_proto()\nda_columns_proto[field] = list_proto\nfor field, col_any in self_._storage.any_columns.items():\nlist_proto = ListOfAnyProto()\nfor data in col_any:\nlist_proto.data.append(_type_to_protobuf(data))\nany_columns_proto[field] = list_proto\nreturn DocVecProto(\ndoc_columns=doc_columns_proto,\ntensor_columns=tensor_columns_proto,\ndocs_vec_columns=da_columns_proto,\nany_columns=any_columns_proto,\n)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/","title":"BaseDoc","text":""},{"location":"API_reference/base_doc/base_doc/#basedoc","title":"BaseDoc","text":""},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc","title":"<code>docarray.base_doc.doc.BaseDoc</code>","text":"<p>             Bases: <code>BaseDocWithoutId</code></p> <p>BaseDoc is the base class for all Documents. This class should be subclassed to create new Document types with a specific schema.</p> <p>The schema of a Document is defined by the fields of the class.</p> <p>Example: </p><pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray, ImageUrl\nimport numpy as np\nclass MyDoc(BaseDoc):\nembedding: NdArray[512]\nimage: ImageUrl\ndoc = MyDoc(embedding=np.zeros(512), image='https://example.com/image.jpg')\n</code></pre> <p>BaseDoc is a subclass of pydantic.BaseModel and can be used in a similar way.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>class BaseDoc(BaseDocWithoutId):\n\"\"\"\n    BaseDoc is the base class for all Documents. This class should be subclassed\n    to create new Document types with a specific schema.\n    The schema of a Document is defined by the fields of the class.\n    Example:\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import NdArray, ImageUrl\n    import numpy as np\n    class MyDoc(BaseDoc):\n        embedding: NdArray[512]\n        image: ImageUrl\n    doc = MyDoc(embedding=np.zeros(512), image='https://example.com/image.jpg')\n    ```\n    BaseDoc is a subclass of [pydantic.BaseModel](\n    https://docs.pydantic.dev/usage/models/) and can be used in a similar way.\n    \"\"\"\nid: Optional[ID] = Field(\ndescription='The ID of the BaseDoc. This is useful for indexing in vector stores. If not set by user, it will automatically be assigned a random value',\ndefault_factory=lambda: ID(os.urandom(16).hex()),\nexample=os.urandom(16).hex(),\n)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.doc.BaseDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin","title":"<code>docarray.base_doc.mixins.io.IOMixin</code>","text":"<p>             Bases: <code>Iterable[Tuple[str, Any]]</code></p> <p>IOMixin to define all the bytes/protobuf/json related part of BaseDoc</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>class IOMixin(Iterable[Tuple[str, Any]]):\n\"\"\"\n    IOMixin to define all the bytes/protobuf/json related part of BaseDoc\n    \"\"\"\n_docarray_fields: Dict[str, 'FieldInfo']\nclass Config:\n_load_extra_fields_from_protobuf: bool\n@classmethod\n@abstractmethod\ndef _get_field_annotation(cls, field: str) -&gt; Type:\n...\n@classmethod\ndef _get_field_annotation_array(cls, field: str) -&gt; Type:\nreturn cls._get_field_annotation(field)\ndef __bytes__(self) -&gt; bytes:\nreturn self.to_bytes()\ndef to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n        For more Pythonic code, please use ``bytes(...)``.\n        :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n        :param compress: compression algorithm to use\n        :return: the binary serialization in bytes\n        \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n        :param data: binary bytes\n        :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n        :param compress: compress method to use\n        :return: a Document object\n        \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\ndef to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n        :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n        :param compress: compress method to use\n        :return: a base64 encoded string\n        \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n        :param data: a base64 encoded string\n        :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n        :param compress: compress method to use\n        :return: a Document object\n        \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n        :param pb_msg: the proto message of the Document\n        :return: a Document initialize with the proto data\n        \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n@classmethod\ndef _get_content_from_node_proto(\ncls,\nvalue: 'NodeProto',\nfield_name: Optional[str] = None,\nfield_type: Optional[Type] = None,\n) -&gt; Any:\n\"\"\"\n        load the proto data from a node proto\n        :param value: the proto node value\n        :param field_name: the name of the field\n        :return: the loaded field\n        \"\"\"\nif field_name is not None and field_type is not None:\nraise ValueError(\"field_type and field_name cannot be both passed\")\nfield_type = field_type or (\ncls._get_field_annotation(field_name) if field_name else None\n)\ncontent_type_dict = _PROTO_TYPE_NAME_TO_CLASS\ncontent_key = value.WhichOneof('content')\ndocarray_type = (\nvalue.type if value.WhichOneof('docarray_type') is not None else None\n)\nreturn_field: Any\nif docarray_type in content_type_dict:\nreturn_field = content_type_dict[docarray_type].from_protobuf(\ngetattr(value, content_key)\n)\nelif content_key == 'doc':\nif field_type is None:\nraise ValueError(\n'field_type cannot be None when trying to deserialize a BaseDoc'\n)\ntry:\nreturn_field = field_type.from_protobuf(\ngetattr(value, content_key)\n)  # we get to the parent class\nexcept Exception:\nif get_origin(field_type) is Union:\nraise ValueError(\n'Union type is not supported for proto deserialization. Please use JSON serialization instead'\n)\nraise ValueError(\nf'{field_type} is not supported for proto deserialization'\n)\nelif content_key == 'doc_array':\nif field_type is not None and field_name is None:\nreturn_field = field_type.from_protobuf(getattr(value, content_key))\nelif field_name is not None:\nreturn_field = cls._get_field_annotation_array(\nfield_name\n).from_protobuf(\ngetattr(value, content_key)\n)  # we get to the parent class\nelse:\nraise ValueError(\n'field_name and field_type cannot be None when trying to deserialize a DocArray'\n)\nelif content_key is None:\nreturn_field = None\nelif docarray_type is None:\narg_to_container: Dict[str, Callable] = {\n'list': list,\n'set': set,\n'tuple': tuple,\n}\nif content_key in ['text', 'blob', 'integer', 'float', 'boolean']:\nreturn_field = getattr(value, content_key)\nelif content_key in arg_to_container.keys():\nif field_name and field_name in cls._docarray_fields():\nfield_type = cls._get_field_inner_type(field_name)\nif isinstance(field_type, GenericAlias):\nfield_type = get_args(field_type)[0]\nreturn_field = arg_to_container[content_key](\ncls._get_content_from_node_proto(node, field_type=field_type)\nfor node in getattr(value, content_key).data\n)\nelif content_key == 'dict':\ndeser_dict: Dict[str, Any] = dict()\nif field_name and field_name in cls._docarray_fields():\nif is_pydantic_v2:\ndict_args = get_args(\ncls._docarray_fields()[field_name].annotation\n)\nif len(dict_args) &lt; 2:\nfield_type = Any\nelse:\nfield_type = dict_args[1]\nelse:\nfield_type = cls._docarray_fields()[field_name].type_\nelse:\nfield_type = None\nfor key_name, node in value.dict.data.items():\ndeser_dict[key_name] = cls._get_content_from_node_proto(\nnode, field_type=field_type\n)\nreturn_field = deser_dict\nelse:\nraise ValueError(\nf'key {content_key} is not supported for deserialization'\n)\nelse:\nraise ValueError(\nf'type {docarray_type}, with key {content_key} is not supported for'\nf' deserialization'\n)\nreturn return_field\ndef to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n        :return: the protobuf message\n        \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\ndef _to_node_protobuf(self) -&gt; 'NodeProto':\n\"\"\"Convert Document into a NodeProto protobuf message. This function should be\n        called when the Document is nest into another Document that need to be\n        converted into a protobuf\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nreturn NodeProto(doc=self.to_protobuf())\n@classmethod\ndef _get_access_paths(cls) -&gt; List[str]:\n\"\"\"\n        Get \"__\"-separated access paths of all fields, including nested ones.\n        :return: list of all access paths\n        \"\"\"\nfrom docarray import BaseDoc\npaths = []\nfor field in cls._docarray_fields().keys():\nfield_type = cls._get_field_annotation(field)\nif not is_union_type(field_type) and safe_issubclass(field_type, BaseDoc):\nsub_paths = field_type._get_access_paths()\nfor path in sub_paths:\npaths.append(f'{field}__{path}')\nelse:\npaths.append(field)\nreturn paths\n@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n        :return: a Document object\n        \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.io.IOMixin.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.update.UpdateMixin","title":"<code>docarray.base_doc.mixins.update.UpdateMixin</code>","text":"Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>class UpdateMixin:\n_docarray_fields: Dict[str, 'ModelField']\ndef _get_string_for_regex_filter(self):\nreturn str(self)\n@classmethod\n@abstractmethod\ndef _get_field_annotation(cls, field: str) -&gt; Type['UpdateMixin']:\n...\ndef update(self, other: T):\n\"\"\"\n        Updates self with the content of other. Changes are applied to self.\n        Updating one Document with another consists in the following:\n         - Setting data properties of the second Document to the first Document\n         if they are not None\n         - Concatenating lists and updating sets\n         - Updating recursively Documents and DocLists\n         - Updating Dictionaries of the left with the right\n        It behaves as an update operation for Dictionaries, except that since\n        it is applied to a static schema type, the presence of the field is\n        given by the field not having a None value and that DocLists,\n        lists and sets are concatenated. It is worth mentioning that Tuples\n        are not merged together since they are meant to be immutable,\n        so they behave as regular types and the value of `self` is updated\n        with the value of `other`.\n        ---\n        ```python\n        from typing import List, Optional\n        from docarray import BaseDoc\n        class MyDocument(BaseDoc):\n            content: str\n            title: Optional[str] = None\n            tags_: List\n        doc1 = MyDocument(\n            content='Core content of the document', title='Title', tags_=['python', 'AI']\n        )\n        doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n        doc1.update(doc2)\n        assert doc1.content == 'Core content updated'\n        assert doc1.title == 'Title'\n        assert doc1.tags_ == ['python', 'AI', 'docarray']\n        ```\n        ---\n        :param other: The Document with which to update the contents of this\n        \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/base_doc/base_doc/#docarray.base_doc.mixins.update.UpdateMixin.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/data/data/","title":"TorchDataset","text":""},{"location":"API_reference/data/data/#torchdataset","title":"TorchDataset","text":""},{"location":"API_reference/data/data/#docarray.data.torch_dataset.MultiModalDataset","title":"<code>docarray.data.torch_dataset.MultiModalDataset</code>","text":"<p>             Bases: <code>Dataset</code>, <code>Generic[T_doc]</code></p> <p>A dataset that can be used inside a PyTorch DataLoader. In other words, it implements the PyTorch Dataset interface.</p> <p>The preprocessing dictionary passed to the constructor consists of keys that are field names and values that are functions that take a single argument and return a single argument.</p> <pre><code>from torch.utils.data import DataLoader\nfrom docarray import DocList\nfrom docarray.data import MultiModalDataset\nfrom docarray.documents import TextDoc\ndef prepend_number(text: str):\nreturn f\"Number {text}\"\ndocs = DocList[TextDoc](TextDoc(text=str(i)) for i in range(16))\nds = MultiModalDataset[TextDoc](docs, preprocessing={'text': prepend_number})\nloader = DataLoader(ds, batch_size=4, collate_fn=MultiModalDataset[TextDoc].collate_fn)\nfor batch in loader:\nprint(batch.text)\n</code></pre> <p>Nested fields can be accessed by using dot notation. The document itself can be accessed using the empty string as the key.</p> <p>Transformations that operate on reference types (such as Documents) can optionally not return a value.</p> <p>The transformations will be applied according to their order in the dictionary.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader\nfrom docarray import DocList, BaseDoc\nfrom docarray.data import MultiModalDataset\nfrom docarray.documents import TextDoc\nclass Thesis(BaseDoc):\ntitle: TextDoc\nclass Student(BaseDoc):\nthesis: Thesis\ndef embed_title(title: TextDoc):\ntitle.embedding = torch.ones(4)\ndef normalize_embedding(thesis: Thesis):\nthesis.title.embedding = thesis.title.embedding / thesis.title.embedding.norm()\ndef add_nonsense(student: Student):\nstudent.thesis.title.embedding = student.thesis.title.embedding + int(\nstudent.thesis.title.text\n)\ndocs = DocList[Student](Student(thesis=Thesis(title=str(i))) for i in range(16))\nds = MultiModalDataset[Student](\ndocs,\npreprocessing={\n\"thesis.title\": embed_title,\n\"thesis\": normalize_embedding,\n\"\": add_nonsense,\n},\n)\nloader = DataLoader(ds, batch_size=4, collate_fn=ds.collate_fn)\nfor batch in loader:\nprint(batch.thesis.title.embedding)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>DocList[T_doc]</code> <p>the <code>DocList</code> to be used as the dataset</p> required <code>preprocessing</code> <code>Dict[str, Callable]</code> <p>a dictionary of field names and preprocessing functions</p> required Source code in <code>docarray/data/torch_dataset.py</code> <pre><code>class MultiModalDataset(Dataset, Generic[T_doc]):\n\"\"\"\n    A dataset that can be used inside a PyTorch DataLoader.\n    In other words, it implements the PyTorch Dataset interface.\n    The preprocessing dictionary passed to the constructor consists of keys that are\n    field names and values that are functions that take a single argument and return\n    a single argument.\n    ---\n    ```python\n    from torch.utils.data import DataLoader\n    from docarray import DocList\n    from docarray.data import MultiModalDataset\n    from docarray.documents import TextDoc\n    def prepend_number(text: str):\n        return f\"Number {text}\"\n    docs = DocList[TextDoc](TextDoc(text=str(i)) for i in range(16))\n    ds = MultiModalDataset[TextDoc](docs, preprocessing={'text': prepend_number})\n    loader = DataLoader(ds, batch_size=4, collate_fn=MultiModalDataset[TextDoc].collate_fn)\n    for batch in loader:\n        print(batch.text)\n    ```\n    ---\n    Nested fields can be accessed by using dot notation.\n    The document itself can be accessed using the empty string as the key.\n    Transformations that operate on reference types (such as Documents) can optionally\n    not return a value.\n    The transformations will be applied according to their order in the dictionary.\n    ---\n    ```python\n    import torch\n    from torch.utils.data import DataLoader\n    from docarray import DocList, BaseDoc\n    from docarray.data import MultiModalDataset\n    from docarray.documents import TextDoc\n    class Thesis(BaseDoc):\n        title: TextDoc\n    class Student(BaseDoc):\n        thesis: Thesis\n    def embed_title(title: TextDoc):\n        title.embedding = torch.ones(4)\n    def normalize_embedding(thesis: Thesis):\n        thesis.title.embedding = thesis.title.embedding / thesis.title.embedding.norm()\n    def add_nonsense(student: Student):\n        student.thesis.title.embedding = student.thesis.title.embedding + int(\n            student.thesis.title.text\n        )\n    docs = DocList[Student](Student(thesis=Thesis(title=str(i))) for i in range(16))\n    ds = MultiModalDataset[Student](\n        docs,\n        preprocessing={\n            \"thesis.title\": embed_title,\n            \"thesis\": normalize_embedding,\n            \"\": add_nonsense,\n        },\n    )\n    loader = DataLoader(ds, batch_size=4, collate_fn=ds.collate_fn)\n    for batch in loader:\n        print(batch.thesis.title.embedding)\n    ```\n    ---\n    :param docs: the `DocList` to be used as the dataset\n    :param preprocessing: a dictionary of field names and preprocessing functions\n    \"\"\"\ndoc_type: Optional[Type[BaseDoc]] = None\n__typed_ds__: Dict[Type[BaseDoc], Type['MultiModalDataset']] = {}\ndef __init__(\nself, docs: 'DocList[T_doc]', preprocessing: Dict[str, Callable]\n) -&gt; None:\nself.docs = docs\nself._preprocessing = preprocessing\ndef __len__(self):\nreturn len(self.docs)\ndef __getitem__(self, item: int):\ndoc = self.docs[item].copy(deep=True)\nfor field, preprocess in self._preprocessing.items():\nif len(field) == 0:\ndoc = preprocess(doc) or doc\nelse:\nacc_path = field.split('.')\n_field_ref = doc\nfor attr in acc_path[:-1]:\n_field_ref = getattr(_field_ref, attr)\nattr = acc_path[-1]\nvalue = getattr(_field_ref, attr)\nsetattr(_field_ref, attr, preprocess(value) or value)\nreturn doc\n@classmethod\ndef collate_fn(cls, batch: List[T_doc]):\ndoc_type = cls.doc_type\nif doc_type:\nbatch_da = DocVec[doc_type](  # type: ignore\nbatch,\ntensor_type=TorchTensor,\n)\nelse:\nbatch_da = DocVec(batch, tensor_type=TorchTensor)\nreturn batch_da\n@classmethod\ndef __class_getitem__(cls, item: Type[BaseDoc]) -&gt; Type['MultiModalDataset']:\nif not safe_issubclass(item, BaseDoc):\nraise ValueError(\nf'{cls.__name__}[item] item should be a Document not a {item} '\n)\nif item not in cls.__typed_ds__:\nglobal _TypedDataset\nclass _TypedDataset(cls):  # type: ignore\ndoc_type = item\nchange_cls_name(\n_TypedDataset, f'{cls.__name__}[{item.__name__}]', globals()\n)\ncls.__typed_ds__[item] = _TypedDataset\nreturn cls.__typed_ds__[item]\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/","title":"DocIndex","text":""},{"location":"API_reference/doc_index/doc_index/#docindex","title":"DocIndex","text":""},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex","title":"<code>docarray.index.abstract.BaseDocIndex</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[TSchema]</code></p> <p>Abstract class for all Document Stores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>class BaseDocIndex(ABC, Generic[TSchema]):\n\"\"\"Abstract class for all Document Stores\"\"\"\n# the BaseDoc that defines the schema of the store\n# for subclasses this is filled automatically\n_schema: Optional[Type[BaseDoc]] = None\ndef __init__(self, db_config=None, subindex: bool = False, **kwargs):\nif self._schema is None:\nraise ValueError(\n'A DocumentIndex must be typed with a Document type.'\n'To do so, use the syntax: DocumentIndex[DocumentType]'\n)\nif subindex:\nclass _NewSchema(self._schema):  # type: ignore\nparent_id: Optional[ID] = None\nself._ori_schema = self._schema\nself._schema = cast(Type[BaseDoc], _NewSchema)\nself._logger = logging.getLogger('docarray')\nself._db_config = db_config or self.DBConfig(**kwargs)\nif not isinstance(self._db_config, self.DBConfig):\nraise ValueError(f'db_config must be of type {self.DBConfig}')\nself._logger.info('DB config created')\nself._runtime_config = self.RuntimeConfig()\nself._logger.info('Runtime config created')\nself._column_infos: Dict[str, _ColumnInfo] = self._create_column_infos(\nself._schema\n)\nself._is_subindex = subindex\nself._subindices: Dict[str, BaseDocIndex] = {}\nself._init_subindex()\n###############################################\n# Inner classes for query builder and configs #\n# Subclasses must subclass &amp; implement these  #\n###############################################\nclass QueryBuilder(ABC):\n@abstractmethod\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the DB specific query object.\n            The DB specific implementation can leverage self._queries to do so.\n            The output of this should be able to be passed to execute_query().\n            \"\"\"\n...\n# TODO support subindex in QueryBuilder\n# the methods below need to be implemented by subclasses\n# If, in your subclass, one of these is not usable in a query builder, but\n# can be called directly on the DocumentIndex, use `_raise_not_composable`.\n# If the method is not supported _at all_, use `_raise_not_supported`.\nfind = abstractmethod(lambda *args, **kwargs: ...)\nfilter = abstractmethod(lambda *args, **kwargs: ...)\ntext_search = abstractmethod(lambda *args, **kwargs: ...)\nfind_batched = abstractmethod(lambda *args, **kwargs: ...)\nfilter_batched = abstractmethod(lambda *args, **kwargs: ...)\ntext_search_batched = abstractmethod(lambda *args, **kwargs: ...)\n@dataclass\nclass DBConfig(ABC):\nindex_name: Optional[str] = None\n# default configurations for every column type\n# a dictionary from a column type (DB specific) to a dictionary\n# of default configurations for that type\n# These configs are used if no configs are specified in the `Field(...)`\n# of a field in the Document schema (`cls._schema`)\n# Example: `default_column_config['VARCHAR'] = {'length': 255}`\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(default_factory=dict)\n@dataclass\nclass RuntimeConfig(ABC):\npass\n@property\ndef index_name(self):\n\"\"\"Return the name of the index in the database.\"\"\"\n...\n#####################################\n# Abstract methods                  #\n# Subclasses must implement these   #\n#####################################\n@abstractmethod\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type,\n            or None if ``python_type`` is not supported.\n        \"\"\"\n...\n@abstractmethod\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\n\"\"\"index a document into the store\"\"\"\n# `column_to_data` is a dictionary from column name to a generator\n# that yields the data for that column.\n# If you want to work directly on documents, you can implement index() instead\n# If you implement index(), _index() only needs a dummy implementation.\n...\n@abstractmethod\ndef num_docs(self) -&gt; int:\n\"\"\"Return the number of indexed documents\"\"\"\n...\n@property\ndef _is_index_empty(self) -&gt; bool:\n\"\"\"\n        Check if index is empty by comparing the number of documents to zero.\n        :return: True if the index is empty, False otherwise.\n        \"\"\"\nreturn self.num_docs() == 0\n@abstractmethod\ndef _del_items(self, doc_ids: Sequence[str]):\n\"\"\"Delete Documents from the index.\n        :param doc_ids: ids to delete from the Document Store\n        \"\"\"\n...\n@abstractmethod\ndef _get_items(\nself, doc_ids: Sequence[str]\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\n\"\"\"Get Documents from the index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param doc_ids: ids to get from the Document index\n        :return: Sequence of Documents, sorted corresponding to the order of `doc_ids`. Duplicate `doc_ids` can be omitted in the output.\n        \"\"\"\n...\n@abstractmethod\ndef execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the database.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\n...\n@abstractmethod\ndef _doc_exists(self, doc_id: str) -&gt; bool:\n\"\"\"\n        Checks if a given document exists in the index.\n        :param doc_id: The id of a document to check.\n        :return: True if the document exists in the index, False otherwise.\n        \"\"\"\n...\n@abstractmethod\ndef _find(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\n\"\"\"Find documents in the index\n        :param query: query vector for KNN/ANN search. Has single axis.\n        :param limit: maximum number of documents to return per query\n        :param search_field: name of the field to search on\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\n# NOTE: in standard implementations,\n# `search_field` is equal to the column name to search on\n...\n@abstractmethod\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\n\"\"\"Find documents in the index\n        :param queries: query vectors for KNN/ANN search.\n            Has shape (batch_size, vector_dim)\n        :param limit: maximum number of documents to return\n        :param search_field: name of the field to search on\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\n...\n@abstractmethod\ndef _filter(\nself,\nfilter_query: Any,\nlimit: int,\n) -&gt; Union[DocList, List[Dict]]:\n\"\"\"Find documents in the index based on a filter query\n        :param filter_query: the DB specific filter query to execute\n        :param limit: maximum number of documents to return\n        :return: a DocList containing the documents that match the filter query\n        \"\"\"\n...\n@abstractmethod\ndef _filter_batched(\nself,\nfilter_queries: Any,\nlimit: int,\n) -&gt; Union[List[DocList], List[List[Dict]]]:\n\"\"\"Find documents in the index based on multiple filter queries.\n        Each query is considered individually, and results are returned per query.\n        :param filter_queries: the DB specific filter queries to execute\n        :param limit: maximum number of documents to return per query\n        :return: List of DocLists containing the documents that match the filter\n            queries\n        \"\"\"\n...\n@abstractmethod\ndef _text_search(\nself,\nquery: str,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\n\"\"\"Find documents in the index based on a text search query\n        :param query: The text to search for\n        :param limit: maximum number of documents to return\n        :param search_field: name of the field to search on\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\n# NOTE: in standard implementations,\n# `search_field` is equal to the column name to search on\n...\n@abstractmethod\ndef _text_search_batched(\nself,\nqueries: Sequence[str],\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\n\"\"\"Find documents in the index based on a text search query\n        :param queries: The texts to search for\n        :param limit: maximum number of documents to return per query\n        :param search_field: name of the field to search on\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\n# NOTE: in standard implementations,\n# `search_field` is equal to the column name to search on\n...\n####################################################\n# Optional overrides                               #\n# Subclasses may or may not need to change these #\n####################################################\ndef __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param key: id or ids to get from the Document index\n        \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\ndef __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param key: id or ids to delete from the Document index\n        \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\ndef __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n        Checks if a given document exists in the index.\n        :param item: The document to check.\n            It must be an instance of BaseDoc or its subclass.\n        :return: True if the document exists in the index, False otherwise.\n        \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\ndef configure(self, runtime_config=None, **kwargs):\n\"\"\"\n        Configure the DocumentIndex.\n        You can either pass a config object to `config` or pass individual config\n        parameters as keyword arguments.\n        If a configuration object is passed, it will replace the current configuration.\n        If keyword arguments are passed, they will update the current configuration.\n        :param runtime_config: the configuration to apply\n        :param kwargs: individual configuration parameters\n        \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\ndef index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n        !!! note\n            Passing a sequence of Documents that is not a DocList\n            (such as a List of Docs) comes at a performance penalty.\n            This is because the Index needs to check compatibility between itself and\n            the data. With a DocList as input this is a single check; for other inputs\n            compatibility needs to be checked for every Document individually.\n        :param docs: Documents to index.\n        \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n        :param query: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.)\n            with a single axis, or a Document\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n        :param query: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.)\n            with a single axis, or a Document\n        :param subindex: name of the subindex to search on\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return\n        :return: a named tuple containing root docs, subindex docs and scores\n        \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\ndef find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n        :param queries: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n            or a DocList.\n            If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return per query\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\ndef filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n        :param filter_query: the DB specific filter query to execute\n        :param limit: maximum number of documents to return\n        :return: a DocList containing the documents that match the filter query\n        \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\ndef filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n        :param filter_query: the DB specific filter query to execute\n        :param subindex: name of the subindex to search on\n        :param limit: maximum number of documents to return\n        :return: a DocList containing the subindex level documents that match the filter query\n        \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\ndef filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n        :param filter_queries: the DB specific filter query to execute\n        :param limit: maximum number of documents to return\n        :return: a DocList containing the documents that match the filter query\n        \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\ndef text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n        :param query: The text to search for\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n        :param queries: The texts to search for\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\ndef _filter_by_parent_id(self, id: str) -&gt; Optional[List[str]]:\n\"\"\"Filter the ids of the subindex documents given id of root document.\n        :param id: the root document id to filter by\n        :return: a list of ids of the subindex documents\n        \"\"\"\nreturn None\n##########################################################\n# Helper methods                                         #\n# These might be useful in your subclass implementation  #\n##########################################################\n@staticmethod\ndef _get_values_by_column(docs: Sequence[BaseDoc], col_name: str) -&gt; List[Any]:\n\"\"\"Get the value of a column of a document.\n        :param docs: The DocList to get the values from\n        :param col_name: The name of the column, e.g. 'text' or 'image__tensor'\n        :return: The value of the column of `doc`\n        \"\"\"\nleaf_vals = []\nfor doc in docs:\nif '__' in col_name:\nfields = col_name.split('__')\nleaf_doc: BaseDoc = doc\nfor f in fields[:-1]:\nleaf_doc = getattr(leaf_doc, f)\nleaf_vals.append(getattr(leaf_doc, fields[-1]))\nelse:\nleaf_vals.append(getattr(doc, col_name))\nreturn leaf_vals\n@staticmethod\ndef _transpose_col_value_dict(\ncol_value_dict: Mapping[str, Iterable[Any]]\n) -&gt; Generator[Dict[str, Any], None, None]:\n\"\"\"'Transpose' the output of `_get_col_value_dict()`: Yield rows of columns, where each row represent one Document.\n        Since a generator is returned, this process comes at negligible cost.\n        :param docs: The DocList to get the values from\n        :return: The `docs` flattened out as rows. Each row is a dictionary mapping from column name to value\n        \"\"\"\nreturn (dict(zip(col_value_dict, row)) for row in zip(*col_value_dict.values()))\ndef _get_col_value_dict(\nself, docs: Union[BaseDoc, Sequence[BaseDoc]]\n) -&gt; Dict[str, Generator[Any, None, None]]:\n\"\"\"\n        Get all data from a (sequence of) document(s), flattened out by column.\n        This can be seen as the transposed representation of `_get_rows()`.\n        :param docs: The document(s) to get the data from\n        :return: A dictionary mapping column names to a generator of values\n        \"\"\"\nif isinstance(docs, BaseDoc):\ndocs_seq: Sequence[BaseDoc] = [docs]\nelse:\ndocs_seq = docs\ndef _col_gen(col_name: str):\nreturn (\nself._to_numpy(\nself._get_values_by_column([doc], col_name)[0],\nallow_passthrough=True,\n)\nfor doc in docs_seq\n)\nreturn {col_name: _col_gen(col_name) for col_name in self._column_infos}\ndef _update_subindex_data(\nself,\ndocs: DocList[BaseDoc],\n):\n\"\"\"\n        Add `parent_id` to all sublevel documents.\n        :param docs: The document(s) to update the `parent_id` for\n        \"\"\"\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc in docs:\n_list = getattr(doc, field_name)\nfor i, nested_doc in enumerate(_list):\nnested_doc = self._subindices[field_name]._schema(  # type: ignore\n**nested_doc.__dict__\n)\nnested_doc.parent_id = doc.id\n_list[i] = nested_doc\n##################################################\n# Behind-the-scenes magic                        #\n# Subclasses should not need to implement these  #\n##################################################\ndef __class_getitem__(cls, item: Type[TSchema]):\nif not isinstance(item, type):\n# do nothing\n# enables use in static contexts with type vars, e.g. as type annotation\nreturn Generic.__class_getitem__.__func__(cls, item)\nif not safe_issubclass(item, BaseDoc):\nraise ValueError(\nf'{cls.__name__}[item] `item` should be a Document not a {item} '\n)\nclass _DocumentIndexTyped(cls):  # type: ignore\n_schema: Type[TSchema] = item\n_DocumentIndexTyped.__name__ = f'{cls.__name__}[{item.__name__}]'\n_DocumentIndexTyped.__qualname__ = f'{cls.__qualname__}[{item.__name__}]'\nreturn _DocumentIndexTyped\ndef build_query(self) -&gt; QueryBuilder:\n\"\"\"\n        Build a query for this DocumentIndex.\n        :return: a new `QueryBuilder` object for this DocumentIndex\n        \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n@classmethod\ndef _flatten_schema(\ncls, schema: Type[BaseDoc], name_prefix: str = ''\n) -&gt; List[Tuple[str, Type, 'ModelField']]:\n\"\"\"Flatten the schema of a Document into a list of column names and types.\n        Nested Documents are handled in a recursive manner by adding `'__'` as a prefix to the column name.\n        :param schema: The schema to flatten\n        :param name_prefix: prefix to append to the column names. Used for recursive calls to handle nesting.\n        :return: A list of column names, types, and fields\n        \"\"\"\nnames_types_fields: List[Tuple[str, Type, 'ModelField']] = []\nfor field_name, field_ in schema._docarray_fields().items():\nt_ = schema._get_field_annotation(field_name)\ninner_prefix = name_prefix + field_name + '__'\nif is_union_type(t_):\nunion_args = get_args(t_)\nif is_tensor_union(t_):\nnames_types_fields.append(\n(name_prefix + field_name, AbstractTensor, field_)\n)\nelif len(union_args) == 2 and type(None) in union_args:\n# simple \"Optional\" type, treat as special case:\n# treat as if it was a single non-optional type\nfor t_arg in union_args:\nif t_arg is not type(None):\nif safe_issubclass(t_arg, BaseDoc):\nnames_types_fields.extend(\ncls._flatten_schema(t_arg, name_prefix=inner_prefix)\n)\nelse:\nnames_types_fields.append(\n(name_prefix + field_name, t_arg, field_)\n)\nelse:\nraise ValueError(\nf'Union type {t_} is not supported. Only Union of subclasses of AbstractTensor or Union[type, None] are supported.'\n)\nelif safe_issubclass(t_, BaseDoc):\nnames_types_fields.extend(\ncls._flatten_schema(t_, name_prefix=inner_prefix)\n)\nelif safe_issubclass(t_, AbstractTensor):\nnames_types_fields.append(\n(name_prefix + field_name, AbstractTensor, field_)\n)\nelse:\nnames_types_fields.append((name_prefix + field_name, t_, field_))\nreturn names_types_fields\ndef _create_column_infos(self, schema: Type[BaseDoc]) -&gt; Dict[str, _ColumnInfo]:\n\"\"\"Collects information about every column that is implied by a given schema.\n        :param schema: The schema (subclass of BaseDoc) to analyze and parse\n            columns from\n        :returns: A dictionary mapping from column names to column information.\n        \"\"\"\ncolumn_infos: Dict[str, _ColumnInfo] = dict()\nfor field_name, type_, field_ in self._flatten_schema(schema):\n# Union types are handle in _flatten_schema\nif safe_issubclass(type_, AnyDocArray):\ncolumn_infos[field_name] = _ColumnInfo(\ndocarray_type=type_, db_type=None, config=dict(), n_dim=None\n)\nelse:\ncolumn_infos[field_name] = self._create_single_column(field_, type_)\nreturn column_infos\ndef _create_single_column(self, field: 'ModelField', type_: Type) -&gt; _ColumnInfo:\ncustom_config = (\nfield.json_schema_extra if is_pydantic_v2 else field.field_info.extra\n)\nif custom_config is None:\ncustom_config = dict()\nif 'col_type' in custom_config.keys():\ndb_type = custom_config['col_type']\ncustom_config.pop('col_type')\nif db_type not in self._db_config.default_column_config.keys():\nraise ValueError(\nf'The given col_type is not a valid db type: {db_type}'\n)\nelse:\ndb_type = self.python_type_to_db_type(type_)\nconfig = self._db_config.default_column_config[db_type].copy()\nconfig.update(custom_config)\n# parse n_dim from parametrized tensor type\nfield_type = field.annotation if is_pydantic_v2 else field.type_\nif (\nhasattr(field_type, '__docarray_target_shape__')\nand field_type.__docarray_target_shape__\n):\nif len(field_type.__docarray_target_shape__) == 1:\nn_dim = field_type.__docarray_target_shape__[0]\nelse:\nn_dim = field_type.__docarray_target_shape__\nelse:\nn_dim = None\nreturn _ColumnInfo(\ndocarray_type=type_, db_type=db_type, config=config, n_dim=n_dim\n)\ndef _init_subindex(\nself,\n):\n\"\"\"Initialize subindices if any column is subclass of AnyDocArray.\"\"\"\nfor col_name, col in self._column_infos.items():\nif safe_issubclass(col.docarray_type, AnyDocArray):\nsub_db_config = copy.deepcopy(self._db_config)\nsub_db_config.index_name = f'{self.index_name}__{col_name}'\nself._subindices[col_name] = self.__class__[col.docarray_type.doc_type](  # type: ignore\ndb_config=sub_db_config, subindex=True\n)\ndef _validate_docs(\nself, docs: Union[BaseDoc, Sequence[BaseDoc]]\n) -&gt; DocList[BaseDoc]:\n\"\"\"Validates Document against the schema of the Document Index.\n        For validation to pass, the schema of `docs` and the schema of the Document\n        Index need to evaluate to the same flattened columns.\n        If Validation fails, a ValueError is raised.\n        :param docs: Document to evaluate. If this is a DocList, validation is\n            performed using its `doc_type` (parametrization), without having to check\n            ever Document in `docs`. If this check fails, or if `docs` is not a\n            DocList, evaluation is performed for every Document in `docs`.\n        :return: A DocList containing the Documents in `docs`\n        \"\"\"\nif isinstance(docs, BaseDoc):\ndocs = [docs]\nif isinstance(docs, DocList):\n# validation shortcut for DocList; only look at the schema\nreference_schema_flat = self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n)\nreference_names = [name for (name, _, _) in reference_schema_flat]\nreference_types = [t_ for (_, t_, _) in reference_schema_flat]\ntry:\ninput_schema_flat = self._flatten_schema(docs.doc_type)\nexcept ValueError:\npass\nelse:\ninput_names = [name for (name, _, _) in input_schema_flat]\ninput_types = [t_ for (_, t_, _) in input_schema_flat]\n# this could be relaxed in the future,\n# see schema translation ideas in the design doc\nnames_compatible = reference_names == input_names\ntypes_compatible = all(\n(safe_issubclass(t2, t1))\nfor (t1, t2) in zip(reference_types, input_types)\n)\nif names_compatible and types_compatible:\nreturn docs\nout_docs = []\nfor i in range(len(docs)):\n# validate the data\ntry:\nout_docs.append(\ncast(Type[BaseDoc], self._schema).parse_obj(dict(docs[i]))\n)\nexcept (ValueError, ValidationError) as e:\nraise ValueError(\n'The schema of the input Documents is not compatible with the schema of the Document Index.'\n' Ensure that the field names of your data match the field names of the Document Index schema,'\n' and that the types of your data match the types of the Document Index schema.'\nf'original error {e}'\n)\nreturn DocList[BaseDoc].construct(out_docs)\ndef _validate_search_field(self, search_field: Union[str, None]) -&gt; bool:\n\"\"\"\n        Validate if the given `search_field` corresponds to one of the\n        columns that was parsed from the schema.\n        Some backends, like weaviate, don't use search fields, so the function\n        returns True if `search_field` is empty or None.\n        :param search_field: search field to validate.\n        :return: True if the field exists, False otherwise.\n        \"\"\"\nif not search_field or search_field in self._column_infos.keys():\nif not search_field:\nself._logger.info('Empty search field was passed')\nreturn True\nelse:\nvalid_search_fields = ', '.join(self._column_infos.keys())\nraise ValueError(\nf'{search_field} is not a valid search field. Valid search fields are: {valid_search_fields}'\n)\ndef _to_numpy(self, val: Any, allow_passthrough=False) -&gt; Any:\n\"\"\"\n        Converts a value to a numpy array, if possible.\n        :param val: The value to convert\n        :param allow_passthrough: If True, the value is returned as-is if it is not convertible to a numpy array.\n            If False, a `ValueError` is raised if the value is not convertible to a numpy array.\n        :return: The value as a numpy array, or as-is if `allow_passthrough` is True and the value is not convertible\n        \"\"\"\nif isinstance(val, np.ndarray):\nreturn val\nif tf is not None and isinstance(val, TensorFlowTensor):\nreturn val.unwrap().numpy()\nif isinstance(val, (list, tuple)):\nreturn np.array(val)\nif torch is not None and isinstance(val, torch.Tensor):\nreturn val.detach().numpy()\nif tf is not None and isinstance(val, tf.Tensor):\nreturn val.numpy()\nif allow_passthrough:\nreturn val\nraise ValueError(f'Unsupported input type for {type(self)}: {type(val)}')\ndef _convert_dict_to_doc(\nself, doc_dict: Dict[str, Any], schema: Type[BaseDoc], inner=False\n) -&gt; BaseDoc:\n\"\"\"\n        Convert a dict to a Document object.\n        :param doc_dict: A dict that contains all the flattened fields of a Document, the field names are the keys and follow the pattern {field_name} or {field_name}__{nested_name}\n        :param schema: The schema of the Document object\n        :return: A Document object\n        \"\"\"\nfor field_name, _ in schema._docarray_fields().items():\nt_ = schema._get_field_annotation(field_name)\nif not is_union_type(t_) and safe_issubclass(t_, AnyDocArray):\nself._get_subindex_doclist(doc_dict, field_name)\nif is_optional_type(t_):\nfor t_arg in get_args(t_):\nif t_arg is not type(None):\nt_ = t_arg\nif not is_union_type(t_) and safe_issubclass(t_, BaseDoc):\ninner_dict = {}\nfields = [\nkey for key in doc_dict.keys() if key.startswith(f'{field_name}__')\n]\nfor key in fields:\nnested_name = key[len(f'{field_name}__') :]\ninner_dict[nested_name] = doc_dict.pop(key)\ndoc_dict[field_name] = self._convert_dict_to_doc(\ninner_dict, t_, inner=True\n)\nif self._is_subindex and not inner:\ndoc_dict.pop('parent_id', None)\nschema_cls = cast(Type[BaseDoc], self._ori_schema)\nelse:\nschema_cls = cast(Type[BaseDoc], schema)\ndoc = schema_cls(**doc_dict)\nreturn doc\ndef _dict_list_to_docarray(self, dict_list: Sequence[Dict[str, Any]]) -&gt; DocList:\n\"\"\"Convert a list of docs in dict type to a DocList of the schema type.\"\"\"\ndoc_list = [self._convert_dict_to_doc(doc_dict, self._schema) for doc_dict in dict_list]  # type: ignore\nif self._is_subindex:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._ori_schema))\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nreturn docs_cls(doc_list)\ndef __len__(self) -&gt; int:\nreturn self.num_docs()\ndef _index_subindex(self, column_to_data: Dict[str, Generator[Any, None, None]]):\n\"\"\"Index subindex documents in the corresponding subindex.\n        :param column_to_data: A dictionary from column name to a generator\n        \"\"\"\nfor col_name, col in self._column_infos.items():\nif safe_issubclass(col.docarray_type, AnyDocArray):\ndocs = [\ndoc for doc_list in column_to_data[col_name] for doc in doc_list\n]\nself._subindices[col_name].index(docs)\ncolumn_to_data.pop(col_name, None)\ndef _get_subindex_doclist(self, doc: Dict[str, Any], field_name: str):\n\"\"\"Get subindex Documents from the index and assign them to `field_name`.\n        :param doc: a dictionary mapping from column name to value\n        :param field_name: field name of the subindex Documents\n        \"\"\"\nif field_name not in doc.keys():\nparent_id = doc['id']\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\nparent_id\n)\nif nested_docs_id:\ndoc[field_name] = self._subindices[field_name].__getitem__(\nnested_docs_id\n)\ndef _find_subdocs(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the subindex and return subindex docs and scores.\"\"\"\nfields = subindex.split('__')\nif not subindex or not safe_issubclass(\nself._schema._get_field_annotation(fields[0]), AnyDocArray  # type: ignore\n):\nraise ValueError(f'subindex {subindex} is not valid')\nif len(fields) == 1:\nreturn self._subindices[fields[0]].find(\nquery, search_field=search_field, limit=limit, **kwargs\n)\nreturn self._subindices[fields[0]]._find_subdocs(\nquery,\nsubindex='___'.join(fields[1:]),\nsearch_field=search_field,\nlimit=limit,\n**kwargs,\n)\ndef _get_root_doc_id(self, id: str, root: str, sub: str) -&gt; str:\n\"\"\"Get the root_id given the id of a subindex Document and the root and subindex name\n        :param id: id of the subindex Document\n        :param root: root index name\n        :param sub: subindex name\n        :return: the root_id of the Document\n        \"\"\"\nsubindex = self._subindices[root]\nif not sub:\nsub_doc = subindex._get_items([id])\nparent_id = (\nsub_doc[0]['parent_id']\nif isinstance(sub_doc[0], dict)\nelse sub_doc[0].parent_id\n)\nreturn parent_id\nelse:\nfields = sub.split('__')\ncur_root_id = subindex._get_root_doc_id(\nid, fields[0], '__'.join(fields[1:])\n)\nreturn self._get_root_doc_id(cur_root_id, root, '')\ndef subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n        :param item: the given BaseDoc\n        :return: if the given BaseDoc item is contained in the index/subindices\n        \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.index_name","title":"<code>index_name</code>  <code>property</code>","text":"<p>Return the name of the index in the database.</p>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>class QueryBuilder(ABC):\n@abstractmethod\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the DB specific query object.\n        The DB specific implementation can leverage self._queries to do so.\n        The output of this should be able to be passed to execute_query().\n        \"\"\"\n...\n# TODO support subindex in QueryBuilder\n# the methods below need to be implemented by subclasses\n# If, in your subclass, one of these is not usable in a query builder, but\n# can be called directly on the DocumentIndex, use `_raise_not_composable`.\n# If the method is not supported _at all_, use `_raise_not_supported`.\nfind = abstractmethod(lambda *args, **kwargs: ...)\nfilter = abstractmethod(lambda *args, **kwargs: ...)\ntext_search = abstractmethod(lambda *args, **kwargs: ...)\nfind_batched = abstractmethod(lambda *args, **kwargs: ...)\nfilter_batched = abstractmethod(lambda *args, **kwargs: ...)\ntext_search_batched = abstractmethod(lambda *args, **kwargs: ...)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Build the DB specific query object. The DB specific implementation can leverage self._queries to do so. The output of this should be able to be passed to execute_query().</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>@abstractmethod\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the DB specific query object.\n    The DB specific implementation can leverage self._queries to do so.\n    The output of this should be able to be passed to execute_query().\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Execute a query on the database.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Any</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>@abstractmethod\ndef execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the database.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.num_docs","title":"<code>num_docs()</code>  <code>abstractmethod</code>","text":"<p>Return the number of indexed documents</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>@abstractmethod\ndef num_docs(self) -&gt; int:\n\"\"\"Return the number of indexed documents\"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>  <code>abstractmethod</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>@abstractmethod\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/doc_index/#docarray.index.abstract.BaseDocIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/","title":"ElasticDocIndex","text":""},{"location":"API_reference/doc_index/backends/elastic/#elasticdocindex","title":"ElasticDocIndex","text":""},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex","title":"<code>docarray.index.backends.elastic.ElasticDocIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>class ElasticDocIndex(BaseDocIndex, Generic[TSchema]):\n_index_vector_params: Optional[Tuple[str]] = ('dims', 'similarity', 'index')\n_index_vector_options: Optional[Tuple[str]] = ('m', 'ef_construction')\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize ElasticDocIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(ElasticDocIndex.DBConfig, self._db_config)\nself._logger.debug('Elastic Search index is being initialized')\n# ElasticSearch client creation\nself._client = Elasticsearch(\nhosts=self._db_config.hosts,\n**self._db_config.es_config,\n)\nself._logger.debug('ElasticSearch client has been created')\n# ElasticSearh index setup\nmappings: Dict[str, Any] = {\n'dynamic': True,\n'_source': {'enabled': 'true'},\n'properties': {},\n}\nmappings.update(self._db_config.index_mappings)\nself._logger.debug('Mappings have been updated with db_config.index_mappings')\nfor col_name, col in self._column_infos.items():\nif safe_issubclass(col.docarray_type, AnyDocArray):\ncontinue\nif col.db_type == 'dense_vector' and (\nnot col.n_dim and col.config['dims'] &lt; 0\n):\nself._logger.info(\nf'Not indexing column {col_name}, the dimensionality is not specified'\n)\ncontinue\nmappings['properties'][col_name] = self._create_index_mapping(col)\nself._logger.debug(f'Index mapping created for column {col_name}')\nif self._client.indices.exists(index=self.index_name):\nself._client_put_mapping(mappings)\nself._logger.debug(f'Put mapping for index {self.index_name}')\nelse:\nself._client_create(mappings)\nself._logger.debug(f'Created new index {self.index_name} with mappings')\nif len(self._db_config.index_settings):\nself._client_put_settings(self._db_config.index_settings)\nself._logger.debug('Updated index settings')\nself._refresh(self.index_name)\nself._logger.debug(f'Refreshed index {self.index_name}')\n@property\ndef index_name(self):\ndefault_index_name = (\nself._schema.__name__.lower() if self._schema is not None else None\n)\nif default_index_name is None:\nerr_msg = (\n'A ElasticDocIndex must be typed with a Document type.To do so, use the syntax: '\n'ElasticDocIndex[DocumentType] '\n)\nself._logger.error(err_msg)\nraise ValueError(err_msg)\nindex_name = self._db_config.index_name or default_index_name\nreturn index_name\n###############################################\n# Inner classes for query builder and configs #\n###############################################\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, outer_instance, **kwargs):\nsuper().__init__()\nself._outer_instance = outer_instance\nself._query: Dict[str, Any] = {\n'query': defaultdict(lambda: defaultdict(list))\n}\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search query object.\"\"\"\nself._outer_instance._logger.debug(\n'Building the Elastic Search query object'\n)\nif len(self._query['query']) == 0:\ndel self._query['query']\nelif 'knn' in self._query:\nself._query['knn']['filter'] = self._query['query']\ndel self._query['query']\nreturn self._query\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n            Find k-nearest neighbors of the query.\n            :param query: query vector for KNN/ANN search. Has single axis.\n            :param search_field: name of the field to search on\n            :param limit: maximum number of documents to return per query\n            :param num_candidates: number of candidates\n            :return: self\n            \"\"\"\nself._outer_instance._logger.debug('Executing find query')\nself._outer_instance._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['knn'] = self._outer_instance._form_search_body(\nquery_vec_np,\nlimit,\nsearch_field,\nnum_candidates,\n)['knn']\nreturn self\n# filter accepts Leaf/Compound query clauses\n# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html\ndef filter(self, query: Dict[str, Any], limit: int = 10):\n\"\"\"Find documents in the index based on a filter query\n            :param query: the query to execute\n            :param limit: maximum number of documents to return\n            :return: self\n            \"\"\"\nself._outer_instance._logger.debug('Executing filter query')\nself._query['size'] = limit\nself._query['query']['bool']['filter'].append(query)\nreturn self\ndef text_search(self, query: str, search_field: str = 'text', limit: int = 10):\n\"\"\"Find documents in the index based on a text search query\n            :param query: The text to search for\n            :param search_field: name of the field to search on\n            :param limit: maximum number of documents to find\n            :return: self\n            \"\"\"\nself._outer_instance._logger.debug('Executing text search query')\nself._outer_instance._validate_search_field(search_field)\nself._query['size'] = limit\nself._query['query']['bool']['must'].append(\n{'match': {search_field: query}}\n)\nreturn self\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\ndef build_query(self, **kwargs) -&gt; QueryBuilder:\n\"\"\"\n        Build a query for ElasticDocIndex.\n        :param kwargs: parameters to forward to QueryBuilder initialization\n        :return: QueryBuilder object\n        \"\"\"\nreturn self.QueryBuilder(self, **kwargs)\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of ElasticDocIndex.\"\"\"\nhosts: Union[\nstr, List[Union[str, Mapping[str, Union[str, int]], NodeConfig]], None\n] = 'http://localhost:9200'\nindex_name: Optional[str] = None\nes_config: Dict[str, Any] = field(default_factory=dict)\nindex_settings: Dict[str, Any] = field(default_factory=dict)\nindex_mappings: Dict[str, Any] = field(default_factory=dict)\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(default_factory=dict)\ndef __post_init__(self):\nself.default_column_config = {\n'binary': {},\n'boolean': {},\n'keyword': {},\n'long': {},\n'integer': {},\n'short': {},\n'byte': {},\n'double': {},\n'float': {},\n'half_float': {},\n'scaled_float': {},\n'unsigned_long': {},\n'dates': {},\n'alias': {},\n'object': {},\n'flattened': {},\n'nested': {},\n'join': {},\n'integer_range': {},\n'float_range': {},\n'long_range': {},\n'double_range': {},\n'date_range': {},\n'ip_range': {},\n'ip': {},\n'version': {},\n'histogram': {},\n'text': {},\n'annotated_text': {},\n'completion': {},\n'search_as_you_type': {},\n'token_count': {},\n'sparse_vector': {},\n'rank_feature': {},\n'rank_features': {},\n'geo_point': {},\n'geo_shape': {},\n'point': {},\n'shape': {},\n'percolator': {},\n# `None` is not a Type, but we allow it here anyway\nNone: {},  # type: ignore\n}\nself.default_column_config['dense_vector'] = self.dense_vector_config()\ndef dense_vector_config(self):\n\"\"\"Get the dense vector config.\"\"\"\nconfig = {\n'dims': -1,\n'index': True,\n'similarity': 'cosine',  # 'l2_norm', 'dot_product', 'cosine'\n'm': 16,\n'ef_construction': 100,\n'num_candidates': 10000,\n}\nreturn config\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.\"\"\"\nchunk_size: int = 500\n###############################################\n# Implementation of abstract methods          #\n###############################################\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type,\n            or None if ``python_type`` is not supported.\n        \"\"\"\nself._logger.debug(f'Mapping Python type {python_type} to database type')\nfor allowed_type in ELASTIC_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"dense_vector\"'\n)\nreturn 'dense_vector'\nelastic_py_types = {\ndocarray.typing.ID: 'keyword',\ndocarray.typing.AnyUrl: 'keyword',\nbool: 'boolean',\nint: 'integer',\nfloat: 'float',\nstr: 'text',\nbytes: 'binary',\ndict: 'object',\n}\nfor type in elastic_py_types.keys():\nif safe_issubclass(python_type, type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"{elastic_py_types[type]}\"'\n)\nreturn elastic_py_types[type]\nerr_msg = f'Unsupported column type for {type(self)}: {python_type}'\nself._logger.error(err_msg)\nraise ValueError(err_msg)\ndef _index(\nself,\ncolumn_to_data: Mapping[str, Generator[Any, None, None]],\nrefresh: bool = True,\nchunk_size: Optional[int] = None,\n):\nself._index_subindex(column_to_data)\ndata = self._transpose_col_value_dict(column_to_data)\nrequests = []\nfor row in data:\nrequest = {\n'_index': self.index_name,\n'_id': row['id'],\n}\nfor col_name, col in self._column_infos.items():\nif safe_issubclass(col.docarray_type, AnyDocArray):\ncontinue\nif col.db_type == 'dense_vector' and np.all(row[col_name] == 0):\nrow[col_name] = row[col_name] + 1.0e-9\nif row[col_name] is None:\ncontinue\nrequest[col_name] = row[col_name]\nrequests.append(request)\n_, warning_info = self._send_requests(requests, chunk_size)\nfor info in warning_info:\nwarnings.warn(str(info))\nself._logger.warning('Warning: %s', str(info))\nif refresh:\nself._logger.debug('Refreshing the index')\nself._refresh(self.index_name)\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        \"\"\"\nself._logger.debug('Getting the number of documents in the index')\nreturn self._client.count(index=self.index_name)['count']\ndef _del_items(\nself,\ndoc_ids: Sequence[str],\nchunk_size: Optional[int] = None,\n):\nrequests = []\nfor _id in doc_ids:\nrequests.append(\n{'_op_type': 'delete', '_index': self.index_name, '_id': _id}\n)\n_, warning_info = self._send_requests(requests, chunk_size)\n# raise warning if some ids are not found\nif warning_info:\nids = [info['delete']['_id'] for info in warning_info]\nwarnings.warn(f'No document with id {ids} found')\nself._refresh(self.index_name)\ndef _get_items(self, doc_ids: Sequence[str]) -&gt; Sequence[Dict[str, Any]]:\naccumulated_docs = []\naccumulated_docs_id_not_found = []\nes_rows = self._client_mget(doc_ids)['docs']\nfor row in es_rows:\nif row['found']:\ndoc_dict = row['_source']\naccumulated_docs.append(doc_dict)\nelse:\naccumulated_docs_id_not_found.append(row['_id'])\n# raise warning if some ids are not found\nif accumulated_docs_id_not_found:\nwarnings.warn(f'No document with id {accumulated_docs_id_not_found} found')\nreturn accumulated_docs\ndef execute_query(self, query: Dict[str, Any], *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the ElasticDocIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\nself._logger.debug(f'Executing query: {query}')\nif args or kwargs:\nerr_msg = (\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nself._logger.error(err_msg)\nraise ValueError(err_msg)\nresp = self._client.search(index=self.index_name, **query)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\ndef _find(\nself, query: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nbody = self._form_search_body(query, limit, search_field)\nresp = self._client_search(**body)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nrequest = []\nfor query in queries:\nhead = {'index': self.index_name}\nbody = self._form_search_body(query, limit, search_field)\nrequest.extend([head, body])\nresponses = self._client_msearch(request)\ndas, scores = zip(\n*[self._format_response(resp) for resp in responses['responses']]\n)\nreturn _FindResultBatched(documents=list(das), scores=scores)\ndef _filter(\nself,\nfilter_query: Dict[str, Any],\nlimit: int,\n) -&gt; List[Dict]:\nresp = self._client_search(query=filter_query, size=limit)\ndocs, _ = self._format_response(resp)\nreturn docs\ndef _filter_batched(\nself,\nfilter_queries: Any,\nlimit: int,\n) -&gt; List[List[Dict]]:\nrequest = []\nfor query in filter_queries:\nhead = {'index': self.index_name}\nbody = {'query': query, 'size': limit}\nrequest.extend([head, body])\nresponses = self._client_msearch(request)\ndas, _ = zip(*[self._format_response(resp) for resp in responses['responses']])\nreturn list(das)\ndef _text_search(\nself,\nquery: str,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\nbody = self._form_text_search_body(query, limit, search_field)\nresp = self._client_search(**body)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=np.array(scores))  # type: ignore\ndef _text_search_batched(\nself,\nqueries: Sequence[str],\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nrequest = []\nfor query in queries:\nhead = {'index': self.index_name}\nbody = self._form_text_search_body(query, limit, search_field)\nrequest.extend([head, body])\nresponses = self._client_msearch(request)\ndas, scores = zip(\n*[self._format_response(resp) for resp in responses['responses']]\n)\nreturn _FindResultBatched(documents=list(das), scores=scores)\ndef _filter_by_parent_id(self, id: str) -&gt; List[str]:\nresp = self._client_search(\nquery={'term': {'parent_id': id}}, fields=['id'], _source=False\n)\nids = [hit['fields']['id'][0] for hit in resp['hits']['hits']]\nreturn ids\n###############################################\n# Helpers                                     #\n###############################################\n@classmethod\ndef _create_index_mapping(cls, col: '_ColumnInfo') -&gt; Dict[str, Any]:\n\"\"\"Create a new HNSW index for a column, and initialize it.\"\"\"\nindex = {'type': col.config['type'] if 'type' in col.config else col.db_type}\nif col.db_type == 'dense_vector':\nif cls._index_vector_params is not None:\nfor k in cls._index_vector_params:\nindex[k] = col.config[k]\nif col.n_dim:\nindex['dims'] = col.n_dim\nif cls._index_vector_options is not None:\nindex['index_options'] = dict(\n(k, col.config[k]) for k in cls._index_vector_options\n)\nindex['index_options']['type'] = 'hnsw'\nreturn index\ndef _send_requests(\nself,\nrequest: Iterable[Dict[str, Any]],\nchunk_size: Optional[int] = None,\n**kwargs,\n) -&gt; Tuple[List[Dict], List[Any]]:\n\"\"\"Send bulk request to Elastic and gather the successful info\"\"\"\naccumulated_info = []\nwarning_info = []\nfor success, info in parallel_bulk(\nself._client,\nrequest,\nraise_on_error=False,\nraise_on_exception=False,\nchunk_size=chunk_size if chunk_size else self._runtime_config.chunk_size,  # type: ignore\n**kwargs,\n):\nif not success:\nwarning_info.append(info)\nelse:\naccumulated_info.append(info)\nreturn accumulated_info, warning_info\ndef _form_search_body(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\nnum_candidates: Optional[int] = None,\n) -&gt; Dict[str, Any]:\nif not num_candidates:\nnum_candidates = self._db_config.default_column_config['dense_vector'][\n'num_candidates'\n]\nbody = {\n'size': limit,\n'knn': {\n'field': search_field,\n'query_vector': query,\n'k': limit,\n'num_candidates': num_candidates,\n},\n}\nreturn body\ndef _form_text_search_body(\nself, query: str, limit: int, search_field: str = ''\n) -&gt; Dict[str, Any]:\nbody = {\n'size': limit,\n'query': {\n'bool': {\n'must': {'match': {search_field: query}},\n}\n},\n}\nreturn body\ndef _format_response(self, response: Any) -&gt; Tuple[List[Dict], List[Any]]:\ndocs = []\nscores = []\nfor result in response['hits']['hits']:\nif not isinstance(result, dict):\nresult = result.to_dict()\nif result.get('_source', None):\ndoc_dict = result['_source']\nelse:\ndoc_dict = result['fields']\ndoc_dict['id'] = result['_id']\ndocs.append(doc_dict)\nscores.append(result['_score'])\nreturn docs, [parse_obj_as(NdArray, np.array(s)) for s in scores]\ndef _refresh(self, index_name: str):\nself._client.indices.refresh(index=index_name)\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nif len(doc_id) == 0:\nreturn False\nret = self._client_mget([doc_id])\nreturn ret[\"docs\"][0][\"found\"]\n###############################################\n# API Wrappers                                #\n###############################################\ndef _client_put_mapping(self, mappings: Dict[str, Any]):\nself._client.indices.put_mapping(\nindex=self.index_name, properties=mappings['properties']\n)\ndef _client_create(self, mappings: Dict[str, Any]):\nself._client.indices.create(index=self.index_name, mappings=mappings)\ndef _client_put_settings(self, settings: Dict[str, Any]):\nself._client.indices.put_settings(index=self.index_name, settings=settings)\ndef _client_mget(self, ids: Sequence[str]):\nreturn self._client.mget(index=self.index_name, ids=ids)\ndef _client_search(self, **kwargs):\nreturn self._client.search(index=self.index_name, **kwargs)\ndef _client_msearch(self, request: List[Dict[str, Any]]):\nreturn self._client.msearch(index=self.index_name, searches=request)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of ElasticDocIndex.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of ElasticDocIndex.\"\"\"\nhosts: Union[\nstr, List[Union[str, Mapping[str, Union[str, int]], NodeConfig]], None\n] = 'http://localhost:9200'\nindex_name: Optional[str] = None\nes_config: Dict[str, Any] = field(default_factory=dict)\nindex_settings: Dict[str, Any] = field(default_factory=dict)\nindex_mappings: Dict[str, Any] = field(default_factory=dict)\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(default_factory=dict)\ndef __post_init__(self):\nself.default_column_config = {\n'binary': {},\n'boolean': {},\n'keyword': {},\n'long': {},\n'integer': {},\n'short': {},\n'byte': {},\n'double': {},\n'float': {},\n'half_float': {},\n'scaled_float': {},\n'unsigned_long': {},\n'dates': {},\n'alias': {},\n'object': {},\n'flattened': {},\n'nested': {},\n'join': {},\n'integer_range': {},\n'float_range': {},\n'long_range': {},\n'double_range': {},\n'date_range': {},\n'ip_range': {},\n'ip': {},\n'version': {},\n'histogram': {},\n'text': {},\n'annotated_text': {},\n'completion': {},\n'search_as_you_type': {},\n'token_count': {},\n'sparse_vector': {},\n'rank_feature': {},\n'rank_features': {},\n'geo_point': {},\n'geo_shape': {},\n'point': {},\n'shape': {},\n'percolator': {},\n# `None` is not a Type, but we allow it here anyway\nNone: {},  # type: ignore\n}\nself.default_column_config['dense_vector'] = self.dense_vector_config()\ndef dense_vector_config(self):\n\"\"\"Get the dense vector config.\"\"\"\nconfig = {\n'dims': -1,\n'index': True,\n'similarity': 'cosine',  # 'l2_norm', 'dot_product', 'cosine'\n'm': 16,\n'ef_construction': 100,\n'num_candidates': 10000,\n}\nreturn config\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.DBConfig.dense_vector_config","title":"<code>dense_vector_config()</code>","text":"<p>Get the dense vector config.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def dense_vector_config(self):\n\"\"\"Get the dense vector config.\"\"\"\nconfig = {\n'dims': -1,\n'index': True,\n'similarity': 'cosine',  # 'l2_norm', 'dot_product', 'cosine'\n'm': 16,\n'ef_construction': 100,\n'num_candidates': 10000,\n}\nreturn config\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, outer_instance, **kwargs):\nsuper().__init__()\nself._outer_instance = outer_instance\nself._query: Dict[str, Any] = {\n'query': defaultdict(lambda: defaultdict(list))\n}\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search query object.\"\"\"\nself._outer_instance._logger.debug(\n'Building the Elastic Search query object'\n)\nif len(self._query['query']) == 0:\ndel self._query['query']\nelif 'knn' in self._query:\nself._query['knn']['filter'] = self._query['query']\ndel self._query['query']\nreturn self._query\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n        Find k-nearest neighbors of the query.\n        :param query: query vector for KNN/ANN search. Has single axis.\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return per query\n        :param num_candidates: number of candidates\n        :return: self\n        \"\"\"\nself._outer_instance._logger.debug('Executing find query')\nself._outer_instance._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['knn'] = self._outer_instance._form_search_body(\nquery_vec_np,\nlimit,\nsearch_field,\nnum_candidates,\n)['knn']\nreturn self\n# filter accepts Leaf/Compound query clauses\n# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html\ndef filter(self, query: Dict[str, Any], limit: int = 10):\n\"\"\"Find documents in the index based on a filter query\n        :param query: the query to execute\n        :param limit: maximum number of documents to return\n        :return: self\n        \"\"\"\nself._outer_instance._logger.debug('Executing filter query')\nself._query['size'] = limit\nself._query['query']['bool']['filter'].append(query)\nreturn self\ndef text_search(self, query: str, search_field: str = 'text', limit: int = 10):\n\"\"\"Find documents in the index based on a text search query\n        :param query: The text to search for\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to find\n        :return: self\n        \"\"\"\nself._outer_instance._logger.debug('Executing text search query')\nself._outer_instance._validate_search_field(search_field)\nself._query['size'] = limit\nself._query['query']['bool']['must'].append(\n{'match': {search_field: query}}\n)\nreturn self\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the elastic search query object.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search query object.\"\"\"\nself._outer_instance._logger.debug(\n'Building the Elastic Search query object'\n)\nif len(self._query['query']) == 0:\ndel self._query['query']\nelif 'knn' in self._query:\nself._query['knn']['filter'] = self._query['query']\ndel self._query['query']\nreturn self._query\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.QueryBuilder.filter","title":"<code>filter(query, limit=10)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>the query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def filter(self, query: Dict[str, Any], limit: int = 10):\n\"\"\"Find documents in the index based on a filter query\n    :param query: the query to execute\n    :param limit: maximum number of documents to return\n    :return: self\n    \"\"\"\nself._outer_instance._logger.debug('Executing filter query')\nself._query['size'] = limit\nself._query['query']['bool']['filter'].append(query)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.QueryBuilder.find","title":"<code>find(query, search_field='embedding', limit=10, num_candidates=None)</code>","text":"<p>Find k-nearest neighbors of the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Has single axis.</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>'embedding'</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <code>num_candidates</code> <code>Optional[int]</code> <p>number of candidates</p> <code>None</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n    Find k-nearest neighbors of the query.\n    :param query: query vector for KNN/ANN search. Has single axis.\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return per query\n    :param num_candidates: number of candidates\n    :return: self\n    \"\"\"\nself._outer_instance._logger.debug('Executing find query')\nself._outer_instance._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['knn'] = self._outer_instance._form_search_body(\nquery_vec_np,\nlimit,\nsearch_field,\nnum_candidates,\n)['knn']\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.QueryBuilder.text_search","title":"<code>text_search(query, search_field='text', limit=10)</code>","text":"<p>Find documents in the index based on a text search query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>'text'</code> <code>limit</code> <code>int</code> <p>maximum number of documents to find</p> <code>10</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def text_search(self, query: str, search_field: str = 'text', limit: int = 10):\n\"\"\"Find documents in the index based on a text search query\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to find\n    :return: self\n    \"\"\"\nself._outer_instance._logger.debug('Executing text search query')\nself._outer_instance._validate_search_field(search_field)\nself._query['size'] = limit\nself._query['query']['bool']['must'].append(\n{'match': {search_field: query}}\n)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.\"\"\"\nchunk_size: int = 500\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize ElasticDocIndex</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize ElasticDocIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(ElasticDocIndex.DBConfig, self._db_config)\nself._logger.debug('Elastic Search index is being initialized')\n# ElasticSearch client creation\nself._client = Elasticsearch(\nhosts=self._db_config.hosts,\n**self._db_config.es_config,\n)\nself._logger.debug('ElasticSearch client has been created')\n# ElasticSearh index setup\nmappings: Dict[str, Any] = {\n'dynamic': True,\n'_source': {'enabled': 'true'},\n'properties': {},\n}\nmappings.update(self._db_config.index_mappings)\nself._logger.debug('Mappings have been updated with db_config.index_mappings')\nfor col_name, col in self._column_infos.items():\nif safe_issubclass(col.docarray_type, AnyDocArray):\ncontinue\nif col.db_type == 'dense_vector' and (\nnot col.n_dim and col.config['dims'] &lt; 0\n):\nself._logger.info(\nf'Not indexing column {col_name}, the dimensionality is not specified'\n)\ncontinue\nmappings['properties'][col_name] = self._create_index_mapping(col)\nself._logger.debug(f'Index mapping created for column {col_name}')\nif self._client.indices.exists(index=self.index_name):\nself._client_put_mapping(mappings)\nself._logger.debug(f'Put mapping for index {self.index_name}')\nelse:\nself._client_create(mappings)\nself._logger.debug(f'Created new index {self.index_name} with mappings')\nif len(self._db_config.index_settings):\nself._client_put_settings(self._db_config.index_settings)\nself._logger.debug('Updated index settings')\nself._refresh(self.index_name)\nself._logger.debug(f'Refreshed index {self.index_name}')\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.build_query","title":"<code>build_query(**kwargs)</code>","text":"<p>Build a query for ElasticDocIndex.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to forward to QueryBuilder initialization</p> <code>{}</code> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def build_query(self, **kwargs) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for ElasticDocIndex.\n    :param kwargs: parameters to forward to QueryBuilder initialization\n    :return: QueryBuilder object\n    \"\"\"\nreturn self.QueryBuilder(self, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the ElasticDocIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def execute_query(self, query: Dict[str, Any], *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the ElasticDocIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\nself._logger.debug(f'Executing query: {query}')\nif args or kwargs:\nerr_msg = (\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nself._logger.error(err_msg)\nraise ValueError(err_msg)\nresp = self._client.search(index=self.index_name, **query)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nself._logger.debug('Getting the number of documents in the index')\nreturn self._client.count(index=self.index_name)['count']\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\nself._logger.debug(f'Mapping Python type {python_type} to database type')\nfor allowed_type in ELASTIC_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"dense_vector\"'\n)\nreturn 'dense_vector'\nelastic_py_types = {\ndocarray.typing.ID: 'keyword',\ndocarray.typing.AnyUrl: 'keyword',\nbool: 'boolean',\nint: 'integer',\nfloat: 'float',\nstr: 'text',\nbytes: 'binary',\ndict: 'object',\n}\nfor type in elastic_py_types.keys():\nif safe_issubclass(python_type, type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"{elastic_py_types[type]}\"'\n)\nreturn elastic_py_types[type]\nerr_msg = f'Unsupported column type for {type(self)}: {python_type}'\nself._logger.error(err_msg)\nraise ValueError(err_msg)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic/#docarray.index.backends.elastic.ElasticDocIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/","title":"ElasticV7DocIndex","text":""},{"location":"API_reference/doc_index/backends/elastic7/#elasticv7docindex","title":"ElasticV7DocIndex","text":""},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex","title":"<code>docarray.index.backends.elasticv7.ElasticV7DocIndex</code>","text":"<p>             Bases: <code>ElasticDocIndex</code></p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>class ElasticV7DocIndex(ElasticDocIndex):\n_index_vector_params: Optional[Tuple[str]] = ('dims',)\n_index_vector_options: Optional[Tuple[str]] = None\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize ElasticV7DocIndex\"\"\"\nfrom elasticsearch import __version__ as __es__version__\nif __es__version__[0] &gt; 7:\nraise ImportError(\n'ElasticV7DocIndex requires the elasticsearch library to be version 7.10.1'\n)\nsuper().__init__(db_config, **kwargs)\n###############################################\n# Inner classes for query builder and configs #\n###############################################\nclass QueryBuilder(ElasticDocIndex.QueryBuilder):\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search v7 query object.\"\"\"\nif (\n'script_score' in self._query['query']\nand 'bool' in self._query['query']\nand len(self._query['query']['bool']) &gt; 0\n):\nself._query['query']['script_score']['query'] = {}\nself._query['query']['script_score']['query']['bool'] = self._query[\n'query'\n]['bool']\ndel self._query['query']['bool']\nreturn self._query\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n            Find k-nearest neighbors of the query.\n            :param query: query vector for KNN/ANN search. Has single axis.\n            :param search_field: name of the field to search on\n            :param limit: maximum number of documents to return per query\n            :return: self\n            \"\"\"\nif num_candidates:\nwarnings.warn('`num_candidates` is not supported in ElasticV7DocIndex')\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['size'] = limit\nself._query['query'][\n'script_score'\n] = self._outer_instance._form_search_body(\nquery_vec_np, limit, search_field\n)[\n'query'\n][\n'script_score'\n]\nreturn self\n@dataclass\nclass DBConfig(ElasticDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of ElasticDocIndex.\"\"\"\nhosts: Union[str, List[str], None] = 'http://localhost:9200'  # type: ignore\ndef dense_vector_config(self):\nreturn {'dims': 128}\n@dataclass\nclass RuntimeConfig(ElasticDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.\"\"\"\npass\n###############################################\n# Implementation of abstract methods          #\n###############################################\ndef execute_query(self, query: Dict[str, Any], *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the ElasticDocIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :return: the result of the query\n        \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nresp = self._client.search(index=self.index_name, body=query)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\n###############################################\n# Helpers                                     #\n###############################################\ndef _form_search_body(self, query: np.ndarray, limit: int, search_field: str = '') -&gt; Dict[str, Any]:  # type: ignore\nbody = {\n'size': limit,\n'query': {\n'script_score': {\n'query': {'match_all': {}},\n'script': {\n'source': f'cosineSimilarity(params.query_vector, \\'{search_field}\\') + 1.0',\n'params': {'query_vector': query},\n},\n}\n},\n}\nreturn body\n###############################################\n# API Wrappers                                #\n###############################################\ndef _client_put_mapping(self, mappings: Dict[str, Any]):\nself._client.indices.put_mapping(index=self.index_name, body=mappings)\ndef _client_create(self, mappings: Dict[str, Any]):\nbody = {'mappings': mappings}\nself._client.indices.create(index=self.index_name, body=body)\ndef _client_put_settings(self, settings: Dict[str, Any]):\nself._client.indices.put_settings(index=self.index_name, body=settings)\ndef _client_mget(self, ids: Sequence[str]):\nreturn self._client.mget(index=self.index_name, body={'ids': ids})\ndef _client_search(self, **kwargs):\nreturn self._client.search(index=self.index_name, body=kwargs)\ndef _client_msearch(self, request: List[Dict[str, Any]]):\nreturn self._client.msearch(index=self.index_name, body=request)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of ElasticDocIndex.</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>@dataclass\nclass DBConfig(ElasticDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of ElasticDocIndex.\"\"\"\nhosts: Union[str, List[str], None] = 'http://localhost:9200'  # type: ignore\ndef dense_vector_config(self):\nreturn {'dims': 128}\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>class QueryBuilder(ElasticDocIndex.QueryBuilder):\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search v7 query object.\"\"\"\nif (\n'script_score' in self._query['query']\nand 'bool' in self._query['query']\nand len(self._query['query']['bool']) &gt; 0\n):\nself._query['query']['script_score']['query'] = {}\nself._query['query']['script_score']['query']['bool'] = self._query[\n'query'\n]['bool']\ndel self._query['query']['bool']\nreturn self._query\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n        Find k-nearest neighbors of the query.\n        :param query: query vector for KNN/ANN search. Has single axis.\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return per query\n        :return: self\n        \"\"\"\nif num_candidates:\nwarnings.warn('`num_candidates` is not supported in ElasticV7DocIndex')\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['size'] = limit\nself._query['query'][\n'script_score'\n] = self._outer_instance._form_search_body(\nquery_vec_np, limit, search_field\n)[\n'query'\n][\n'script_score'\n]\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the elastic search v7 query object.</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the elastic search v7 query object.\"\"\"\nif (\n'script_score' in self._query['query']\nand 'bool' in self._query['query']\nand len(self._query['query']['bool']) &gt; 0\n):\nself._query['query']['script_score']['query'] = {}\nself._query['query']['script_score']['query']['bool'] = self._query[\n'query'\n]['bool']\ndel self._query['query']['bool']\nreturn self._query\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.QueryBuilder.filter","title":"<code>filter(query, limit=10)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>the query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def filter(self, query: Dict[str, Any], limit: int = 10):\n\"\"\"Find documents in the index based on a filter query\n    :param query: the query to execute\n    :param limit: maximum number of documents to return\n    :return: self\n    \"\"\"\nself._outer_instance._logger.debug('Executing filter query')\nself._query['size'] = limit\nself._query['query']['bool']['filter'].append(query)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.QueryBuilder.find","title":"<code>find(query, search_field='embedding', limit=10, num_candidates=None)</code>","text":"<p>Find k-nearest neighbors of the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Has single axis.</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>'embedding'</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = 'embedding',\nlimit: int = 10,\nnum_candidates: Optional[int] = None,\n):\n\"\"\"\n    Find k-nearest neighbors of the query.\n    :param query: query vector for KNN/ANN search. Has single axis.\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return per query\n    :return: self\n    \"\"\"\nif num_candidates:\nwarnings.warn('`num_candidates` is not supported in ElasticV7DocIndex')\nif isinstance(query, BaseDoc):\nquery_vec = BaseDocIndex._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = BaseDocIndex._to_numpy(self._outer_instance, query_vec)\nself._query['size'] = limit\nself._query['query'][\n'script_score'\n] = self._outer_instance._form_search_body(\nquery_vec_np, limit, search_field\n)[\n'query'\n][\n'script_score'\n]\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.QueryBuilder.text_search","title":"<code>text_search(query, search_field='text', limit=10)</code>","text":"<p>Find documents in the index based on a text search query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>'text'</code> <code>limit</code> <code>int</code> <p>maximum number of documents to find</p> <code>10</code> <p>Returns:</p> Type Description <p>self</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def text_search(self, query: str, search_field: str = 'text', limit: int = 10):\n\"\"\"Find documents in the index based on a text search query\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to find\n    :return: self\n    \"\"\"\nself._outer_instance._logger.debug('Executing text search query')\nself._outer_instance._validate_search_field(search_field)\nself._query['size'] = limit\nself._query['query']['bool']['must'].append(\n{'match': {search_field: query}}\n)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>@dataclass\nclass RuntimeConfig(ElasticDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of ElasticDocIndex.\"\"\"\npass\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize ElasticV7DocIndex</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize ElasticV7DocIndex\"\"\"\nfrom elasticsearch import __version__ as __es__version__\nif __es__version__[0] &gt; 7:\nraise ImportError(\n'ElasticV7DocIndex requires the elasticsearch library to be version 7.10.1'\n)\nsuper().__init__(db_config, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.build_query","title":"<code>build_query(**kwargs)</code>","text":"<p>Build a query for ElasticDocIndex.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to forward to QueryBuilder initialization</p> <code>{}</code> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def build_query(self, **kwargs) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for ElasticDocIndex.\n    :param kwargs: parameters to forward to QueryBuilder initialization\n    :return: QueryBuilder object\n    \"\"\"\nreturn self.QueryBuilder(self, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the ElasticDocIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>the query to execute</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/elasticv7.py</code> <pre><code>def execute_query(self, query: Dict[str, Any], *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the ElasticDocIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :return: the result of the query\n    \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nresp = self._client.search(index=self.index_name, body=query)\ndocs, scores = self._format_response(resp)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nself._logger.debug('Getting the number of documents in the index')\nreturn self._client.count(index=self.index_name)['count']\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/elastic.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\nself._logger.debug(f'Mapping Python type {python_type} to database type')\nfor allowed_type in ELASTIC_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"dense_vector\"'\n)\nreturn 'dense_vector'\nelastic_py_types = {\ndocarray.typing.ID: 'keyword',\ndocarray.typing.AnyUrl: 'keyword',\nbool: 'boolean',\nint: 'integer',\nfloat: 'float',\nstr: 'text',\nbytes: 'binary',\ndict: 'object',\n}\nfor type in elastic_py_types.keys():\nif safe_issubclass(python_type, type):\nself._logger.info(\nf'Mapped Python type {python_type} to database type \"{elastic_py_types[type]}\"'\n)\nreturn elastic_py_types[type]\nerr_msg = f'Unsupported column type for {type(self)}: {python_type}'\nself._logger.error(err_msg)\nraise ValueError(err_msg)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/elastic7/#docarray.index.backends.elasticv7.ElasticV7DocIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/","title":"EpsillaDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/epsilla/#epsilladocumentindex","title":"EpsillaDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex","title":"<code>docarray.index.backends.epsilla.EpsillaDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/epsilla.py</code> <pre><code>class EpsillaDocumentIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(self, db_config=None, **kwargs):\n# will set _db_config from args / kwargs\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: EpsillaDocumentIndex.DBConfig = cast(\nEpsillaDocumentIndex.DBConfig, self._db_config\n)\nself._db_config.validate_config()\nself._validate_column_info()\nself._table_name = (\nself._db_config.table_name\nif self._db_config.table_name\nelse self._schema.__name__\n)\nif self._db_config.is_self_hosted:\nself._db = vectordb.Client(\nprotocol=self._db_config.protocol,\nhost=self._db_config.host,\nport=self._db_config.port,\n)\nstatus_code, response = self._db.load_db(\ndb_name=self._db_config.db_name,\ndb_path=self._db_config.db_path,\n)\nif status_code != HTTPStatus.OK:\nif status_code == HTTPStatus.CONFLICT:\nself._logger.info(f'{self._db_config.db_name} already loaded.')\nelse:\nraise IOError(\nf\"Failed to load database {self._db_config.db_name}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nself._db.use_db(self._db_config.db_name)\nstatus_code, response = self._db.list_tables()\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to list tables. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nif self._table_name not in response[\"result\"]:\nself._create_table_self_hosted()\nelse:\nself._client = cloud.Client(\nproject_id=self._db_config.cloud_project_id,\napi_key=self._db_config.api_key,\n)\nself._db = self._client.vectordb(self._db_config.cloud_db_id)\nstatus_code, response = self._db.list_tables()\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to list tables. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\n# Epsilla cloud requires table to be created in the web UI before inserting data\n# It does not support creating tables from Python client yet.\ndef _validate_column_info(self):\nvector_columns = []\nfor info in self._column_infos.values():\nfor type in [list, np.ndarray, AbstractTensor]:\nif safe_issubclass(info.docarray_type, type) and info.config.get(\n'is_embedding', False\n):\n# check that dimension is present\nif info.n_dim is None and info.config.get('dim', None) is None:\nraise ValueError(\"The dimension information is missing\")\nvector_columns.append(info.docarray_type)\nbreak\nif len(vector_columns) == 0:\nraise ValueError(\n\"Unable to find any vector columns. Please make sure that at least one \"\n\"column is of a vector type with the is_embedding=True attribute specified.\"\n)\nelif len(vector_columns) &gt; 1:\nraise ValueError(\"Specifying multiple vector fields is not supported.\")\ndef _create_table_self_hosted(self):\n\"\"\"Use _column_infos to create a table in the database.\"\"\"\ntable_fields = []\nprimary_keys = []\nfor column_name, column_info in self._column_infos.items():\nif column_info.docarray_type == ID:\nprimary_keys.append(column_name)\n# when there is a nested schema, we may have multiple \"ID\" fields. We use the presence of \"__\"\n# to determine if the field is nested or not\nif len(primary_keys) &gt; 1:\nsorted_pkeys = sorted(primary_keys, key=lambda x: x.count(\"__\"))\nprimary_keys = sorted_pkeys[:1]\nfor column_name, column_info in self._column_infos.items():\ndim = (\ncolumn_info.n_dim\nif column_info.n_dim is not None\nelse column_info.config.get('dim', None)\n)\nif dim is None:\ntable_fields.append(\n{\n'name': column_name,\n'dataType': column_info.db_type,\n'primaryKey': column_name in primary_keys,\n}\n)\nelse:\ntable_fields.append(\n{\n'name': column_name,\n'dataType': column_info.db_type,\n'dimensions': dim,\n}\n)\nstatus_code, response = self._db.create_table(\ntable_name=self._table_name,\ntable_fields=table_fields,\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to create table {self._table_name}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\n@dataclass\nclass Query:\n\"\"\"Dataclass describing a query.\"\"\"\nvector_field: Optional[str]\nvector_query: Optional[NdArray]\nfilter: Optional[str]\nlimit: int\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(\nself,\nvector_search_field: Optional[str] = None,\nvector_queries: Optional[List[NdArray]] = None,\nfilter: Optional[str] = None,\n):\nself._vector_search_field: Optional[str] = vector_search_field\nself._vector_queries: List[NdArray] = vector_queries or []\nself._filter: Optional[str] = filter\ndef find(self, query: NdArray, search_field: str = ''):\nif self._vector_search_field and self._vector_search_field != search_field:\nraise ValueError(\nf'Trying to call .find for search_field = {search_field}, but '\nf'previously {self._vector_search_field} was used. Only a single '\nf'field might be used in chained calls.'\n)\nreturn EpsillaDocumentIndex.QueryBuilder(\nvector_search_field=search_field,\nvector_queries=self._vector_queries + [query],\nfilter=self._filter,\n)\ndef filter(self, filter_query: str):  # type: ignore[override]\nreturn EpsillaDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_queries=self._vector_queries,\nfilter=filter_query,\n)\ndef build(self, limit: int) -&gt; Any:\nif len(self._vector_queries) &gt; 0:\n# If there are multiple vector queries applied, we can average them and\n# perform semantic search on a single vector instead\nvector_query = np.average(self._vector_queries, axis=0)\nelse:\nvector_query = None\nreturn EpsillaDocumentIndex.Query(\nvector_field=self._vector_search_field,\nvector_query=vector_query,\nfilter=self._filter,\nlimit=limit,\n)\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search = _raise_not_supported('text_search')\ntext_search_batched = _raise_not_supported('text_search_batched')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Static configuration for EpsillaDocumentIndex\"\"\"\n# default value is the schema type name\ntable_name: Optional[str] = None\n# Indicator for self-hosted or cloud version\nis_self_hosted: bool = False\n# self-hosted version uses the following configs\nprotocol: Optional[str] = None\nhost: Optional[str] = None\nport: Optional[int] = 8888\ndb_path: Optional[str] = None\ndb_name: Optional[str] = None\n# cloud version uses the following configs\ncloud_project_id: Optional[str] = None\ncloud_db_id: Optional[str] = None\napi_key: Optional[str] = None\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(\ndefault_factory=lambda: {\n'TINYINT': {},\n'SMALLINT': {},\n'INT': {},\n'BIGINT': {},\n'FLOAT': {},\n'DOUBLE': {},\n'STRING': {},\n'BOOL': {},\n'JSON': {},\n'VECTOR_FLOAT': {},\n}\n)\ndef validate_config(self):\nif self.is_self_hosted:\nself.validate_self_hosted_config()\nelse:\nself.validate_cloud_config()\ndef validate_self_hosted_config(self):\nmissing_attributes = [\nattr\nfor attr in [\"protocol\", \"host\", \"port\", \"db_path\", \"db_name\"]\nif getattr(self, attr, None) is None\n]\nif missing_attributes:\nraise ValueError(\nf\"Missing required attributes for self-hosted version: {', '.join(missing_attributes)}\"\n)\ndef validate_cloud_config(self):\nmissing_attributes_cloud = [\nattr\nfor attr in [\"cloud_project_id\", \"cloud_db_id\", \"api_key\"]\nif getattr(self, attr, None) is None\n]\nif missing_attributes_cloud:\nraise ValueError(\nf\"Missing required attributes for cloud version: {', '.join(missing_attributes_cloud)}\"\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n# No dynamic config used\npass\n@property\ndef collection_name(self):\nreturn self._db_config.table_name\n@property\ndef index_name(self):\nreturn self.collection_name\ndef python_type_to_db_type(self, python_type: Type) -&gt; str:\n# AbstractTensor does not have n_dims, which is required by Epsilla\n# Use NdArray instead\nfor allowed_type in [list, np.ndarray, AbstractTensor]:\nif safe_issubclass(python_type, allowed_type):\nreturn 'VECTOR_FLOAT'\npy_type_map = {\nID: 'STRING',\nstr: 'STRING',\nbytes: 'STRING',\nint: 'BIGINT',\nfloat: 'FLOAT',\nbool: 'BOOL',\nnp.ndarray: 'VECTOR_FLOAT',\n}\nfor py_type, epsilla_type in py_type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn epsilla_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\nself._index_subindex(column_to_data)\nrows = list(self._transpose_col_value_dict(column_to_data))\nnormalized_rows = []\nfor row in rows:\nnormalized_row = {}\nfor key, value in row.items():\nif isinstance(value, NdArray):\nnormalized_row[key] = value.tolist()\nelif isinstance(value, np.ndarray):\nnormalized_row[key] = value.tolist()\nelse:\nnormalized_row[key] = value\nnormalized_rows.append(normalized_row)\nstatus_code, response = self._db.insert(\ntable_name=self._table_name, records=normalized_rows\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to insert documents. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\ndef num_docs(self) -&gt; int:\nraise NotImplementedError\n@property\ndef _is_index_empty(self) -&gt; bool:\n\"\"\"\n        Check if index is empty by comparing the number of documents to zero.\n        :return: True if the index is empty, False otherwise.\n        \"\"\"\n# Overriding this method to always return False because Epsilla does not have a count API for num_docs\nreturn False\ndef _del_items(self, doc_ids: Sequence[str]):\nstatus_code, response = self._db.delete(\ntable_name=self._table_name,\nprimary_keys=list(doc_ids),\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to get documents with ids {doc_ids}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nreturn response['message']\ndef _get_items(\nself, doc_ids: Sequence[str]\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\nstatus_code, response = self._db.get(\ntable_name=self._table_name,\nprimary_keys=list(doc_ids),\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to get documents with ids {doc_ids}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nreturn response['result']\ndef execute_query(self, query: Query) -&gt; DocList:\nif query.vector_query is not None:\nresult = self._find_with_filter_batched(\nqueries=np.expand_dims(query.vector_query, axis=0),\nfilter=query.filter,\nlimit=query.limit,\nsearch_field=query.vector_field,\n)\nreturn self._dict_list_to_docarray(result.documents[0])\nelse:\nreturn self._dict_list_to_docarray(\nself._filter(\nfilter_query=query.filter,\nlimit=query.limit,\n)\n)\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nreturn len(self._get_items([doc_id])) &gt; 0\ndef _find(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\nquery_batched = np.expand_dims(query, axis=0)\ndocs, scores = self._find_batched(\nqueries=query_batched, limit=limit, search_field=search_field\n)\nreturn _FindResult(documents=docs[0], scores=scores[0])\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nreturn self._find_with_filter_batched(\nqueries=queries, limit=limit, search_field=search_field\n)\ndef _find_with_filter_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str,\nfilter: Optional[str] = None,\n) -&gt; _FindResultBatched:\nif search_field == '':\nraise ValueError(\n'EpsillaDocumentIndex requires a search_field to be specified.'\n)\nresponses = []\nfor query in queries:\nstatus_code, response = self._db.query(\ntable_name=self._table_name,\nquery_field=search_field,\nlimit=limit,\nfilter=filter if filter is not None else '',\nquery_vector=query.tolist(),\nwith_distance=True,\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to find documents with query {query}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nresults = response['result']\nscores = NdArray._docarray_from_native(\nnp.array([result['@distance'] for result in results])\n)\ndocuments = []\nfor result in results:\ndoc = copy.copy(result)\ndel doc[\"@distance\"]\ndocuments.append(doc)\nresponses.append((documents, scores))\nreturn _FindResultBatched(\ndocuments=[r[0] for r in responses],\nscores=[r[1] for r in responses],\n)\ndef _filter(\nself,\nfilter_query: str,\nlimit: int,\n) -&gt; Union[DocList, List[Dict]]:\nquery_batched = [filter_query]\ndocs = self._filter_batched(filter_queries=query_batched, limit=limit)\nreturn docs[0]\ndef _filter_batched(\nself,\nfilter_queries: str,\nlimit: int,\n) -&gt; Union[List[DocList], List[List[Dict]]]:\nresponses = []\nfor filter_query in filter_queries:\nstatus_code, response = self._db.get(\ntable_name=self._table_name,\nlimit=limit,\nfilter=filter_query,\n)\nif status_code != HTTPStatus.OK:\nraise IOError(\nf\"Failed to find documents with filter {filter_query}. \"\nf\"Error code: {status_code}. Error message: {response}.\"\n)\nresults = response['result']\nresponses.append(results)\nreturn responses\ndef _text_search(\nself,\nquery: str,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _text_search_batched(\nself,\nqueries: Sequence[str],\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nraise NotImplementedError(f'{type(self)} does not support text search.')\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Static configuration for EpsillaDocumentIndex</p> Source code in <code>docarray/index/backends/epsilla.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Static configuration for EpsillaDocumentIndex\"\"\"\n# default value is the schema type name\ntable_name: Optional[str] = None\n# Indicator for self-hosted or cloud version\nis_self_hosted: bool = False\n# self-hosted version uses the following configs\nprotocol: Optional[str] = None\nhost: Optional[str] = None\nport: Optional[int] = 8888\ndb_path: Optional[str] = None\ndb_name: Optional[str] = None\n# cloud version uses the following configs\ncloud_project_id: Optional[str] = None\ncloud_db_id: Optional[str] = None\napi_key: Optional[str] = None\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(\ndefault_factory=lambda: {\n'TINYINT': {},\n'SMALLINT': {},\n'INT': {},\n'BIGINT': {},\n'FLOAT': {},\n'DOUBLE': {},\n'STRING': {},\n'BOOL': {},\n'JSON': {},\n'VECTOR_FLOAT': {},\n}\n)\ndef validate_config(self):\nif self.is_self_hosted:\nself.validate_self_hosted_config()\nelse:\nself.validate_cloud_config()\ndef validate_self_hosted_config(self):\nmissing_attributes = [\nattr\nfor attr in [\"protocol\", \"host\", \"port\", \"db_path\", \"db_name\"]\nif getattr(self, attr, None) is None\n]\nif missing_attributes:\nraise ValueError(\nf\"Missing required attributes for self-hosted version: {', '.join(missing_attributes)}\"\n)\ndef validate_cloud_config(self):\nmissing_attributes_cloud = [\nattr\nfor attr in [\"cloud_project_id\", \"cloud_db_id\", \"api_key\"]\nif getattr(self, attr, None) is None\n]\nif missing_attributes_cloud:\nraise ValueError(\nf\"Missing required attributes for cloud version: {', '.join(missing_attributes_cloud)}\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.Query","title":"<code>Query</code>  <code>dataclass</code>","text":"<p>Dataclass describing a query.</p> Source code in <code>docarray/index/backends/epsilla.py</code> <pre><code>@dataclass\nclass Query:\n\"\"\"Dataclass describing a query.\"\"\"\nvector_field: Optional[str]\nvector_query: Optional[NdArray]\nfilter: Optional[str]\nlimit: int\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/epsilla/#docarray.index.backends.epsilla.EpsillaDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/","title":"HnswDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/hnswlib/#hnswdocumentindex","title":"HnswDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex","title":"<code>docarray.index.backends.hnswlib.HnswDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>class HnswDocumentIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize HnswDocumentIndex\"\"\"\nif db_config is not None and getattr(db_config, 'index_name'):\ndb_config.work_dir = db_config.index_name.replace(\"__\", \"/\")\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(HnswDocumentIndex.DBConfig, self._db_config)\nself._work_dir = self._db_config.work_dir\nself._logger.debug(f'Working directory set to {self._work_dir}')\nload_existing = os.path.exists(self._work_dir) and glob.glob(\nf'{self._work_dir}/*.bin'\n)\nPath(self._work_dir).mkdir(parents=True, exist_ok=True)\n# HNSWLib setup\nself._index_construct_params = ('space', 'dim')\nself._index_init_params = (\n'max_elements',\n'ef_construction',\n'M',\n'allow_replace_deleted',\n)\nself._hnsw_locations = {\ncol_name: os.path.join(self._work_dir, f'{col_name}.bin')\nfor col_name, col in self._column_infos.items()\nif col.config\n}\nself._hnsw_indices = {}\nsub_docs_exist = False\ncosine_metric_index_exist = False\nfor col_name, col in self._column_infos.items():\nif '__' in col_name:\nsub_docs_exist = True\nif safe_issubclass(col.docarray_type, AnyDocArray):\ncontinue\nif not col.config or 'dim' not in col.config:\n# non-tensor type; don't create an index\ncontinue\nif not load_existing and (\n(not col.n_dim and col.config['dim'] &lt; 0) or not col.config['index']\n):\n# tensor type, but don't index\nself._logger.info(\nf'Not indexing column {col_name}; either `index=False` is set or no dimensionality is specified'\n)\ncontinue\nif load_existing:\nself._hnsw_indices[col_name] = self._load_index(col_name, col)\nself._logger.info(f'Loading an existing index for column `{col_name}`')\nelse:\nself._hnsw_indices[col_name] = self._create_index(col_name, col)\nself._logger.info(f'Created a new index for column `{col_name}`')\nif self._hnsw_indices[col_name].space == 'cosine':\ncosine_metric_index_exist = True\nself._apply_optim_no_embedding_in_sqlite = (\nnot sub_docs_exist and not cosine_metric_index_exist\n)  # optimization consisting in not serializing embeddings to SQLite because they are expensive to send and they can be reconstructed from the HNSW index itself.\n# SQLite setup\nself._sqlite_db_path = os.path.join(self._work_dir, 'docs_sqlite.db')\nself._logger.debug(f'DB path set to {self._sqlite_db_path}')\nself._sqlite_conn = sqlite3.connect(self._sqlite_db_path)\nself._logger.info('Connection to DB has been established')\nself._sqlite_cursor = self._sqlite_conn.cursor()\nself._column_names: List[str] = []\nself._create_docs_table()\nself._sqlite_conn.commit()\nself._num_docs = 0  # recompute again when needed\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n@property\ndef index_name(self):\nreturn self._db_config.work_dir  # type: ignore\n@property\ndef out_schema(self) -&gt; Type[BaseDoc]:\n\"\"\"Return the real schema of the index.\"\"\"\nif self._is_subindex:\nreturn self._ori_schema\nreturn cast(Type[BaseDoc], self._schema)\n###############################################\n# Inner classes for query builder and configs #\n###############################################\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_supported('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('find_batched')\ntext_search_batched = _raise_not_supported('text_search')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of HnswDocumentIndex.\"\"\"\nwork_dir: str = '.'\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nnp.ndarray: {\n'dim': -1,\n'index': True,  # if False, don't index at all\n'space': 'l2',  # 'l2', 'ip', 'cosine'\n'max_elements': 1024,\n'ef_construction': 200,\n'ef': 10,\n'M': 16,\n'allow_replace_deleted': True,\n'num_threads': 1,\n},\n},\n)\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of HnswDocumentIndex.\"\"\"\npass\n###############################################\n# Implementation of abstract methods          #\n###############################################\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type,\n            or None if ``python_type`` is not supported.\n        \"\"\"\nfor allowed_type in HNSWLIB_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nreturn np.ndarray\n# types allowed for filtering\ntype_map = {\nint: 'INTEGER',\nfloat: 'REAL',\nstr: 'TEXT',\n}\nfor py_type, sqlite_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn sqlite_type\nreturn None  # all types allowed, but no db type needed\ndef _index(\nself,\ncolumn_to_data: Dict[str, Generator[Any, None, None]],\ndocs_validated: Sequence[BaseDoc] = [],\n):\nself._index_subindex(column_to_data)\n# not needed, we implement `index` directly\nhashed_ids = tuple(self._to_hashed_id(doc.id) for doc in docs_validated)\n# indexing into HNSWLib and SQLite sequentially\n# could be improved by processing in parallel\nfor col_name, index in self._hnsw_indices.items():\ndata = column_to_data[col_name]\ndata_np = [self._to_numpy(arr) for arr in data]\ndata_stacked = np.stack(data_np)\nnum_docs_to_index = len(hashed_ids)\nindex_max_elements = index.get_max_elements()\ncurrent_elements = index.get_current_count()\nif current_elements + num_docs_to_index &gt; index_max_elements:\nnew_capacity = max(\nindex_max_elements, current_elements + num_docs_to_index\n)\nself._logger.info(f'Resizing the index to {new_capacity}')\nindex.resize_index(new_capacity)\nindex.add_items(data_stacked, ids=hashed_ids)\nindex.save_index(self._hnsw_locations[col_name])\ndef index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"Index Documents into the index.\n        !!! note\n            Passing a sequence of Documents that is not a DocList\n            (such as a List of Docs) comes at a performance penalty.\n            This is because the Index needs to check compatibility between itself and\n            the data. With a DocList as input this is a single check; for other inputs\n            compatibility needs to be checked for every Document individually.\n        :param docs: Documents to index.\n        \"\"\"\nif kwargs:\nraise ValueError(f'{list(kwargs.keys())} are not valid keyword arguments')\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, docs_validated, **kwargs)\nself._send_docs_to_sqlite(docs_validated)\nself._sqlite_conn.commit()\nself._num_docs = 0  # recompute again when needed\ndef execute_query(self, query: List[Tuple[str, Dict]], *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the HnswDocumentIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nreturn self._execute_find_and_filter_query(query)\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nreturn self._search_and_filter(\nqueries=queries, limit=limit, search_field=search_field\n)\ndef _find(\nself, query: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nquery_batched = np.expand_dims(query, axis=0)\ndocs, scores = self._find_batched(\nqueries=query_batched, limit=limit, search_field=search_field\n)\nreturn _FindResult(\ndocuments=docs[0], scores=NdArray._docarray_from_native(scores[0])\n)\ndef _filter(\nself,\nfilter_query: Any,\nlimit: int,\n) -&gt; DocList:\nrows = self._execute_filter(filter_query=filter_query, limit=limit)\nhashed_ids = [doc_id for doc_id, _ in rows]\nembeddings: OrderedDict[str, list] = OrderedDict()\nfor col_name, index in self._hnsw_indices.items():\nembeddings[col_name] = index.get_items(hashed_ids)\ndocs = DocList.__class_getitem__(cast(Type[BaseDoc], self.out_schema))()\nfor i, row in enumerate(rows):\nreconstruct_embeddings = {}\nfor col_name in embeddings.keys():\nreconstruct_embeddings[col_name] = embeddings[col_name][i]\ndocs.append(self._doc_from_bytes(row[1], reconstruct_embeddings))\nreturn docs\ndef _filter_batched(\nself,\nfilter_queries: Any,\nlimit: int,\n) -&gt; List[DocList]:\nraise NotImplementedError(\nf'{type(self)} does not support filter-only queries.'\nf' To perform post-filtering on a query, use'\nf' `build_query()` and `execute_query()`.'\n)\ndef _text_search(\nself,\nquery: str,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _text_search_batched(\nself,\nqueries: Sequence[str],\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _del_items(self, doc_ids: Sequence[str]):\n# delete from the indices\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor id in doc_ids:\ndoc = self.__getitem__(id)\nsub_ids = [sub_doc.id for sub_doc in getattr(doc, field_name)]\ndel self._subindices[field_name][sub_ids]\ntry:\nfor doc_id in doc_ids:\nid_ = self._to_hashed_id(doc_id)\nfor col_name, index in self._hnsw_indices.items():\nindex.mark_deleted(id_)\nexcept RuntimeError:\nraise KeyError(f'No document with id {doc_ids} found')\nself._delete_docs_from_sqlite(doc_ids)\nself._sqlite_conn.commit()\nself._num_docs = 0  # recompute again when needed\ndef _get_items(self, doc_ids: Sequence[str], out: bool = True) -&gt; Sequence[TSchema]:\n\"\"\"Get Documents from the hnswlib index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param doc_ids: ids to get from the Document index\n        :param out: return the documents in the original schema(True) or inner schema(False) for subindex\n        :return: Sequence of Documents, sorted corresponding to the order of `doc_ids`. Duplicate `doc_ids` can be omitted in the output.\n        \"\"\"\nout_docs = self._get_docs_sqlite_doc_id(doc_ids, out)\nif len(out_docs) == 0:\nraise KeyError(f'No document with id {doc_ids} found')\nreturn out_docs\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nhash_id = self._to_hashed_id(doc_id)\nself._sqlite_cursor.execute(f\"SELECT data FROM docs WHERE doc_id = '{hash_id}'\")\nrows = self._sqlite_cursor.fetchall()\nreturn len(rows) &gt; 0\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        \"\"\"\nif self._num_docs == 0:\nself._num_docs = self._get_num_docs_sqlite()\nreturn self._num_docs\n###############################################\n# Helpers                                     #\n###############################################\n# general helpers\n@staticmethod\ndef _to_hashed_id(doc_id: Optional[str]) -&gt; int:\n# https://stackoverflow.com/questions/16008670/how-to-hash-a-string-into-8-digits\n# hashing to 18 digits avoids overflow of sqlite INTEGER\nif doc_id is None:\nraise ValueError(\n'The Document id is None. To use DocumentIndex it needs to be set.'\n)\nreturn int(hashlib.sha256(doc_id.encode('utf-8')).hexdigest(), 16) % 10**18\ndef _load_index(self, col_name: str, col: '_ColumnInfo') -&gt; hnswlib.Index:\n\"\"\"Load an existing HNSW index from disk.\"\"\"\nindex = self._create_index_class(col)\nindex.load_index(self._hnsw_locations[col_name])\nreturn index\n# HNSWLib helpers\ndef _create_index_class(self, col: '_ColumnInfo') -&gt; hnswlib.Index:\n\"\"\"Create an instance of hnswlib.index without initializing it.\"\"\"\nconstruct_params = dict(\n(k, col.config[k]) for k in self._index_construct_params\n)\nif col.n_dim:\nconstruct_params['dim'] = col.n_dim\nreturn hnswlib.Index(**construct_params)\ndef _create_index(self, col_name: str, col: '_ColumnInfo') -&gt; hnswlib.Index:\n\"\"\"Create a new HNSW index for a column, and initialize it.\"\"\"\nindex = self._create_index_class(col)\ninit_params = dict((k, col.config[k]) for k in self._index_init_params)\nindex.init_index(**init_params)\nindex.set_ef(col.config['ef'])\nindex.set_num_threads(col.config['num_threads'])\nindex.save_index(self._hnsw_locations[col_name])\nreturn index\n# SQLite helpers\ndef _create_docs_table(self):\ncolumns: List[Tuple[str, str]] = []\nfor col, info in self._column_infos.items():\nif (\ncol == 'id'\nor '__' in col\nor not info.db_type\nor info.db_type == np.ndarray\n):\ncontinue\ncolumns.append((col, info.db_type))\ncolumns_str = ', '.join(f'{name} {type}' for name, type in columns)\nif columns_str:\ncolumns_str = ', ' + columns_str\nquery = f'CREATE TABLE IF NOT EXISTS docs (doc_id INTEGER PRIMARY KEY, data BLOB{columns_str})'\nself._sqlite_cursor.execute(query)\ndef _send_docs_to_sqlite(self, docs: Sequence[BaseDoc]):\n# Generate the IDs\nids = (self._to_hashed_id(doc.id) for doc in docs)\ncolumn_names = self._get_column_names()\n# Construct the field names and placeholders for the SQL query\nall_fields = ', '.join(column_names)\nplaceholders = ', '.join(['?'] * len(column_names))\n# Prepare the SQL statement\nquery = f'INSERT OR REPLACE INTO docs ({all_fields}) VALUES ({placeholders})'\n# Prepare the data for insertion\ndata_to_insert = (\n(id_, self._doc_to_bytes(doc))\n+ tuple(getattr(doc, field) for field in column_names[2:])\nfor id_, doc in zip(ids, docs)\n)\n# Execute the query\nself._sqlite_cursor.executemany(query, data_to_insert)\ndef _get_docs_sqlite_unsorted(self, univ_ids: Sequence[int], out: bool = True):\nfor id_ in univ_ids:\n# I hope this protects from injection attacks\n# properly binding with '?' doesn't work for some reason\nassert isinstance(id_, int) or is_np_int(id_)\nsql_id_list = '(' + ', '.join(str(id_) for id_ in univ_ids) + ')'\nself._sqlite_cursor.execute(\n'SELECT doc_id, data FROM docs WHERE doc_id IN %s' % sql_id_list,\n)\nrows = (\nself._sqlite_cursor.fetchall()\n)  # doc_ids do not come back in the same order\nembeddings: OrderedDict[str, list] = OrderedDict()\nfor col_name, index in self._hnsw_indices.items():\nembeddings[col_name] = index.get_items([row[0] for row in rows])\nschema = self.out_schema if out else self._schema\ndocs = DocList.__class_getitem__(cast(Type[BaseDoc], schema))()\nfor i, (_, data_bytes) in enumerate(rows):\nreconstruct_embeddings = {}\nfor col_name in embeddings.keys():\nreconstruct_embeddings[col_name] = embeddings[col_name][i]\ndocs.append(self._doc_from_bytes(data_bytes, reconstruct_embeddings, out))\nreturn docs\ndef _get_docs_sqlite_doc_id(\nself, doc_ids: Sequence[str], out: bool = True\n) -&gt; DocList[TSchema]:\nhashed_ids = tuple(self._to_hashed_id(id_) for id_ in doc_ids)\ndocs_unsorted = self._get_docs_sqlite_unsorted(hashed_ids, out)\nschema = self.out_schema if out else self._schema\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], schema))\nreturn docs_cls(sorted(docs_unsorted, key=lambda doc: doc_ids.index(doc.id)))\ndef _get_docs_sqlite_hashed_id(self, hashed_ids: Sequence[int]) -&gt; DocList:\ndocs_unsorted = self._get_docs_sqlite_unsorted(hashed_ids)\ndef _in_position(doc):\nreturn hashed_ids.index(self._to_hashed_id(doc.id))\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self.out_schema))\nreturn docs_cls(sorted(docs_unsorted, key=_in_position))\ndef _delete_docs_from_sqlite(self, doc_ids: Sequence[Union[str, int]]):\nids = tuple(\nself._to_hashed_id(id_) if isinstance(id_, str) else id_ for id_ in doc_ids\n)\nself._sqlite_cursor.execute(\n'DELETE FROM docs WHERE doc_id IN (%s)' % ','.join('?' * len(ids)),\nids,\n)\ndef _get_num_docs_sqlite(self) -&gt; int:\nself._sqlite_cursor.execute('SELECT COUNT(*) FROM docs')\nreturn self._sqlite_cursor.fetchone()[0]\n# serialization helpers\ndef _doc_to_bytes(self, doc: BaseDoc) -&gt; bytes:\npb = doc.to_protobuf()\nif self._apply_optim_no_embedding_in_sqlite:\nfor col_name in self._hnsw_indices.keys():\npb.data[col_name].Clear()\npb.data[col_name].Clear()\nreturn pb.SerializeToString()\ndef _doc_from_bytes(\nself, data: bytes, reconstruct_embeddings: Dict, out: bool = True\n) -&gt; BaseDoc:\nschema = self.out_schema if out else self._schema\nschema_cls = cast(Type[BaseDoc], schema)\npb = DocProto.FromString(\ndata\n)  # I cannot reconstruct directly the DA object because it may fail at validation because embedding may not be Optional\nif self._apply_optim_no_embedding_in_sqlite:\nfor k, v in reconstruct_embeddings.items():\nnode_proto = (\nschema_cls._get_field_annotation(k)\n._docarray_from_ndarray(np.array(v))\n._to_node_protobuf()\n)\npb.data[k].MergeFrom(node_proto)\ndoc = schema_cls.from_protobuf(pb)\nreturn doc\ndef _get_root_doc_id(self, id: str, root: str, sub: str) -&gt; str:\n\"\"\"Get the root_id given the id of a subindex Document and the root and subindex name for hnswlib.\n        :param id: id of the subindex Document\n        :param root: root index name\n        :param sub: subindex name\n        :return: the root_id of the Document\n        \"\"\"\nsubindex = self._subindices[root]\nif not sub:\nsub_doc = subindex._get_items([id], out=False)  # type: ignore\nparent_id = (\nsub_doc[0]['parent_id']\nif isinstance(sub_doc[0], dict)\nelse sub_doc[0].parent_id\n)\nreturn parent_id\nelse:\nfields = sub.split('__')\ncur_root_id = subindex._get_root_doc_id(\nid, fields[0], '__'.join(fields[1:])\n)\nreturn self._get_root_doc_id(cur_root_id, root, '')\ndef _get_column_names(self) -&gt; List[str]:\n\"\"\"\n        Retrieves the column names of the 'docs' table in the SQLite database.\n        The column names are cached in `self._column_names` to prevent multiple queries to the SQLite database.\n        :return: A list of strings, where each string is a column name.\n        \"\"\"\nif not self._column_names:\nself._sqlite_cursor.execute('PRAGMA table_info(docs)')\ninfo = self._sqlite_cursor.fetchall()\nself._column_names = [row[1] for row in info]\nreturn self._column_names\ndef _search_and_filter(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\nhashed_ids: Optional[Set[str]] = None,\n) -&gt; _FindResultBatched:\n\"\"\"\n        Executes a search and filter operation on the database.\n        :param queries: A numpy array of queries.\n        :param limit: The maximum number of results to return.\n        :param search_field: The field to search in.\n        :param hashed_ids: A set of hashed IDs to filter the results with.\n        :return: An instance of _FindResultBatched, containing the matching\n            documents and their corresponding scores.\n        \"\"\"\n# If there are no documents or hashed_ids is an empty set, return an empty _FindResultBatched\nif hashed_ids is not None and len(hashed_ids) == 0:\nreturn _FindResultBatched(documents=[], scores=[])  # type: ignore\n# Set limit as the minimum of the provided limit and the total number of documents\nlimit = limit\n# Ensure the search field is in the HNSW indices\nif search_field not in self._hnsw_indices:\nraise ValueError(\nf'Search field {search_field} is not present in the HNSW indices'\n)\ndef accept_hashed_ids(id):\n\"\"\"Accepts IDs that are in hashed_ids.\"\"\"\nreturn id in hashed_ids  # type: ignore[operator]\nextra_kwargs = {'filter': accept_hashed_ids} if hashed_ids else {}\n# If hashed_ids is provided, k is the minimum of limit and the length of hashed_ids; else it is limit\nk = min(limit, len(hashed_ids)) if hashed_ids else limit\nindex = self._hnsw_indices[search_field]\ntry:\nlabels, distances = index.knn_query(queries, k=k, **extra_kwargs)\nexcept RuntimeError:\nk = min(k, self.num_docs())\nlabels, distances = index.knn_query(queries, k=k, **extra_kwargs)\nresult_das = [\nself._get_docs_sqlite_hashed_id(\nids_per_query.tolist(),\n)\nfor ids_per_query in labels\n]\nreturn _FindResultBatched(documents=result_das, scores=distances)\n@classmethod\ndef _build_filter_query(\ncls, query: Union[Dict, str], param_values: List[Any]\n) -&gt; str:\n\"\"\"\n        Builds a filter query for database operations.\n        :param query: Query for filtering.\n        :param param_values: A list to store the parameters for the query.\n        :return: A string representing a SQL filter query.\n        \"\"\"\nif not isinstance(query, dict):\nraise ValueError('Invalid query')\nif len(query) != 1:\nraise ValueError('Each nested dict must have exactly one key')\nkey, value = next(iter(query.items()))\nif key in ['$and', '$or']:\n# Combine subqueries using the AND or OR operator\nsubqueries = [cls._build_filter_query(q, param_values) for q in value]\nreturn f'({f\" {key[1:].upper()} \".join(subqueries)})'\nelif key == '$not':\n# Negate the query\nreturn f'NOT {cls._build_filter_query(value, param_values)}'\nelse:  # normal field\nfield = key\nif not isinstance(value, dict) or len(value) != 1:\nraise ValueError(f'Invalid condition for field {field}')\noperator_key, operator_value = next(iter(value.items()))\nif operator_key == \"$exists\":\n# Check for the existence or non-existence of a field\nif operator_value:\nreturn f'{field} IS NOT NULL'\nelse:\nreturn f'{field} IS NULL'\nelif operator_key not in OPERATOR_MAPPING:\nraise ValueError(f\"Invalid operator {operator_key}\")\nelse:\n# If the operator is valid, create a placeholder and append the value to param_values\noperator = OPERATOR_MAPPING[operator_key]\nplaceholder = '?'\nparam_values.append(operator_value)\nreturn f'{field} {operator} {placeholder}'\ndef _execute_filter(\nself,\nfilter_query: Any,\nlimit: int,\n) -&gt; List[Tuple[str, bytes]]:\n\"\"\"\n        Executes a filter query on the database.\n        :param filter_query: Query for filtering.\n        :param limit: Maximum number of rows to be fetched.\n        :return: A list of rows fetched from the database.\n        \"\"\"\nparam_values: List[Any] = []\nsql_query = self._build_filter_query(filter_query, param_values)\nsql_query = f'SELECT doc_id, data FROM docs WHERE {sql_query} LIMIT {limit}'\nreturn self._sqlite_cursor.execute(sql_query, param_values).fetchall()\ndef _execute_find_and_filter_query(\nself, query: List[Tuple[str, Dict]]\n) -&gt; FindResult:\n\"\"\"\n        Executes a query to find and filter documents.\n        :param query: A list of operations and their corresponding arguments.\n        :return: A FindResult object containing filtered documents and their scores.\n        \"\"\"\n# Dictionary to store the score of each document\ndoc_to_score: Dict[BaseDoc, Any] = {}\n# Pre- and post-filter conditions\npre_filters: Dict[str, Dict] = {}\npost_filters: Dict[str, Dict] = {}\n# Define filter limits\npre_filter_limit = self.num_docs()\npost_filter_limit = self.num_docs()\nfind_executed: bool = False\n# Document list with output schema\nout_docs: DocList = DocList[self.out_schema]()  # type: ignore[name-defined]\nfor op, op_kwargs in query:\nif op == 'find':\nhashed_ids: Optional[Set[str]] = None\nif pre_filters:\nhashed_ids = self._pre_filtering(pre_filters, pre_filter_limit)\nquery_vector = self._get_vector_for_query_builder(op_kwargs)\n# Perform search and filter if hashed_ids returned by pre-filtering is not empty\nif not (pre_filters and not hashed_ids):\n# Returns batched output, so we need to get the first lists\nout_docs, scores = self._search_and_filter(  # type: ignore[assignment]\nqueries=query_vector,\nlimit=op_kwargs.get('limit', self.num_docs()),\nsearch_field=op_kwargs['search_field'],\nhashed_ids=hashed_ids,\n)\nout_docs = DocList[self.out_schema](out_docs[0])  # type: ignore[name-defined]\ndoc_to_score.update(zip(out_docs.__getattribute__('id'), scores[0]))\nfind_executed = True\nelif op == 'filter':\nif find_executed:\npost_filters, post_filter_limit = self._update_filter_conditions(\npost_filters, op_kwargs, post_filter_limit\n)\nelse:\npre_filters, pre_filter_limit = self._update_filter_conditions(\npre_filters, op_kwargs, pre_filter_limit\n)\nelse:\nraise ValueError(f'Query operation is not supported: {op}')\nif post_filters:\nout_docs = self._post_filtering(\nout_docs, post_filters, post_filter_limit, find_executed\n)\nreturn self._prepare_out_docs(out_docs, doc_to_score)\ndef _update_filter_conditions(\nself, filter_conditions: Dict, operation_args: Dict, filter_limit: int\n) -&gt; Tuple[Dict, int]:\n\"\"\"\n        Updates filter conditions based on the operation arguments and updates the filter limit.\n        :param filter_conditions: Current filter conditions.\n        :param operation_args: Arguments of the operation to be executed.\n        :param filter_limit: Current filter limit.\n        :return: Updated filter conditions and filter limit.\n        \"\"\"\n# Use '$and' operator if filter_conditions is not empty, else use operation_args['filter_query']\nupdated_filter_conditions = (\n{'$and': {**filter_conditions, **operation_args['filter_query']}}\nif filter_conditions\nelse operation_args['filter_query']\n)\n# Update filter limit based on the operation_args limit\nupdated_filter_limit = min(\nfilter_limit, operation_args.get('limit', filter_limit)\n)\nreturn updated_filter_conditions, updated_filter_limit\ndef _pre_filtering(\nself, pre_filters: Dict[str, Dict], pre_filter_limit: int\n) -&gt; Set[str]:\n\"\"\"\n        Performs pre-filtering on the data.\n        :param pre_filters: Filter conditions.\n        :param pre_filter_limit: Limit for the filtering.\n        :return: A set of hashed IDs from the filtered rows.\n        \"\"\"\nrows = self._execute_filter(filter_query=pre_filters, limit=pre_filter_limit)\nreturn set(hashed_id for hashed_id, _ in rows)\ndef _get_vector_for_query_builder(self, find_args: Dict[str, Any]) -&gt; np.ndarray:\n\"\"\"\n        Prepares the query vector for search operation.\n        :param find_args: Arguments for the 'find' operation.\n        :return: A numpy array representing the query vector.\n        \"\"\"\nif isinstance(find_args['query'], BaseDoc):\nquery_vec = self._get_values_by_column(\n[find_args['query']], find_args['search_field']\n)[0]\nelse:\nquery_vec = find_args['query']\nquery_vec_np = self._to_numpy(query_vec)\nquery_batched = np.expand_dims(query_vec_np, axis=0)\nreturn query_batched\ndef _post_filtering(\nself,\nout_docs: DocList,\npost_filters: Dict[str, Dict],\npost_filter_limit: int,\nfind_executed: bool,\n) -&gt; DocList:\n\"\"\"\n        Performs post-filtering on the found documents.\n        :param out_docs: The documents found by the 'find' operation.\n        :param post_filters: The post-filter conditions.\n        :param post_filter_limit: Limit for the post-filtering.\n        :param find_executed: Whether 'find' operation was executed.\n        :return: Filtered documents as per the post-filter conditions.\n        \"\"\"\nif not find_executed:\nout_docs = self.filter(post_filters, limit=self.num_docs())\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self.out_schema))\nout_docs = docs_cls(filter_docs(out_docs, post_filters))\nif post_filters:\nout_docs = out_docs[:post_filter_limit]\nreturn out_docs\ndef _prepare_out_docs(\nself, out_docs: DocList, doc_to_score: Dict[BaseDoc, Any]\n) -&gt; FindResult:\n\"\"\"\n        Prepares output documents with their scores.\n        :param out_docs: The documents to be output.\n        :param doc_to_score: Mapping of documents to their scores.\n        :return: FindResult object with documents and their scores.\n        \"\"\"\nif out_docs:\n# If the \"find\" operation isn't called through the query builder,\n# all returned scores will be 0\ndocs_and_scores = zip(\nout_docs, (doc_to_score.get(doc.id, 0) for doc in out_docs)\n)\ndocs_sorted = sorted(docs_and_scores, key=lambda x: x[1])\nout_docs, out_scores = zip(*docs_sorted)\nelse:\nout_docs, out_scores = [], []  # type: ignore[assignment]\nreturn FindResult(documents=out_docs, scores=out_scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.out_schema","title":"<code>out_schema: Type[BaseDoc]</code>  <code>property</code>","text":"<p>Return the real schema of the index.</p>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of HnswDocumentIndex.</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of HnswDocumentIndex.\"\"\"\nwork_dir: str = '.'\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nnp.ndarray: {\n'dim': -1,\n'index': True,  # if False, don't index at all\n'space': 'l2',  # 'l2', 'ip', 'cosine'\n'max_elements': 1024,\n'ef_construction': 200,\n'ef': 10,\n'M': 16,\n'allow_replace_deleted': True,\n'num_threads': 1,\n},\n},\n)\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_supported('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('find_batched')\ntext_search_batched = _raise_not_supported('text_search')\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the query object.</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of HnswDocumentIndex.</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of HnswDocumentIndex.\"\"\"\npass\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize HnswDocumentIndex</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize HnswDocumentIndex\"\"\"\nif db_config is not None and getattr(db_config, 'index_name'):\ndb_config.work_dir = db_config.index_name.replace(\"__\", \"/\")\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(HnswDocumentIndex.DBConfig, self._db_config)\nself._work_dir = self._db_config.work_dir\nself._logger.debug(f'Working directory set to {self._work_dir}')\nload_existing = os.path.exists(self._work_dir) and glob.glob(\nf'{self._work_dir}/*.bin'\n)\nPath(self._work_dir).mkdir(parents=True, exist_ok=True)\n# HNSWLib setup\nself._index_construct_params = ('space', 'dim')\nself._index_init_params = (\n'max_elements',\n'ef_construction',\n'M',\n'allow_replace_deleted',\n)\nself._hnsw_locations = {\ncol_name: os.path.join(self._work_dir, f'{col_name}.bin')\nfor col_name, col in self._column_infos.items()\nif col.config\n}\nself._hnsw_indices = {}\nsub_docs_exist = False\ncosine_metric_index_exist = False\nfor col_name, col in self._column_infos.items():\nif '__' in col_name:\nsub_docs_exist = True\nif safe_issubclass(col.docarray_type, AnyDocArray):\ncontinue\nif not col.config or 'dim' not in col.config:\n# non-tensor type; don't create an index\ncontinue\nif not load_existing and (\n(not col.n_dim and col.config['dim'] &lt; 0) or not col.config['index']\n):\n# tensor type, but don't index\nself._logger.info(\nf'Not indexing column {col_name}; either `index=False` is set or no dimensionality is specified'\n)\ncontinue\nif load_existing:\nself._hnsw_indices[col_name] = self._load_index(col_name, col)\nself._logger.info(f'Loading an existing index for column `{col_name}`')\nelse:\nself._hnsw_indices[col_name] = self._create_index(col_name, col)\nself._logger.info(f'Created a new index for column `{col_name}`')\nif self._hnsw_indices[col_name].space == 'cosine':\ncosine_metric_index_exist = True\nself._apply_optim_no_embedding_in_sqlite = (\nnot sub_docs_exist and not cosine_metric_index_exist\n)  # optimization consisting in not serializing embeddings to SQLite because they are expensive to send and they can be reconstructed from the HNSW index itself.\n# SQLite setup\nself._sqlite_db_path = os.path.join(self._work_dir, 'docs_sqlite.db')\nself._logger.debug(f'DB path set to {self._sqlite_db_path}')\nself._sqlite_conn = sqlite3.connect(self._sqlite_db_path)\nself._logger.info('Connection to DB has been established')\nself._sqlite_cursor = self._sqlite_conn.cursor()\nself._column_names: List[str] = []\nself._create_docs_table()\nself._sqlite_conn.commit()\nself._num_docs = 0  # recompute again when needed\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the HnswDocumentIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>List[Tuple[str, Dict]]</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def execute_query(self, query: List[Tuple[str, Dict]], *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the HnswDocumentIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nreturn self._execute_find_and_filter_query(query)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>Index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"Index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nif kwargs:\nraise ValueError(f'{list(kwargs.keys())} are not valid keyword arguments')\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, docs_validated, **kwargs)\nself._send_docs_to_sqlite(docs_validated)\nself._sqlite_conn.commit()\nself._num_docs = 0  # recompute again when needed\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nif self._num_docs == 0:\nself._num_docs = self._get_num_docs_sqlite()\nreturn self._num_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/hnswlib.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\nfor allowed_type in HNSWLIB_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nreturn np.ndarray\n# types allowed for filtering\ntype_map = {\nint: 'INTEGER',\nfloat: 'REAL',\nstr: 'TEXT',\n}\nfor py_type, sqlite_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn sqlite_type\nreturn None  # all types allowed, but no db type needed\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/hnswlib/#docarray.index.backends.hnswlib.HnswDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/","title":"In memory","text":""},{"location":"API_reference/doc_index/backends/in_memory/#inmemoryexactnnindex","title":"InMemoryExactNNIndex","text":""},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex","title":"<code>docarray.index.backends.in_memory.InMemoryExactNNIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>class InMemoryExactNNIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(\nself,\ndocs: Optional[DocList] = None,\ndb_config=None,\n**kwargs,\n):\n\"\"\"Initialize InMemoryExactNNIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._runtime_config = self.RuntimeConfig()\nself._db_config = cast(InMemoryExactNNIndex.DBConfig, self._db_config)\nself._index_file_path = self._db_config.index_file_path\nif docs and self._index_file_path:\nraise ValueError(\n'Initialize `InMemoryExactNNIndex` with either `docs` or '\n'`index_file_path`, not both. Provide `docs` for a fresh index, or '\n'`index_file_path` to use an existing file.'\n)\nif self._index_file_path:\nif os.path.exists(self._index_file_path):\nself._logger.info(\nf'Loading index from a binary file: {self._index_file_path}'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n).load_binary(file=self._index_file_path)\ndata_by_columns = self._get_col_value_dict(self._docs)\nself._update_subindex_data(self._docs)\nself._index_subindex(data_by_columns)\nelse:\nself._logger.warning(\nf'Index file does not exist: {self._index_file_path}. '\nf'Initializing empty InMemoryExactNNIndex.'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n)()\nelse:\nif docs:\nself._logger.info('Docs provided. Initializing with provided docs.')\nself._docs = docs\nelse:\nself._logger.info(\n'No docs or index file provided. Initializing empty InMemoryExactNNIndex.'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n)()\nself._embedding_map: Dict[str, Tuple[AnyTensor, Optional[List[int]]]] = {}\nself._ids_to_positions: Dict[str, int] = {}\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type,\n            or None if ``python_type`` is not supported.\n        \"\"\"\nreturn python_type\n@property\ndef out_schema(self) -&gt; Type[BaseDoc]:\n\"\"\"Return the original schema (without the parent_id from new_schema type)\"\"\"\nif self._is_subindex:\nreturn self._ori_schema\nreturn cast(Type[BaseDoc], self._schema)\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfind_batched = _collect_query_args('find_batched')\nfilter = _collect_query_args('filter')\nfilter_batched = _raise_not_supported('find_batched')\ntext_search = _raise_not_supported('text_search')\ntext_search_batched = _raise_not_supported('text_search')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of InMemoryExactNNIndex.\"\"\"\nindex_file_path: Optional[str] = None\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nAbstractTensor: {'space': 'cosine_sim'},\n},\n)\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of InMemoryExactNNIndex.\"\"\"\npass\ndef index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n        !!! note\n            Passing a sequence of Documents that is not a DocList\n            (such as a List of Docs) comes at a performance penalty.\n            This is because the Index needs to check compatibility between itself and\n            the data. With a DocList as input this is a single check; for other inputs\n            compatibility needs to be checked for every Document individually.\n        :param docs: Documents to index.\n        \"\"\"\n# implementing the public option because conversion to column dict is not needed\ndocs = self._validate_docs(docs)\nids_to_positions = self._get_ids_to_positions()\nfor doc in docs:\nif doc.id in ids_to_positions:\nself._docs[ids_to_positions[doc.id]] = doc\nelse:\nself._docs.append(doc)\nself._ids_to_positions[str(doc.id)] = len(self._ids_to_positions)\n# Add parent_id to all sub-index documents and store sub-index documents\ndata_by_columns = self._get_col_value_dict(docs)\nself._update_subindex_data(docs)\nself._index_subindex(data_by_columns)\nself._rebuild_embedding()\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\nraise NotImplementedError\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        \"\"\"\nreturn len(self._docs)\ndef _rebuild_embedding(self):\n\"\"\"\n        Reconstructs the embeddings map for each field. This is performed to store pre-stacked\n        embeddings, thereby optimizing performance by avoiding repeated stacking of embeddings.\n        Note: '_embedding_map' is a dictionary mapping fields to their corresponding embeddings.\n        \"\"\"\nif self._is_index_empty:\nself._embedding_map = dict()\nelse:\nfor field_, embedding in self._embedding_map.items():\nself._embedding_map[field_] = _extract_embeddings(self._docs, field_)\ndef _del_items(self, doc_ids: Sequence[str]):\n\"\"\"Delete Documents from the index.\n        :param doc_ids: ids to delete from the Document Store\n        \"\"\"\nfor field_, type_, _ in self._flatten_schema(cast(Type[BaseDoc], self._schema)):\nif safe_issubclass(type_, AnyDocArray):\nfor id in doc_ids:\ndoc_ = self._get_items([id])\nif len(doc_) == 0:\nraise KeyError(\nf\"The document (id = '{id}') does not exist in the ExactNNIndexer.\"\n)\nsub_ids = [sub_doc.id for sub_doc in getattr(doc_[0], field_)]\ndel self._subindices[field_][sub_ids]\nindices = []\nfor i, doc in enumerate(self._docs):\nif doc.id in doc_ids:\nindices.append(i)\ndel self._docs[indices]\nself._update_ids_to_positions()\nself._rebuild_embedding()\ndef _ori_items(self, doc: BaseDoc) -&gt; BaseDoc:\n\"\"\"\n        The Indexer's backend stores parent_id to support nested data. However,\n        this method enables us to retrieve the original items in their original\n        type, which is what the user interacts with.\n        :param doc: The input document in New_Schema format from the Indexer's backend.\n        :return: The input document with its original schema.\n        \"\"\"\nori_doc = _shallow_copy_doc(doc)\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self.out_schema)\n):\nif safe_issubclass(type_, AnyDocArray):\n_list = getattr(ori_doc, field_name)\nfor i, nested_doc in enumerate(_list):\nsub_indexer: InMemoryExactNNIndex = cast(\nInMemoryExactNNIndex, self._subindices[field_name]\n)\nnested_doc = self._subindices[field_name]._ori_schema(\n**nested_doc.__dict__\n)\n_list[i] = sub_indexer._ori_items(nested_doc)\nreturn ori_doc\ndef _get_items(\nself, doc_ids: Sequence[str], raw: bool = False\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\n\"\"\"Get Documents from the index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param doc_ids: ids to get from the Document index\n        :param raw: if raw, output the new_schema type (with parent id)\n        :return: Sequence of Documents, sorted corresponding to the order of `doc_ids`.\n            Duplicate `doc_ids` can be omitted in the output.\n        \"\"\"\nout_docs = []\nids_to_positions = self._get_ids_to_positions()\nfor doc_id in doc_ids:\nif doc_id not in ids_to_positions:\ncontinue\ndoc = self._docs[ids_to_positions[doc_id]]\nif raw:\nout_docs.append(doc)\nelse:\nori_doc = self._ori_items(doc)\nschema_cls = cast(Type[BaseDoc], self.out_schema)\nnew_doc = schema_cls(**ori_doc.__dict__)\nout_docs.append(new_doc)\nreturn out_docs\ndef execute_query(self, query: List[Tuple[str, Dict]], *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the InMemoryExactNNIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nreturn self._find_and_filter(query)\ndef _find_and_filter(self, query: List[Tuple[str, Dict]]) -&gt; FindResult:\n\"\"\"\n        The function executes search operations such as 'find' and 'filter' in the order\n        they appear in the query. The 'find' operation performs a vector similarity search.\n        The 'filter' operation filters out documents based on a filter query.\n        The documents are finally sorted based on their scores.\n        :param query: The query to execute.\n        :return: A tuple of retrieved documents and their scores.\n        \"\"\"\nout_docs = self._docs\ndoc_to_score: Dict[BaseDoc, Any] = {}\nfor op, op_kwargs in query:\nif op == 'find':\nout_docs, scores = find(\nindex=out_docs,\nquery=op_kwargs['query'],\nsearch_field=op_kwargs['search_field'],\nlimit=op_kwargs.get('limit', len(out_docs)),\nmetric=self._column_infos[op_kwargs['search_field']].config[\n'space'\n],\n)\ndoc_to_score.update(zip(out_docs.id, scores))\nelif op == 'filter':\nout_docs = filter_docs(out_docs, op_kwargs['filter_query'])\nif 'limit' in op_kwargs:\nout_docs = out_docs[: op_kwargs['limit']]\nelse:\nraise ValueError(f'Query operation is not supported: {op}')\nscores_and_docs = zip([doc_to_score[doc.id] for doc in out_docs], out_docs)\nsorted_lists = sorted(scores_and_docs, reverse=True)\nout_scores, out_docs = zip(*sorted_lists)\nreturn FindResult(documents=out_docs, scores=out_scores)\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find Documents in the index using nearest-neighbor search.\n        :param query: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.)\n            with a single axis, or a Document\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of Documents to return\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif self._is_index_empty:\nreturn FindResult(documents=[], scores=[])  # type: ignore\nconfig = self._column_infos[search_field].config\ndocs, scores = find(\nindex=self._docs,\nquery=query,\nsearch_field=search_field,\nlimit=limit,\nmetric=config['space'],\ncache=self._embedding_map,\n)\ndocs_ = []\nfor doc in docs:\nori_doc = self._ori_items(doc)\nschema_cls = cast(Type[BaseDoc], self.out_schema)\ndocs_.append(schema_cls(**ori_doc.__dict__))\ndocs_with_schema = DocList.__class_getitem__(\ncast(Type[BaseDoc], self.out_schema)\n)(docs_)\nreturn FindResult(documents=docs_with_schema, scores=scores)\ndef _find(\nself, query: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nraise NotImplementedError\ndef find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find Documents in the index using nearest-neighbor search.\n        :param queries: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n            or a DocList.\n            If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return per query\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nself._validate_search_field(search_field)\nif self._is_index_empty:\nreturn FindResultBatched(documents=[], scores=[])  # type: ignore\nconfig = self._column_infos[search_field].config\nfind_res = find_batched(\nindex=self._docs,\nquery=cast(NdArray, queries),\nsearch_field=search_field,\nlimit=limit,\nmetric=config['space'],\ncache=self._embedding_map,\n)\nreturn find_res\ndef _find_batched(\nself, queries: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\nraise NotImplementedError\ndef filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n        :param filter_query: the filter query to execute following the query\n            language of\n        :param limit: maximum number of documents to return\n        :return: a DocList containing the documents that match the filter query\n        \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = filter_docs(docs=self._docs, query=filter_query)[:limit]\nreturn cast(DocList, docs)\ndef _filter(self, filter_query: Any, limit: int) -&gt; Union[DocList, List[Dict]]:\nraise NotImplementedError\ndef _filter_batched(\nself, filter_queries: Any, limit: int\n) -&gt; Union[List[DocList], List[List[Dict]]]:\nraise NotImplementedError(f'{type(self)} does not support filtering.')\ndef _text_search(\nself, query: str, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _text_search_batched(\nself, queries: Sequence[str], limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nreturn doc_id in self._get_ids_to_positions()\ndef persist(self, file: Optional[str] = None) -&gt; None:\n\"\"\"Persist InMemoryExactNNIndex into a binary file.\"\"\"\nDEFAULT_INDEX_FILE_PATH = 'in_memory_index.bin'\nfile_to_save = self._index_file_path or file\nif file_to_save is None:\nself._logger.warning(\nf'persisting index to {DEFAULT_INDEX_FILE_PATH} because no `index_file_path` has been used inside DBConfig and no `file` has been passed as argument'\n)\nfile_to_save = file_to_save or DEFAULT_INDEX_FILE_PATH\nself._docs.save_binary(file=file_to_save)\ndef _get_root_doc_id(self, id: str, root: str, sub: str) -&gt; str:\n\"\"\"Get the root_id given the id of a subindex Document and the root and subindex name\n        :param id: id of the subindex Document\n        :param root: root index name\n        :param sub: subindex name\n        :return: the root_id of the Document\n        \"\"\"\nsubindex: InMemoryExactNNIndex = cast(\nInMemoryExactNNIndex, self._subindices[root]\n)\nif not sub:\nsub_doc = subindex._get_items([id], raw=True)\nparent_id = (\nsub_doc[0]['parent_id']\nif isinstance(sub_doc[0], dict)\nelse sub_doc[0].parent_id\n)\nreturn parent_id\nelse:\nfields = sub.split('__')\ncur_root_id = subindex._get_root_doc_id(\nid, fields[0], '__'.join(fields[1:])\n)\nreturn self._get_root_doc_id(cur_root_id, root, '')\ndef _get_ids_to_positions(self) -&gt; Dict[str, int]:\n\"\"\"\n        Obtains a mapping between document IDs and their respective positions\n        within the DocList. If this mapping hasn't been initialized, it will be created.\n        :return: A dictionary mapping each document ID to its corresponding position.\n        \"\"\"\nif not self._ids_to_positions:\nself._update_ids_to_positions()\nreturn self._ids_to_positions\ndef _update_ids_to_positions(self) -&gt; None:\n\"\"\"\n        Generates or updates the mapping between document IDs and their corresponding\n        positions within the DocList.\n        \"\"\"\nself._ids_to_positions = {doc.id: pos for pos, doc in enumerate(self._docs)}\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.index_name","title":"<code>index_name</code>  <code>property</code>","text":"<p>Return the name of the index in the database.</p>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.out_schema","title":"<code>out_schema: Type[BaseDoc]</code>  <code>property</code>","text":"<p>Return the original schema (without the parent_id from new_schema type)</p>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of InMemoryExactNNIndex.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of InMemoryExactNNIndex.\"\"\"\nindex_file_path: Optional[str] = None\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nAbstractTensor: {'space': 'cosine_sim'},\n},\n)\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfind_batched = _collect_query_args('find_batched')\nfilter = _collect_query_args('filter')\nfilter_batched = _raise_not_supported('find_batched')\ntext_search = _raise_not_supported('text_search')\ntext_search_batched = _raise_not_supported('text_search')\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the query object.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of InMemoryExactNNIndex.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of InMemoryExactNNIndex.\"\"\"\npass\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.__init__","title":"<code>__init__(docs=None, db_config=None, **kwargs)</code>","text":"<p>Initialize InMemoryExactNNIndex</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def __init__(\nself,\ndocs: Optional[DocList] = None,\ndb_config=None,\n**kwargs,\n):\n\"\"\"Initialize InMemoryExactNNIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._runtime_config = self.RuntimeConfig()\nself._db_config = cast(InMemoryExactNNIndex.DBConfig, self._db_config)\nself._index_file_path = self._db_config.index_file_path\nif docs and self._index_file_path:\nraise ValueError(\n'Initialize `InMemoryExactNNIndex` with either `docs` or '\n'`index_file_path`, not both. Provide `docs` for a fresh index, or '\n'`index_file_path` to use an existing file.'\n)\nif self._index_file_path:\nif os.path.exists(self._index_file_path):\nself._logger.info(\nf'Loading index from a binary file: {self._index_file_path}'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n).load_binary(file=self._index_file_path)\ndata_by_columns = self._get_col_value_dict(self._docs)\nself._update_subindex_data(self._docs)\nself._index_subindex(data_by_columns)\nelse:\nself._logger.warning(\nf'Index file does not exist: {self._index_file_path}. '\nf'Initializing empty InMemoryExactNNIndex.'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n)()\nelse:\nif docs:\nself._logger.info('Docs provided. Initializing with provided docs.')\nself._docs = docs\nelse:\nself._logger.info(\n'No docs or index file provided. Initializing empty InMemoryExactNNIndex.'\n)\nself._docs = DocList.__class_getitem__(\ncast(Type[BaseDoc], self._schema)\n)()\nself._embedding_map: Dict[str, Tuple[AnyTensor, Optional[List[int]]]] = {}\nself._ids_to_positions: Dict[str, int] = {}\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the InMemoryExactNNIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>List[Tuple[str, Dict]]</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def execute_query(self, query: List[Tuple[str, Dict]], *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the InMemoryExactNNIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\nif args or kwargs:\nraise ValueError(\nf'args and kwargs not supported for `execute_query` on {type(self)}'\n)\nreturn self._find_and_filter(query)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the filter query to execute following the query language of</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the filter query to execute following the query\n        language of\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = filter_docs(docs=self._docs, query=filter_query)[:limit]\nreturn cast(DocList, docs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find Documents in the index using nearest-neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of Documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find Documents in the index using nearest-neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of Documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif self._is_index_empty:\nreturn FindResult(documents=[], scores=[])  # type: ignore\nconfig = self._column_infos[search_field].config\ndocs, scores = find(\nindex=self._docs,\nquery=query,\nsearch_field=search_field,\nlimit=limit,\nmetric=config['space'],\ncache=self._embedding_map,\n)\ndocs_ = []\nfor doc in docs:\nori_doc = self._ori_items(doc)\nschema_cls = cast(Type[BaseDoc], self.out_schema)\ndocs_.append(schema_cls(**ori_doc.__dict__))\ndocs_with_schema = DocList.__class_getitem__(\ncast(Type[BaseDoc], self.out_schema)\n)(docs_)\nreturn FindResult(documents=docs_with_schema, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find Documents in the index using nearest-neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find Documents in the index using nearest-neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nself._validate_search_field(search_field)\nif self._is_index_empty:\nreturn FindResultBatched(documents=[], scores=[])  # type: ignore\nconfig = self._column_infos[search_field].config\nfind_res = find_batched(\nindex=self._docs,\nquery=cast(NdArray, queries),\nsearch_field=search_field,\nlimit=limit,\nmetric=config['space'],\ncache=self._embedding_map,\n)\nreturn find_res\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\n# implementing the public option because conversion to column dict is not needed\ndocs = self._validate_docs(docs)\nids_to_positions = self._get_ids_to_positions()\nfor doc in docs:\nif doc.id in ids_to_positions:\nself._docs[ids_to_positions[doc.id]] = doc\nelse:\nself._docs.append(doc)\nself._ids_to_positions[str(doc.id)] = len(self._ids_to_positions)\n# Add parent_id to all sub-index documents and store sub-index documents\ndata_by_columns = self._get_col_value_dict(docs)\nself._update_subindex_data(docs)\nself._index_subindex(data_by_columns)\nself._rebuild_embedding()\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nreturn len(self._docs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.persist","title":"<code>persist(file=None)</code>","text":"<p>Persist InMemoryExactNNIndex into a binary file.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def persist(self, file: Optional[str] = None) -&gt; None:\n\"\"\"Persist InMemoryExactNNIndex into a binary file.\"\"\"\nDEFAULT_INDEX_FILE_PATH = 'in_memory_index.bin'\nfile_to_save = self._index_file_path or file\nif file_to_save is None:\nself._logger.warning(\nf'persisting index to {DEFAULT_INDEX_FILE_PATH} because no `index_file_path` has been used inside DBConfig and no `file` has been passed as argument'\n)\nfile_to_save = file_to_save or DEFAULT_INDEX_FILE_PATH\nself._docs.save_binary(file=file_to_save)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/in_memory.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\nreturn python_type\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/in_memory/#docarray.index.backends.in_memory.InMemoryExactNNIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/","title":"MilvusDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/milvus/#milvusdocumentindex","title":"MilvusDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex","title":"<code>docarray.index.backends.milvus.MilvusDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>class MilvusDocumentIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize MilvusDocumentIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: MilvusDocumentIndex.DBConfig = cast(\nMilvusDocumentIndex.DBConfig, self._db_config\n)\nself._runtime_config: MilvusDocumentIndex.RuntimeConfig = cast(\nMilvusDocumentIndex.RuntimeConfig, self._runtime_config\n)\nself._client = connections.connect(\ndb_name=\"default\",\nhost=self._db_config.host,\nport=self._db_config.port,\nuser=self._db_config.user,\npassword=self._db_config.password,\ntoken=self._db_config.token,\n)\nself._validate_columns()\nself._field_name = self._get_vector_field_name()\nself._collection = self._create_or_load_collection()\nself._build_index()\nself._collection.load()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of MilvusDocumentIndex.\n        :param index_name: The name of the index in the Milvus database. If not provided, default index name will be used.\n        :param collection_description: Description of the collection in the database.\n        :param host: Hostname of the server where the database resides. Default is 'localhost'.\n        :param port: Port number used to connect to the database. Default is 19530.\n        :param user: User for the database. Can be an empty string if no user is required.\n        :param password: Password for the specified user. Can be an empty string if no password is required.\n        :param token: Token for secure connection. Can be an empty string if no token is required.\n        :param consistency_level: The level of consistency for the database session. Default is 'Session'.\n        :param search_params: Dictionary containing parameters for search operations,\n            default has a single key 'params' with 'nprobe' set to 10.\n        :param serialize_config: Dictionary containing configuration for serialization,\n            default is {'protocol': 'protobuf'}.\n        :param default_column_config: Dictionary that defines the default configuration\n            for each data type column.\n        \"\"\"\nindex_name: Optional[str] = None\ncollection_description: str = \"\"\nhost: str = \"localhost\"\nport: int = 19530\nuser: Optional[str] = \"\"\npassword: Optional[str] = \"\"\ntoken: Optional[str] = \"\"\nconsistency_level: str = 'Session'\nsearch_params: Dict = field(\ndefault_factory=lambda: {\n\"params\": {\"nprobe\": 10},\n}\n)\nserialize_config: Dict = field(default_factory=lambda: {\"protocol\": \"protobuf\"})\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nDataType.FLOAT_VECTOR: {\n'index_type': 'IVF_FLAT',\n'metric_type': 'L2',\n'params': {\"nlist\": 1024},\n},\n},\n)\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.\n        :param batch_size: Batch size for index/get/del.\n        \"\"\"\nbatch_size: int = 100\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_supported('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_supported('text_search_batched')\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type, or None if ``python_type``\n        is not supported.\n        \"\"\"\ntype_map = {\nint: DataType.INT64,\nfloat: DataType.FLOAT,\nstr: DataType.VARCHAR,\nbytes: DataType.VARCHAR,\nnp.ndarray: DataType.FLOAT_VECTOR,\nlist: DataType.FLOAT_VECTOR,\nAnyTensor: DataType.FLOAT_VECTOR,\nAbstractTensor: DataType.FLOAT_VECTOR,\n}\nif issubclass(python_type, ID):\nreturn DataType.VARCHAR\nfor py_type, db_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn db_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\ndef _create_or_load_collection(self) -&gt; Collection:\n\"\"\"\n        This function initializes or retrieves a Milvus collection with a specified schema,\n        storing documents as serialized data and using the document's ID as the collection's ID\n        , while inheriting other schema properties from the indexer's schema.\n        !!! note\n            Milvus framework currently only supports a single vector column, and only one vector\n            column can store in the schema (others are stored in the serialized data)\n        \"\"\"\nif not utility.has_collection(self.index_name):\nfields = [\nFieldSchema(\nname=\"serialized\",\ndtype=DataType.VARCHAR,\nmax_length=MAX_LEN,\n),\nFieldSchema(\nname=\"id\",\ndtype=DataType.VARCHAR,\nis_primary=True,\nmax_length=MAX_LEN,\n),\n]\nfor column_name, info in self._column_infos.items():\nif (\ncolumn_name != 'id'\nand not (\ninfo.db_type == DataType.FLOAT_VECTOR\nand column_name\n!= self._field_name  # Only store one vector field as a column\n)\nand not safe_issubclass(info.docarray_type, AnyDocArray)\n):\nfield_dict: Dict[str, Any] = {}\nif info.db_type == DataType.VARCHAR:\nfield_dict = {'max_length': MAX_LEN}\nelif info.db_type == DataType.FLOAT_VECTOR:\nfield_dict = {'dim': info.n_dim or info.config.get('dim')}\nfields.append(\nFieldSchema(\nname=column_name,\ndtype=info.db_type,\nis_primary=False,\n**field_dict,\n)\n)\nself._logger.info(\"Collection has been created\")\nreturn Collection(\nname=self.index_name,\nschema=CollectionSchema(\nfields=fields,\ndescription=self._db_config.collection_description,\n),\nusing='default',\n)\nreturn Collection(self.index_name)\ndef _validate_columns(self):\n\"\"\"\n        Validates whether the data schema includes at least one vector column used\n        for embedding (as required by Milvus), and ensures that dimension information\n        is specified for that column.\n        \"\"\"\nvector_columns = sum(\nsafe_issubclass(info.docarray_type, AbstractTensor)\nand info.config.get('is_embedding', False)\nfor info in self._column_infos.values()\n)\nif vector_columns == 0:\nraise ValueError(\n\"Unable to find any vector columns. Please make sure that at least one \"\n\"column is of a vector type with the is_embedding=True attribute specified.\"\n)\nelif vector_columns &gt; 1:\nraise ValueError(\"Specifying multiple vector fields is not supported.\")\nfor column, info in self._column_infos.items():\nif info.config.get('is_embedding') and (\nnot info.n_dim and not info.config.get('dim')\n):\nraise ValueError(\nf\"The dimension information is missing for the column '{column}', which is of vector type.\"\n)\n@property\ndef index_name(self):\ndefault_index_name = (\nself._schema.__name__.lower() if self._schema is not None else None\n)\nif default_index_name is None:\nerr_msg = (\n'A MilvusDocumentIndex must be typed with a Document type. '\n'To do so, use the syntax: MilvusDocumentIndex[DocumentType]'\n)\nself._logger.error(err_msg)\nraise ValueError(err_msg)\nindex_name = self._db_config.index_name or default_index_name\nself._logger.debug(f'Retrieved index name: {index_name}')\nreturn index_name\n@property\ndef out_schema(self) -&gt; Type[BaseDoc]:\n\"\"\"Return the real schema of the index.\"\"\"\nif self._is_subindex:\nreturn self._ori_schema\nreturn cast(Type[BaseDoc], self._schema)\ndef _build_index(self):\n\"\"\"\n        Sets up an index configuration for a specific column index, which is\n        required by the Milvus backend.\n        \"\"\"\nexisting_indices = [index.field_name for index in self._collection.indexes]\nif self._field_name in existing_indices:\nreturn\nindex_type = self._column_infos[self._field_name].config['index_type'].upper()\nif index_type not in VALID_INDEX_TYPES:\nraise ValueError(\nf\"Invalid index type '{index_type}' provided. \"\nf\"Must be one of: {', '.join(VALID_INDEX_TYPES)}\"\n)\nmetric_type = (\nself._column_infos[self._field_name].config.get('space', '').upper()\n)\nif metric_type not in VALID_METRICS:\nself._logger.warning(\nf\"Invalid or no distance metric '{metric_type}' was provided. \"\nf\"Should be one of: {', '.join(VALID_INDEX_TYPES)}. \"\nf\"Default distance metric will be used.\"\n)\nmetric_type = self._column_infos[self._field_name].config['metric_type']\nindex = {\n\"index_type\": index_type,\n\"metric_type\": metric_type,\n\"params\": self._column_infos[self._field_name].config['params'],\n}\nself._collection.create_index(self._field_name, index)\nself._logger.info(\nf\"Index for the field '{self._field_name}' has been successfully created\"\n)\ndef _get_vector_field_name(self):\nfor column, info in self._column_infos.items():\nif info.db_type == DataType.FLOAT_VECTOR and info.config.get(\n'is_embedding'\n):\nreturn column\nreturn ''\n@staticmethod\ndef _get_batches(docs, batch_size):\n\"\"\"Yield successive batch_size batches from docs.\"\"\"\nfor i in range(0, len(docs), batch_size):\nyield docs[i : i + batch_size]\ndef index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"Index Documents into the index.\n        !!! note\n            Passing a sequence of Documents that is not a DocList\n            (such as a List of Docs) comes at a performance penalty.\n            This is because the Index needs to check compatibility between itself and\n            the data. With a DocList as input this is a single check; for other inputs\n            compatibility needs to be checked for every Document individually.\n        :param docs: Documents to index.\n        \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs = self._validate_docs(docs)\nself._update_subindex_data(docs)\ndata_by_columns = self._get_col_value_dict(docs)\nself._index_subindex(data_by_columns)\npositions: Dict[str, int] = {\ninfo.name: num for num, info in enumerate(self._collection.schema.fields)\n}\nfor batch in self._get_batches(\ndocs, batch_size=self._runtime_config.batch_size\n):\nentities: List[List[Any]] = [\n[] for _ in range(len(self._collection.schema))\n]\nfor doc in batch:\n# \"serialized\" will always be in the first position\nentities[0].append(doc.to_base64(**self._db_config.serialize_config))\nfor schema_field in self._collection.schema.fields:\nif schema_field.name == 'serialized':\ncontinue\ncolumn_value = self._get_values_by_column([doc], schema_field.name)[\n0\n]\nif schema_field.dtype == DataType.FLOAT_VECTOR:\ncolumn_value = self._map_embedding(column_value)\nentities[positions[schema_field.name]].append(column_value)\nself._collection.insert(entities)\nself._collection.flush()\nself._logger.info(f\"{len(docs)} documents has been indexed\")\ndef _filter_by_parent_id(self, id: str) -&gt; Optional[List[str]]:\n\"\"\"Filter the ids of the subindex documents given id of root document.\n        :param id: the root document id to filter by\n        :return: a list of ids of the subindex documents\n        \"\"\"\ndocs = self._filter(filter_query=f\"parent_id == '{id}'\", limit=self.num_docs())\nreturn [doc.id for doc in docs]  # type: ignore[union-attr]\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        !!! note\n             Cannot use Milvus' num_entities method because it's not precise\n             especially after delete ops (#15201 issue in Milvus)\n        \"\"\"\nself._collection.load()\nresult = self._collection.query(\nexpr=self._always_true_expr(\"id\"),\noffset=0,\noutput_fields=[\"serialized\"],\n)\nreturn len(result)\ndef _get_items(\nself, doc_ids: Sequence[str]\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\n\"\"\"Get Documents from the index, by `id`.\n        If no document is found, a KeyError is raised.\n        :param doc_ids: ids to get from the Document index\n        :param raw: if raw, output the new_schema type (with parent id)\n        :return: Sequence of Documents, sorted corresponding to the order of `doc_ids`.\n                Duplicate `doc_ids` can be omitted in the output.\n        \"\"\"\nself._collection.load()\nresults: List[Dict] = []\nfor batch in self._get_batches(\ndoc_ids, batch_size=self._runtime_config.batch_size\n):\nresults.extend(\nself._collection.query(\nexpr=\"id in \" + str([id for id in batch]),\noffset=0,\noutput_fields=[\"serialized\"],\nconsistency_level=self._db_config.consistency_level,\n)\n)\nself._collection.release()\nreturn self._docs_from_query_response(results)\ndef _del_items(self, doc_ids: Sequence[str]):\n\"\"\"Delete Documents from the index.\n        :param doc_ids: ids to delete from the Document Store\n        \"\"\"\nself._collection.load()\nfor batch in self._get_batches(\ndoc_ids, batch_size=self._runtime_config.batch_size\n):\nself._collection.delete(\nexpr=\"id in \" + str([id for id in batch]),\nconsistency_level=self._db_config.consistency_level,\n)\nself._logger.info(f\"{len(doc_ids)} documents has been deleted\")\ndef _filter(\nself,\nfilter_query: Any,\nlimit: int,\n) -&gt; Union[DocList, List[Dict]]:\n\"\"\"\n        Filters the index based on the given filter query.\n        :param filter_query: The filter condition.\n        :param limit: The maximum number of results to return.\n        :return: Filter results.\n        \"\"\"\nself._collection.load()\nresult = self._collection.query(\nexpr=filter_query,\noffset=0,\nlimit=min(limit, self.num_docs()),\noutput_fields=[\"serialized\"],\n)\nself._collection.release()\nreturn self._docs_from_query_response(result)\ndef _filter_batched(\nself,\nfilter_queries: Any,\nlimit: int,\n) -&gt; Union[List[DocList], List[List[Dict]]]:\n\"\"\"\n        Filters the index based on the given batch of filter queries.\n        :param filter_queries: The filter conditions.\n        :param limit: The maximum number of results to return for each filter query.\n        :return: Filter results.\n        \"\"\"\nreturn [\nself._filter(filter_query=query, limit=limit) for query in filter_queries\n]\ndef _text_search(\nself,\nquery: str,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _text_search_batched(\nself,\nqueries: Sequence[str],\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\nraise NotImplementedError(f'{type(self)} does not support text search.')\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\n\"\"\"index a document into the store\"\"\"\nraise NotImplementedError()\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n        :param query: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.)\n            with a single axis, or a Document\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for MilvusDocumentIndex.'\n'Set search_field to an empty string to proceed.'\n)\nsearch_field = self._field_name\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef _find(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResult:\n\"\"\"\n        Conducts a search on the index.\n        :param query: The vector query to search.\n        :param limit: The maximum number of results to return.\n        :param search_field: The field to search the query.\n        :return: Search results.\n        \"\"\"\nreturn self._hybrid_search(query=query, limit=limit, search_field=search_field)\ndef _hybrid_search(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\nexpr: Optional[str] = None,\n):\n\"\"\"\n        Conducts a hybrid search on the index.\n        :param query: The vector query to search.\n        :param limit: The maximum number of results to return.\n        :param search_field: The field to search the query.\n        :param expr: Boolean expression used for filtering.\n        :return: Search results.\n        \"\"\"\nself._collection.load()\nresults = self._collection.search(\ndata=[query],\nanns_field=search_field,\nparam=self._db_config.search_params,\nlimit=limit,\noffset=0,\nexpr=expr,\noutput_fields=[\"serialized\"],\nconsistency_level=self._db_config.consistency_level,\n)\nself._collection.release()\nresults = next(iter(results), None)  # Only consider the first element\nreturn self._docs_from_find_response(results)\ndef find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n        :param queries: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n            or a DocList.\n            If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return per query\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for MilvusDocumentIndex.'\n'Set search_field to an empty string to proceed.'\n)\nsearch_field = self._field_name\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\n) -&gt; _FindResultBatched:\n\"\"\"\n        Conducts a batched search on the index.\n        :param queries: The queries to search.\n        :param limit: The maximum number of results to return for each query.\n        :param search_field: The field to search the queries.\n        :return: Search results.\n        \"\"\"\nself._collection.load()\nresults = self._collection.search(\ndata=queries,\nanns_field=self._field_name,\nparam=self._db_config.search_params,\nlimit=limit,\nexpr=None,\noutput_fields=[\"serialized\"],\nconsistency_level=self._db_config.consistency_level,\n)\nself._collection.release()\ndocuments, scores = zip(\n*[self._docs_from_find_response(result) for result in results]\n)\nreturn _FindResultBatched(\ndocuments=list(documents),\nscores=list(scores),\n)\ndef execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n        Executes a hybrid query on the index.\n        :param query: Query to execute on the index.\n        :return: Query results.\n        \"\"\"\ncomponents: Dict[str, List[Dict[str, Any]]] = {}\nfor component, value in query:\nif component not in components:\ncomponents[component] = []\ncomponents[component].append(value)\nif (\nlen(components) != 2\nor len(components.get('find', [])) != 1\nor len(components.get('filter', [])) != 1\n):\nraise ValueError(\n'The query must contain exactly one \"find\" and \"filter\" components.'\n)\nexpr = components['filter'][0]['filter_query']\nquery = components['find'][0]['query']\nlimit = (\ncomponents['find'][0].get('limit')\nor components['filter'][0].get('limit')\nor 10\n)\ndocs, scores = self._hybrid_search(\nquery=query,\nexpr=expr,\nsearch_field=self._field_name,\nlimit=limit,\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef _docs_from_query_response(self, result: Sequence[Dict]) -&gt; DocList[Any]:\nreturn DocList[self._schema](  # type: ignore\n[\nself._schema.from_base64(  # type: ignore\nresult[i][\"serialized\"], **self._db_config.serialize_config\n)\nfor i in range(len(result))\n]\n)\ndef _docs_from_find_response(self, result: Hits) -&gt; _FindResult:\nscores: NdArray = NdArray._docarray_from_native(\nnp.array([hit.score for hit in result])\n)\nreturn _FindResult(\ndocuments=DocList[self.out_schema](  # type: ignore\n[\nself.out_schema.from_base64(\nhit.entity.get('serialized'), **self._db_config.serialize_config\n)\nfor hit in result\n]\n),\nscores=scores,\n)\ndef _always_true_expr(self, primary_key: str) -&gt; str:\n\"\"\"\n        Returns a Milvus expression that is always true, thus allowing for the retrieval of all entries in a Collection.\n        Assumes that the primary key is of type DataType.VARCHAR\n        :param primary_key: the name of the primary key\n        :return: a Milvus expression that is always true for that primary key\n        \"\"\"\nreturn f'({primary_key} in [\"1\"]) or ({primary_key} not in [\"1\"])'\ndef _map_embedding(self, embedding: AnyTensor) -&gt; np.ndarray:\n\"\"\"\n        Milvus exclusively supports one-dimensional vectors. If multi-dimensional\n        vectors are provided, they will be automatically flattened to ensure compatibility.\n        :param embedding: The original raw embedding, which can be in the form of a TensorFlow or PyTorch tensor.\n        :return embedding: A one-dimensional numpy array representing the flattened version of the original embedding.\n        \"\"\"\nif embedding is None:\nraise ValueError(\n\"Embedding is None. Each document must have a valid embedding.\"\n)\nembedding = self._to_numpy(embedding)\nif embedding.ndim &gt; 1:\nembedding = np.asarray(embedding).squeeze()  # type: ignore\nreturn embedding\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nresult = self._collection.query(\nexpr=\"id in \" + str([doc_id]),\noffset=0,\noutput_fields=[\"serialized\"],\n)\nreturn len(result) &gt; 0\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.out_schema","title":"<code>out_schema: Type[BaseDoc]</code>  <code>property</code>","text":"<p>Return the real schema of the index.</p>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of MilvusDocumentIndex.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>Optional[str]</code> <p>The name of the index in the Milvus database. If not provided, default index name will be used.</p> <code>None</code> <code>collection_description</code> <code>str</code> <p>Description of the collection in the database.</p> <code>''</code> <code>host</code> <code>str</code> <p>Hostname of the server where the database resides. Default is 'localhost'.</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Port number used to connect to the database. Default is 19530.</p> <code>19530</code> <code>user</code> <code>Optional[str]</code> <p>User for the database. Can be an empty string if no user is required.</p> <code>''</code> <code>password</code> <code>Optional[str]</code> <p>Password for the specified user. Can be an empty string if no password is required.</p> <code>''</code> <code>token</code> <code>Optional[str]</code> <p>Token for secure connection. Can be an empty string if no token is required.</p> <code>''</code> <code>consistency_level</code> <code>str</code> <p>The level of consistency for the database session. Default is 'Session'.</p> <code>'Session'</code> <code>search_params</code> <code>Dict</code> <p>Dictionary containing parameters for search operations, default has a single key 'params' with 'nprobe' set to 10.</p> <code>field(default_factory=lambda : {'params': {'nprobe': 10}})</code> <code>serialize_config</code> <code>Dict</code> <p>Dictionary containing configuration for serialization, default is {'protocol': 'protobuf'}.</p> <code>field(default_factory=lambda : {'protocol': 'protobuf'})</code> <code>default_column_config</code> <code>Dict[Type, Dict[str, Any]]</code> <p>Dictionary that defines the default configuration for each data type column.</p> <code>field(default_factory=lambda : defaultdict(dict, {FLOAT_VECTOR: {'index_type': 'IVF_FLAT', 'metric_type': 'L2', 'params': {'nlist': 1024}}}))</code> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of MilvusDocumentIndex.\n    :param index_name: The name of the index in the Milvus database. If not provided, default index name will be used.\n    :param collection_description: Description of the collection in the database.\n    :param host: Hostname of the server where the database resides. Default is 'localhost'.\n    :param port: Port number used to connect to the database. Default is 19530.\n    :param user: User for the database. Can be an empty string if no user is required.\n    :param password: Password for the specified user. Can be an empty string if no password is required.\n    :param token: Token for secure connection. Can be an empty string if no token is required.\n    :param consistency_level: The level of consistency for the database session. Default is 'Session'.\n    :param search_params: Dictionary containing parameters for search operations,\n        default has a single key 'params' with 'nprobe' set to 10.\n    :param serialize_config: Dictionary containing configuration for serialization,\n        default is {'protocol': 'protobuf'}.\n    :param default_column_config: Dictionary that defines the default configuration\n        for each data type column.\n    \"\"\"\nindex_name: Optional[str] = None\ncollection_description: str = \"\"\nhost: str = \"localhost\"\nport: int = 19530\nuser: Optional[str] = \"\"\npassword: Optional[str] = \"\"\ntoken: Optional[str] = \"\"\nconsistency_level: str = 'Session'\nsearch_params: Dict = field(\ndefault_factory=lambda: {\n\"params\": {\"nprobe\": 10},\n}\n)\nserialize_config: Dict = field(default_factory=lambda: {\"protocol\": \"protobuf\"})\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nDataType.FLOAT_VECTOR: {\n'index_type': 'IVF_FLAT',\n'metric_type': 'L2',\n'params': {\"nlist\": 1024},\n},\n},\n)\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_supported('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_supported('text_search_batched')\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the query object.</p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for index/get/del.</p> <code>100</code> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.\n    :param batch_size: Batch size for index/get/del.\n    \"\"\"\nbatch_size: int = 100\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize MilvusDocumentIndex</p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize MilvusDocumentIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: MilvusDocumentIndex.DBConfig = cast(\nMilvusDocumentIndex.DBConfig, self._db_config\n)\nself._runtime_config: MilvusDocumentIndex.RuntimeConfig = cast(\nMilvusDocumentIndex.RuntimeConfig, self._runtime_config\n)\nself._client = connections.connect(\ndb_name=\"default\",\nhost=self._db_config.host,\nport=self._db_config.port,\nuser=self._db_config.user,\npassword=self._db_config.password,\ntoken=self._db_config.token,\n)\nself._validate_columns()\nself._field_name = self._get_vector_field_name()\nself._collection = self._create_or_load_collection()\nself._build_index()\nself._collection.load()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Executes a hybrid query on the index.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Any</code> <p>Query to execute on the index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Query results.</p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n    Executes a hybrid query on the index.\n    :param query: Query to execute on the index.\n    :return: Query results.\n    \"\"\"\ncomponents: Dict[str, List[Dict[str, Any]]] = {}\nfor component, value in query:\nif component not in components:\ncomponents[component] = []\ncomponents[component].append(value)\nif (\nlen(components) != 2\nor len(components.get('find', [])) != 1\nor len(components.get('filter', [])) != 1\n):\nraise ValueError(\n'The query must contain exactly one \"find\" and \"filter\" components.'\n)\nexpr = components['filter'][0]['filter_query']\nquery = components['find'][0]['query']\nlimit = (\ncomponents['find'][0].get('limit')\nor components['filter'][0].get('limit')\nor 10\n)\ndocs, scores = self._hybrid_search(\nquery=query,\nexpr=expr,\nsearch_field=self._field_name,\nlimit=limit,\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for MilvusDocumentIndex.'\n'Set search_field to an empty string to proceed.'\n)\nsearch_field = self._field_name\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for MilvusDocumentIndex.'\n'Set search_field to an empty string to proceed.'\n)\nsearch_field = self._field_name\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>Index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"Index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs = self._validate_docs(docs)\nself._update_subindex_data(docs)\ndata_by_columns = self._get_col_value_dict(docs)\nself._index_subindex(data_by_columns)\npositions: Dict[str, int] = {\ninfo.name: num for num, info in enumerate(self._collection.schema.fields)\n}\nfor batch in self._get_batches(\ndocs, batch_size=self._runtime_config.batch_size\n):\nentities: List[List[Any]] = [\n[] for _ in range(len(self._collection.schema))\n]\nfor doc in batch:\n# \"serialized\" will always be in the first position\nentities[0].append(doc.to_base64(**self._db_config.serialize_config))\nfor schema_field in self._collection.schema.fields:\nif schema_field.name == 'serialized':\ncontinue\ncolumn_value = self._get_values_by_column([doc], schema_field.name)[\n0\n]\nif schema_field.dtype == DataType.FLOAT_VECTOR:\ncolumn_value = self._map_embedding(column_value)\nentities[positions[schema_field.name]].append(column_value)\nself._collection.insert(entities)\nself._collection.flush()\nself._logger.info(f\"{len(docs)} documents has been indexed\")\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> <p>Note</p> <p>Cannot use Milvus' num_entities method because it's not precise  especially after delete ops (#15201 issue in Milvus)</p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    !!! note\n         Cannot use Milvus' num_entities method because it's not precise\n         especially after delete ops (#15201 issue in Milvus)\n    \"\"\"\nself._collection.load()\nresult = self._collection.query(\nexpr=self._always_true_expr(\"id\"),\noffset=0,\noutput_fields=[\"serialized\"],\n)\nreturn len(result)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/milvus.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type, or None if ``python_type``\n    is not supported.\n    \"\"\"\ntype_map = {\nint: DataType.INT64,\nfloat: DataType.FLOAT,\nstr: DataType.VARCHAR,\nbytes: DataType.VARCHAR,\nnp.ndarray: DataType.FLOAT_VECTOR,\nlist: DataType.FLOAT_VECTOR,\nAnyTensor: DataType.FLOAT_VECTOR,\nAbstractTensor: DataType.FLOAT_VECTOR,\n}\nif issubclass(python_type, ID):\nreturn DataType.VARCHAR\nfor py_type, db_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn db_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/milvus/#docarray.index.backends.milvus.MilvusDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/","title":"QdrantDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/qdrant/#qdrantdocumentindex","title":"QdrantDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex","title":"<code>docarray.index.backends.qdrant.QdrantDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>class QdrantDocumentIndex(BaseDocIndex, Generic[TSchema]):\nUUID_NAMESPACE = uuid.UUID('3896d314-1e95-4a3a-b45a-945f9f0b541d')\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize QdrantDocumentIndex\"\"\"\nif db_config is not None and getattr(\ndb_config, 'index_name'\n):  # this is needed for subindices\ndb_config.collection_name = db_config.index_name\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: QdrantDocumentIndex.DBConfig = cast(\nQdrantDocumentIndex.DBConfig, self._db_config\n)\nself._client = qdrant_client.QdrantClient(\nlocation=self._db_config.location,\nurl=self._db_config.url,\nport=self._db_config.port,\ngrpc_port=self._db_config.grpc_port,\nprefer_grpc=self._db_config.prefer_grpc,\nhttps=self._db_config.https,\napi_key=self._db_config.api_key,\nprefix=self._db_config.prefix,\ntimeout=self._db_config.timeout,\nhost=self._db_config.host,\npath=self._db_config.path,\n)\nself._initialize_collection()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n@property\ndef collection_name(self):\ndefault_collection_name = (\nself._schema.__name__.lower() if self._schema is not None else None\n)\nif default_collection_name is None:\nraise ValueError(\n'A QdrantDocumentIndex must be typed with a Document type.'\n'To do so, use the syntax: QdrantDocumentIndex[DocumentType]'\n)\nreturn self._db_config.collection_name or default_collection_name\n@property\ndef index_name(self):\nreturn self.collection_name\n@dataclass\nclass Query:\n\"\"\"Dataclass describing a query.\"\"\"\nvector_field: Optional[str]\nvector_query: Optional[NdArray]\nfilter: Optional[rest.Filter]\nlimit: int\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(\nself,\nvector_search_field: Optional[str] = None,\nvector_filters: Optional[List[NdArray]] = None,\npayload_filters: Optional[List[rest.Filter]] = None,\ntext_search_filters: Optional[List[Tuple[str, str]]] = None,\n):\nself._vector_search_field: Optional[str] = vector_search_field\nself._vector_filters: List[NdArray] = vector_filters or []\nself._payload_filters: List[rest.Filter] = payload_filters or []\nself._text_search_filters: List[Tuple[str, str]] = text_search_filters or []\ndef build(self, limit: int) -&gt; 'QdrantDocumentIndex.Query':\n\"\"\"\n            Build a query object for QdrantDocumentIndex.\n            :return: QdrantDocumentIndex.Query object\n            \"\"\"\nvector_query = None\nif len(self._vector_filters) &gt; 0:\n# If there are multiple vector queries applied, we can average them and\n# perform semantic search on a single vector instead\nvector_query = np.average(self._vector_filters, axis=0)\nmerged_filter = None\nif len(self._payload_filters) &gt; 0:\nmerged_filter = rest.Filter(must=self._payload_filters)\nif len(self._text_search_filters) &gt; 0:\n# Text search is just a special case of payload filtering, so the\n# payload filter is simply extended\nmerged_filter = merged_filter or rest.Filter(must=[])\nfor search_field, query in self._text_search_filters:\nmerged_filter.must.append(  # type: ignore[union-attr]\nrest.FieldCondition(\nkey=search_field,\nmatch=rest.MatchText(text=query),\n)\n)\nreturn QdrantDocumentIndex.Query(\nvector_field=self._vector_search_field,\nvector_query=vector_query,\nfilter=merged_filter,\nlimit=limit,\n)\ndef find(  # type: ignore[override]\nself, query: NdArray, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"\n            Find k-nearest neighbors of the query.\n            :param query: query vector for search. Has single axis.\n            :param search_field: field to perform search on\n            :return: QueryBuilder object\n            \"\"\"\nif self._vector_search_field and self._vector_search_field != search_field:\nraise ValueError(\nf'Trying to call .find for search_field = {search_field}, but '\nf'previously {self._vector_search_field} was used. Only a single '\nf'field might be used in chained calls.'\n)\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=search_field,\nvector_filters=self._vector_filters + [query],\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters,\n)\ndef filter(  # type: ignore[override]\nself, filter_query: rest.Filter\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a filter query\n            :param filter_query: a filter\n            :return: QueryBuilder object\n            \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters + [filter_query],\ntext_search_filters=self._text_search_filters,\n)\ndef text_search(  # type: ignore[override]\nself, query: str, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a text search query\n            :param query: The text to search for\n            :param search_field: name of the field to search on\n            :return: QueryBuilder object\n            \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters + [(search_field, query)],\n)\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of QdrantDocumentIndex.\"\"\"\nlocation: Optional[str] = None\nurl: Optional[str] = None\nport: Optional[int] = 6333\ngrpc_port: int = 6334\nprefer_grpc: bool = True\nhttps: Optional[bool] = None\napi_key: Optional[str] = None\nprefix: Optional[str] = None\ntimeout: Optional[float] = None\nhost: Optional[str] = None\npath: Optional[str] = None\ncollection_name: Optional[str] = None\nshard_number: Optional[int] = None\nreplication_factor: Optional[int] = None\nwrite_consistency_factor: Optional[int] = None\non_disk_payload: Optional[bool] = None\nhnsw_config: Optional[types.HnswConfigDiff] = None\noptimizers_config: Optional[types.OptimizersConfigDiff] = None\nwal_config: Optional[types.WalConfigDiff] = None\nquantization_config: Optional[types.QuantizationConfig] = None\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: {\n'id': {},  # type: ignore[dict-item]\n'vector': {'dim': 128},  # type: ignore[dict-item]\n'payload': {},  # type: ignore[dict-item]\n}\n)\ndef __post_init__(self):\nif self.collection_name is None and self.index_name is not None:\nself.collection_name = self.index_name\nif self.index_name is None and self.collection_name is not None:\nself.index_name = self.collection_name\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of QdrantDocumentIndex.\"\"\"\npass\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type.\n        \"\"\"\nif any(safe_issubclass(python_type, vt) for vt in QDRANT_PY_VECTOR_TYPES):\nreturn 'vector'\nif safe_issubclass(python_type, docarray.typing.id.ID):\nreturn 'id'\nreturn 'payload'\ndef _initialize_collection(self):\ntry:\nself._client.get_collection(self.collection_name)\nexcept (UnexpectedResponse, RpcError, ValueError):\nvectors_config = {}\nfor column_name, column_info in self._column_infos.items():\nif column_info.db_type == 'vector':\nvectors_config[column_name] = self._to_qdrant_vector_params(\ncolumn_info\n)\nself._client.create_collection(\ncollection_name=self.collection_name,\nvectors_config=vectors_config,\nshard_number=self._db_config.shard_number,\nreplication_factor=self._db_config.replication_factor,\nwrite_consistency_factor=self._db_config.write_consistency_factor,\non_disk_payload=self._db_config.on_disk_payload,\nhnsw_config=self._db_config.hnsw_config,\noptimizers_config=self._db_config.optimizers_config,\nwal_config=self._db_config.wal_config,\nquantization_config=self._db_config.quantization_config,\n)\nself._client.create_payload_index(\ncollection_name=self.collection_name,\nfield_name='__generated_vectors',\nfield_schema=rest.PayloadSchemaType.KEYWORD,\n)\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\nself._index_subindex(column_to_data)\nrows = self._transpose_col_value_dict(column_to_data)\n# TODO: add batching the documents to avoid timeouts\npoints = [self._build_point_from_row(row) for row in rows]\nself._client.upsert(\ncollection_name=self.collection_name,\npoints=points,\n)\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        \"\"\"\nreturn self._client.count(collection_name=self.collection_name).count\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nresponse, _ = self._client.scroll(\ncollection_name=self.index_name,\nscroll_filter=rest.Filter(\nmust=[\nrest.HasIdCondition(has_id=[self._to_qdrant_id(doc_id)]),\n],\n),\n)\nreturn len(response) &gt; 0\ndef _del_items(self, doc_ids: Sequence[str]):\nitems = self._get_items(doc_ids)\nif len(items) &lt; len(doc_ids):\nfound_keys = set(item['id'] for item in items)  # type: ignore[index]\nmissing_keys = set(doc_ids) - found_keys\nraise KeyError('Document keys could not found: %s' % ','.join(missing_keys))\nself._client.delete(\ncollection_name=self.collection_name,\npoints_selector=rest.PointIdsList(\npoints=[self._to_qdrant_id(doc_id) for doc_id in doc_ids],\n),\n)\ndef _get_items(\nself, doc_ids: Sequence[str]\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\nresponse, _ = self._client.scroll(\ncollection_name=self.collection_name,\nscroll_filter=rest.Filter(\nmust=[\nrest.HasIdCondition(\nhas_id=[self._to_qdrant_id(doc_id) for doc_id in doc_ids],\n),\n],\n),\nlimit=len(doc_ids),\nwith_payload=True,\nwith_vectors=True,\n)\nreturn sorted(\n[self._convert_to_doc(point) for point in response],\nkey=lambda x: doc_ids.index(x['id']),\n)\ndef execute_query(self, query: Union[Query, RawQuery], *args, **kwargs) -&gt; DocList:\n\"\"\"\n        Execute a query on the QdrantDocumentIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index's `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\nif not isinstance(query, QdrantDocumentIndex.Query):\npoints = self._execute_raw_query(query.copy())\nelif query.vector_field:\n# We perform semantic search with some vectors with Qdrant's search method\n# should be called\npoints = self._client.search(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nquery_vector=(query.vector_field, query.vector_query),  # type: ignore[arg-type]\nquery_filter=rest.Filter(\nmust=[query.filter],\n# The following filter takes care of using only those points which\n# do not have the vector generated. Those are excluded from the\n# search results.\nmust_not=[\nrest.FieldCondition(\nkey='__generated_vectors',\nmatch=rest.MatchValue(value=query.vector_field),\n)\n],\n),\nlimit=query.limit,\nwith_payload=True,\nwith_vectors=True,\n)\nelse:\n# Just filtering, so Qdrant's scroll has to be used instead\npoints, _ = self._client.scroll(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nscroll_filter=query.filter,\nlimit=query.limit,\nwith_payload=True,\nwith_vectors=True,\n)\ndocs = [self._convert_to_doc(point) for point in points]\nreturn self._dict_list_to_docarray(docs)\ndef _execute_raw_query(\nself, query: RawQuery\n) -&gt; Sequence[Union[rest.ScoredPoint, rest.Record]]:\npayload_filter = query.pop('filter', None)\nif payload_filter:\npayload_filter = rest.Filter.parse_obj(payload_filter)  # type: ignore[assignment]\nif 'vector' in query:\n# We perform semantic search with some vectors with Qdrant's search method\n# should be called\nsearch_params = query.pop('params', None)\nif search_params:\nsearch_params = rest.SearchParams.parse_obj(search_params)  # type: ignore[assignment]\npoints = self._client.search(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nquery_vector=query.pop('vector'),\nquery_filter=payload_filter,\nsearch_params=search_params,\n**query,\n)\nelse:\n# Just filtering, so Qdrant's scroll has to be used instead\npoints, _ = self._client.scroll(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nscroll_filter=payload_filter,\n**query,\n)\nreturn points\ndef _find(\nself, query: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nquery_batched = np.expand_dims(query, axis=0)\ndocs, scores = self._find_batched(\nqueries=query_batched, limit=limit, search_field=search_field\n)\nreturn _FindResult(documents=docs[0], scores=scores[0])  # type: ignore[arg-type]\ndef _find_batched(\nself, queries: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\nresponses = self._client.search_batch(\ncollection_name=self.collection_name,\nrequests=[\nrest.SearchRequest(\nvector=rest.NamedVector(\nname=search_field,\nvector=query.tolist(),  # type: ignore\n),\n# The following filter takes care of using only those points which\n# do not have the vector generated. Those are excluded from the\n# search results.\nfilter=rest.Filter(\nmust_not=[\nrest.FieldCondition(\nkey='__generated_vectors',\nmatch=rest.MatchValue(value=search_field),\n)\n]\n),\nlimit=limit,\nwith_vector=True,\nwith_payload=True,\n)\nfor query in queries\n],\n)\nreturn _FindResultBatched(\ndocuments=[\n[self._convert_to_doc(point) for point in response]\nfor response in responses\n],\nscores=[\nNdArray._docarray_from_native(\nnp.array([point.score for point in response])\n)\nfor response in responses\n],\n)\ndef _filter(\nself, filter_query: rest.Filter, limit: int\n) -&gt; Union[DocList, List[Dict]]:\nquery_batched = [filter_query]\ndocs = self._filter_batched(filter_queries=query_batched, limit=limit)\nreturn docs[0]\ndef _filter_batched(\nself, filter_queries: Sequence[rest.Filter], limit: int\n) -&gt; Union[List[DocList], List[List[Dict]]]:\nresponses = []\nfor filter_query in filter_queries:\n# There is no batch scroll available in Qdrant client yet, so we need to\n# perform the queries one by one. It will be changed in the future versions.\nresponse, _ = self._client.scroll(\ncollection_name=self.collection_name,\nscroll_filter=filter_query,\nlimit=limit,\nwith_payload=True,\nwith_vectors=True,\n)\nresponses.append(response)\nreturn [\n[self._convert_to_doc(point) for point in response]\nfor response in responses\n]\ndef _text_search(\nself, query: str, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nquery_batched = [query]\ndocs, scores = self._text_search_batched(\nqueries=query_batched, limit=limit, search_field=search_field\n)\nreturn _FindResult(documents=docs[0], scores=scores[0])  # type: ignore[arg-type]\ndef _text_search_batched(\nself, queries: Sequence[str], limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\nfilter_queries = [\nrest.Filter(\nmust=[\nrest.FieldCondition(\nkey=search_field,\nmatch=rest.MatchText(text=query),\n)\n]\n)\nfor query in queries\n]\ndocuments_batched = self._filter_batched(\nfilter_queries=filter_queries, limit=limit\n)\n# Qdrant does not return any scores if we just filter the objects, without using\n# semantic search over vectors. Thus, each document is scored with a value of 1\nreturn _FindResultBatched(\ndocuments=documents_batched,\nscores=[\nNdArray._docarray_from_native(np.ones(len(docs)))\nfor docs in documents_batched\n],\n)\ndef _filter_by_parent_id(self, id: str) -&gt; Optional[List[str]]:\nresponse, _ = self._client.scroll(\ncollection_name=self.collection_name,  # type: ignore\nscroll_filter=rest.Filter(\nmust=[\nrest.FieldCondition(\nkey='parent_id', match=rest.MatchValue(value=id)\n)\n]\n),\nwith_payload=rest.PayloadSelectorInclude(include=['id']),\n)\nids = [point.payload['id'] for point in response]  # type: ignore\nreturn ids\ndef _build_point_from_row(self, row: Dict[str, Any]) -&gt; rest.PointStruct:\npoint_id = self._to_qdrant_id(row.get('id'))\nvectors: Dict[str, List[float]] = {}\npayload: Dict[str, Any] = {'__generated_vectors': []}\nfor column_name, column_info in self._column_infos.items():\nif safe_issubclass(column_info.docarray_type, AnyDocArray):\ncontinue\nif column_info.db_type in ['id', 'payload']:\npayload[column_name] = row.get(column_name)\ncontinue\nvector = row.get(column_name)\nif column_info.db_type == 'vector' and vector is not None:\nvectors[column_name] = vector.tolist()\nelif column_info.db_type == 'vector' and vector is None:\n# In that case vector was not provided. Qdrant does not support optional\n# vectors - each point needs to have all the vectors already assigned.\n# Thus, we put a fake embedding with the correct dimensionality and mark\n# such point a point with a boolean flag in the payload.\nvector_size = column_info.n_dim or column_info.config.get('dim')\nvectors[column_name] = np.ones(vector_size).tolist()  # type: ignore[arg-type]\npayload['__generated_vectors'].append(column_name)\nelse:\nraise ValueError(\nf'Could not handle the conversion for column {column_name}. '\nf'Column info: {column_info}'\n)\nreturn rest.PointStruct(\nid=point_id,\nvector=vectors,\npayload=payload,\n)\ndef _to_qdrant_id(self, external_id: Optional[str]) -&gt; str:\nif external_id is None:\nreturn uuid.uuid4().hex\nreturn uuid.uuid5(QdrantDocumentIndex.UUID_NAMESPACE, external_id).hex\ndef _to_qdrant_vector_params(self, column_info: _ColumnInfo) -&gt; rest.VectorParams:\nreturn rest.VectorParams(\nsize=column_info.n_dim or column_info.config.get('dim'),\ndistance=QDRANT_SPACE_MAPPING[column_info.config.get('space', 'cosine')],\n)\ndef _convert_to_doc(\nself, point: Union[rest.ScoredPoint, rest.Record]\n) -&gt; Dict[str, Any]:\ndocument = cast(Dict[str, Any], point.payload)\ngenerated_vectors = (\ndocument.pop('__generated_vectors')\nif '__generated_vectors' in document\nelse []\n)\nvectors = point.vector if point.vector else dict()\nif not isinstance(vectors, dict):\nvectors = {'__default__': vectors}\nfor vector_name, vector in vectors.items():\nif vector_name in generated_vectors:\n# That means the vector was generated during the upload, and should not\n# be returned along the other vectors.\npass\ndocument[vector_name] = vector\nreturn document\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of QdrantDocumentIndex.</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of QdrantDocumentIndex.\"\"\"\nlocation: Optional[str] = None\nurl: Optional[str] = None\nport: Optional[int] = 6333\ngrpc_port: int = 6334\nprefer_grpc: bool = True\nhttps: Optional[bool] = None\napi_key: Optional[str] = None\nprefix: Optional[str] = None\ntimeout: Optional[float] = None\nhost: Optional[str] = None\npath: Optional[str] = None\ncollection_name: Optional[str] = None\nshard_number: Optional[int] = None\nreplication_factor: Optional[int] = None\nwrite_consistency_factor: Optional[int] = None\non_disk_payload: Optional[bool] = None\nhnsw_config: Optional[types.HnswConfigDiff] = None\noptimizers_config: Optional[types.OptimizersConfigDiff] = None\nwal_config: Optional[types.WalConfigDiff] = None\nquantization_config: Optional[types.QuantizationConfig] = None\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: {\n'id': {},  # type: ignore[dict-item]\n'vector': {'dim': 128},  # type: ignore[dict-item]\n'payload': {},  # type: ignore[dict-item]\n}\n)\ndef __post_init__(self):\nif self.collection_name is None and self.index_name is not None:\nself.collection_name = self.index_name\nif self.index_name is None and self.collection_name is not None:\nself.index_name = self.collection_name\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.Query","title":"<code>Query</code>  <code>dataclass</code>","text":"<p>Dataclass describing a query.</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>@dataclass\nclass Query:\n\"\"\"Dataclass describing a query.\"\"\"\nvector_field: Optional[str]\nvector_query: Optional[NdArray]\nfilter: Optional[rest.Filter]\nlimit: int\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(\nself,\nvector_search_field: Optional[str] = None,\nvector_filters: Optional[List[NdArray]] = None,\npayload_filters: Optional[List[rest.Filter]] = None,\ntext_search_filters: Optional[List[Tuple[str, str]]] = None,\n):\nself._vector_search_field: Optional[str] = vector_search_field\nself._vector_filters: List[NdArray] = vector_filters or []\nself._payload_filters: List[rest.Filter] = payload_filters or []\nself._text_search_filters: List[Tuple[str, str]] = text_search_filters or []\ndef build(self, limit: int) -&gt; 'QdrantDocumentIndex.Query':\n\"\"\"\n        Build a query object for QdrantDocumentIndex.\n        :return: QdrantDocumentIndex.Query object\n        \"\"\"\nvector_query = None\nif len(self._vector_filters) &gt; 0:\n# If there are multiple vector queries applied, we can average them and\n# perform semantic search on a single vector instead\nvector_query = np.average(self._vector_filters, axis=0)\nmerged_filter = None\nif len(self._payload_filters) &gt; 0:\nmerged_filter = rest.Filter(must=self._payload_filters)\nif len(self._text_search_filters) &gt; 0:\n# Text search is just a special case of payload filtering, so the\n# payload filter is simply extended\nmerged_filter = merged_filter or rest.Filter(must=[])\nfor search_field, query in self._text_search_filters:\nmerged_filter.must.append(  # type: ignore[union-attr]\nrest.FieldCondition(\nkey=search_field,\nmatch=rest.MatchText(text=query),\n)\n)\nreturn QdrantDocumentIndex.Query(\nvector_field=self._vector_search_field,\nvector_query=vector_query,\nfilter=merged_filter,\nlimit=limit,\n)\ndef find(  # type: ignore[override]\nself, query: NdArray, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"\n        Find k-nearest neighbors of the query.\n        :param query: query vector for search. Has single axis.\n        :param search_field: field to perform search on\n        :return: QueryBuilder object\n        \"\"\"\nif self._vector_search_field and self._vector_search_field != search_field:\nraise ValueError(\nf'Trying to call .find for search_field = {search_field}, but '\nf'previously {self._vector_search_field} was used. Only a single '\nf'field might be used in chained calls.'\n)\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=search_field,\nvector_filters=self._vector_filters + [query],\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters,\n)\ndef filter(  # type: ignore[override]\nself, filter_query: rest.Filter\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a filter query\n        :param filter_query: a filter\n        :return: QueryBuilder object\n        \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters + [filter_query],\ntext_search_filters=self._text_search_filters,\n)\ndef text_search(  # type: ignore[override]\nself, query: str, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a text search query\n        :param query: The text to search for\n        :param search_field: name of the field to search on\n        :return: QueryBuilder object\n        \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters + [(search_field, query)],\n)\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.QueryBuilder.build","title":"<code>build(limit)</code>","text":"<p>Build a query object for QdrantDocumentIndex.</p> <p>Returns:</p> Type Description <code>Query</code> <p>QdrantDocumentIndex.Query object</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def build(self, limit: int) -&gt; 'QdrantDocumentIndex.Query':\n\"\"\"\n    Build a query object for QdrantDocumentIndex.\n    :return: QdrantDocumentIndex.Query object\n    \"\"\"\nvector_query = None\nif len(self._vector_filters) &gt; 0:\n# If there are multiple vector queries applied, we can average them and\n# perform semantic search on a single vector instead\nvector_query = np.average(self._vector_filters, axis=0)\nmerged_filter = None\nif len(self._payload_filters) &gt; 0:\nmerged_filter = rest.Filter(must=self._payload_filters)\nif len(self._text_search_filters) &gt; 0:\n# Text search is just a special case of payload filtering, so the\n# payload filter is simply extended\nmerged_filter = merged_filter or rest.Filter(must=[])\nfor search_field, query in self._text_search_filters:\nmerged_filter.must.append(  # type: ignore[union-attr]\nrest.FieldCondition(\nkey=search_field,\nmatch=rest.MatchText(text=query),\n)\n)\nreturn QdrantDocumentIndex.Query(\nvector_field=self._vector_search_field,\nvector_query=vector_query,\nfilter=merged_filter,\nlimit=limit,\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.QueryBuilder.filter","title":"<code>filter(filter_query)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Filter</code> <p>a filter</p> required <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def filter(  # type: ignore[override]\nself, filter_query: rest.Filter\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: a filter\n    :return: QueryBuilder object\n    \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters + [filter_query],\ntext_search_filters=self._text_search_filters,\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.QueryBuilder.find","title":"<code>find(query, search_field='')</code>","text":"<p>Find k-nearest neighbors of the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>NdArray</code> <p>query vector for search. Has single axis.</p> required <code>search_field</code> <code>str</code> <p>field to perform search on</p> <code>''</code> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def find(  # type: ignore[override]\nself, query: NdArray, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"\n    Find k-nearest neighbors of the query.\n    :param query: query vector for search. Has single axis.\n    :param search_field: field to perform search on\n    :return: QueryBuilder object\n    \"\"\"\nif self._vector_search_field and self._vector_search_field != search_field:\nraise ValueError(\nf'Trying to call .find for search_field = {search_field}, but '\nf'previously {self._vector_search_field} was used. Only a single '\nf'field might be used in chained calls.'\n)\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=search_field,\nvector_filters=self._vector_filters + [query],\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters,\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.QueryBuilder.text_search","title":"<code>text_search(query, search_field='')</code>","text":"<p>Find documents in the index based on a text search query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def text_search(  # type: ignore[override]\nself, query: str, search_field: str = ''\n) -&gt; 'QdrantDocumentIndex.QueryBuilder':\n\"\"\"Find documents in the index based on a text search query\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :return: QueryBuilder object\n    \"\"\"\nreturn QdrantDocumentIndex.QueryBuilder(\nvector_search_field=self._vector_search_field,\nvector_filters=self._vector_filters,\npayload_filters=self._payload_filters,\ntext_search_filters=self._text_search_filters + [(search_field, query)],\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of QdrantDocumentIndex.</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of QdrantDocumentIndex.\"\"\"\npass\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize QdrantDocumentIndex</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize QdrantDocumentIndex\"\"\"\nif db_config is not None and getattr(\ndb_config, 'index_name'\n):  # this is needed for subindices\ndb_config.collection_name = db_config.index_name\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: QdrantDocumentIndex.DBConfig = cast(\nQdrantDocumentIndex.DBConfig, self._db_config\n)\nself._client = qdrant_client.QdrantClient(\nlocation=self._db_config.location,\nurl=self._db_config.url,\nport=self._db_config.port,\ngrpc_port=self._db_config.grpc_port,\nprefer_grpc=self._db_config.prefer_grpc,\nhttps=self._db_config.https,\napi_key=self._db_config.api_key,\nprefix=self._db_config.prefix,\ntimeout=self._db_config.timeout,\nhost=self._db_config.host,\npath=self._db_config.path,\n)\nself._initialize_collection()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the QdrantDocumentIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index's <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[Query, RawQuery]</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>DocList</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def execute_query(self, query: Union[Query, RawQuery], *args, **kwargs) -&gt; DocList:\n\"\"\"\n    Execute a query on the QdrantDocumentIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index's `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\nif not isinstance(query, QdrantDocumentIndex.Query):\npoints = self._execute_raw_query(query.copy())\nelif query.vector_field:\n# We perform semantic search with some vectors with Qdrant's search method\n# should be called\npoints = self._client.search(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nquery_vector=(query.vector_field, query.vector_query),  # type: ignore[arg-type]\nquery_filter=rest.Filter(\nmust=[query.filter],\n# The following filter takes care of using only those points which\n# do not have the vector generated. Those are excluded from the\n# search results.\nmust_not=[\nrest.FieldCondition(\nkey='__generated_vectors',\nmatch=rest.MatchValue(value=query.vector_field),\n)\n],\n),\nlimit=query.limit,\nwith_payload=True,\nwith_vectors=True,\n)\nelse:\n# Just filtering, so Qdrant's scroll has to be used instead\npoints, _ = self._client.scroll(  # type: ignore[assignment]\ncollection_name=self.collection_name,\nscroll_filter=query.filter,\nlimit=query.limit,\nwith_payload=True,\nwith_vectors=True,\n)\ndocs = [self._convert_to_doc(point) for point in points]\nreturn self._dict_list_to_docarray(docs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nreturn self._client.count(collection_name=self.collection_name).count\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type.</p> Source code in <code>docarray/index/backends/qdrant.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type.\n    \"\"\"\nif any(safe_issubclass(python_type, vt) for vt in QDRANT_PY_VECTOR_TYPES):\nreturn 'vector'\nif safe_issubclass(python_type, docarray.typing.id.ID):\nreturn 'id'\nreturn 'payload'\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/qdrant/#docarray.index.backends.qdrant.QdrantDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/","title":"RedisDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/redis/#redisdocumentindex","title":"RedisDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex","title":"<code>docarray.index.backends.redis.RedisDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>class RedisDocumentIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize RedisDocumentIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(RedisDocumentIndex.DBConfig, self._db_config)\nself._runtime_config: RedisDocumentIndex.RuntimeConfig = cast(\nRedisDocumentIndex.RuntimeConfig, self._runtime_config\n)\nself._prefix = self.index_name + ':'\nself._text_scorer = self._db_config.text_scorer\n# initialize Redis client\nself._client = redis.Redis(\nhost=self._db_config.host,\nport=self._db_config.port,\nusername=self._db_config.username,\npassword=self._db_config.password,\ndecode_responses=False,\n)\nself._create_index()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\ndef _create_index(self) -&gt; None:\n\"\"\"Create a new index in the Redis database if it doesn't already exist.\"\"\"\nif not self._check_index_exists(self.index_name):\nschema = []\nfor column, info in self._column_infos.items():\nif safe_issubclass(info.docarray_type, AnyDocArray):\ncontinue\nelif info.db_type == VectorField:\nspace = info.config.get('space') or info.config.get('distance')\nif not space or space.upper() not in VALID_DISTANCES:\nraise ValueError(\nf\"Invalid distance metric '{space}' provided. \"\nf\"Must be one of: {', '.join(VALID_DISTANCES)}\"\n)\nspace = space.upper()\nattributes = {\n'TYPE': 'FLOAT32',\n'DIM': info.n_dim or info.config.get('dim'),\n'DISTANCE_METRIC': space,\n'EF_CONSTRUCTION': info.config['ef_construction'],\n'EF_RUNTIME': info.config['ef_runtime'],\n'M': info.config['m'],\n'INITIAL_CAP': info.config['initial_cap'],\n}\nattributes = {\nname: value for name, value in attributes.items() if value\n}\nalgorithm = info.config['algorithm'].upper()\nif algorithm not in VALID_ALGORITHMS:\nraise ValueError(\nf\"Invalid algorithm '{algorithm}' provided. \"\nf\"Must be one of: {', '.join(VALID_ALGORITHMS)}\"\n)\nschema.append(\ninfo.db_type(\n'$.' + column,\nalgorithm=algorithm,\nattributes=attributes,\nas_name=column,\n)\n)\nelif column in ['id', 'parent_id']:\nschema.append(TagField('$.' + column, as_name=column))\nelse:\nschema.append(info.db_type('$.' + column, as_name=column))\n# Create Redis Index\nself._client.ft(self.index_name).create_index(\nschema,\ndefinition=IndexDefinition(\nprefix=[self._prefix], index_type=IndexType.JSON\n),\n)\nself._logger.info(f'index {self.index_name} has been created')\nelse:\nself._logger.info(f'connected to existing {self.index_name} index')\ndef _check_index_exists(self, index_name: str) -&gt; bool:\n\"\"\"\n        Check if an index exists in the Redis database.\n        :param index_name: The name of the index.\n        :return: True if the index exists, False otherwise.\n        \"\"\"\ntry:\nself._client.ft(index_name).info()\nexcept:  # noqa: E722\nself._logger.info(f'Index {index_name} does not exist')\nreturn False\nself._logger.info(f'Index {index_name} already exists')\nreturn True\n@property\ndef index_name(self):\ndefault_index_name = (\nself._schema.__name__.lower() if self._schema is not None else None\n)\nif default_index_name is None:\nerr_msg = (\n'A RedisDocumentIndex must be typed with a Document type. '\n'To do so, use the syntax: RedisDocumentIndex[DocumentType]'\n)\nself._logger.error(err_msg)\nraise ValueError(err_msg)\nindex_name = self._db_config.index_name or default_index_name\nself._logger.debug(f'Retrieved index name: {index_name}')\nreturn index_name\n@property\ndef out_schema(self) -&gt; Type[BaseDoc]:\n\"\"\"Return the real schema of the index.\"\"\"\nif self._is_subindex:\nreturn self._ori_schema\nreturn cast(Type[BaseDoc], self._schema)\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_composable('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of RedisDocumentIndex.\n        :param host: The host address for the Redis server. Default is 'localhost'.\n        :param port: The port number for the Redis server. Default is 6379.\n        :param index_name: The name of the index in the Redis database.\n            If not provided, default index name will be used.\n        :param username: The username for the Redis server. Default is None.\n        :param password: The password for the Redis server. Default is None.\n        :param text_scorer: The method for scoring text during text search.\n            Default is 'BM25'.\n        :param default_column_config: Default configuration for columns.\n        \"\"\"\nhost: str = 'localhost'\nport: int = 6379\nindex_name: Optional[str] = None\nusername: Optional[str] = None\npassword: Optional[str] = None\ntext_scorer: str = field(default='BM25')\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nVectorField: {\n'algorithm': 'FLAT',\n'distance': 'COSINE',\n'ef_construction': None,\n'm': None,\n'ef_runtime': None,\n'initial_cap': None,\n},\n},\n)\n)\ndef __post_init__(self):\nself.text_scorer = self.text_scorer.upper()\nif self.text_scorer not in VALID_TEXT_SCORERS:\nraise ValueError(\nf\"Invalid text scorer '{self.text_scorer}' provided. \"\nf\"Must be one of: {', '.join(VALID_TEXT_SCORERS)}\"\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.\n        :param batch_size: Batch size for index/get/del.\n        \"\"\"\nbatch_size: int = 100\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"\n        Map python types to corresponding Redis types.\n        :param python_type: Python type.\n        :return: Corresponding Redis type.\n        \"\"\"\ntype_map = {\nint: NumericField,\nfloat: NumericField,\nstr: TextField,\nbytes: TextField,\nnp.ndarray: VectorField,\nlist: VectorField,\nAbstractTensor: VectorField,\n}\nfor py_type, redis_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn redis_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\n@staticmethod\ndef _generate_items(\ncolumn_to_data: Dict[str, Generator[Any, None, None]],\nbatch_size: int,\n) -&gt; Iterator[List[Dict[str, Any]]]:\n\"\"\"\n        Given a dictionary of data generators, yield a list of dictionaries where each\n        item consists of a column name and a single item from the corresponding generator.\n        :param column_to_data: A dictionary where each key is a column name and each value\n            is a generator.\n        :param batch_size: Size of batch to generate each time.\n        :yield: A list of dictionaries where each item consists of a column name and\n            an item from the corresponding generator. Yields until all generators\n            are exhausted.\n        \"\"\"\ncolumn_names = list(column_to_data.keys())\ndata_generators = [iter(column_to_data[name]) for name in column_names]\nbatch: List[Dict[str, Any]] = []\nwhile True:\ndata_dict = {}\nfor name, generator in zip(column_names, data_generators):\nitem = next(generator, None)\nif name == 'id' and not item:\nif batch:\nyield batch\nreturn\nif isinstance(item, AbstractTensor):\ndata_dict[name] = item._docarray_to_ndarray().tolist()\nelif isinstance(item, ndarray):\ndata_dict[name] = item.astype(np.float32).tolist()\nelif item is not None:\ndata_dict[name] = item\nbatch.append(data_dict)\nif len(batch) == batch_size:\nyield batch\nbatch = []\ndef _index(\nself, column_to_data: Dict[str, Generator[Any, None, None]]\n) -&gt; List[str]:\n\"\"\"\n        Indexes the given data into Redis.\n        :param column_to_data: A dictionary where each key is a column and each value is a generator.\n        :return: A list of document ids that have been indexed.\n        \"\"\"\nself._index_subindex(column_to_data)\nids: List[str] = []\nfor items in self._generate_items(\ncolumn_to_data, self._runtime_config.batch_size\n):\ndoc_id_item_pairs = [\n(self._prefix + item['id'], '$', item) for item in items\n]\nids.extend(doc_id for doc_id, _, _ in doc_id_item_pairs)\nself._client.json().mset(doc_id_item_pairs)  # type: ignore[attr-defined]\nreturn ids\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Fetch the number of documents in the index.\n        :return: Number of documents in the index.\n        \"\"\"\nnum_docs = self._client.ft(self.index_name).info()['num_docs']\nreturn int(num_docs)\ndef _del_items(self, doc_ids: Sequence[str]) -&gt; None:\n\"\"\"\n        Deletes documents from the index based on document ids.\n        :param doc_ids: A sequence of document ids to be deleted.\n        \"\"\"\ndoc_ids = [self._prefix + id for id in doc_ids if self._doc_exists(id)]\nif doc_ids:\nfor batch in self._generate_batches(\ndoc_ids, batch_size=self._runtime_config.batch_size\n):\nself._client.delete(*batch)\ndef _doc_exists(self, doc_id: str) -&gt; bool:\n\"\"\"\n        Checks if a document exists in the index.\n        :param doc_id: The id of the document.\n        :return: True if the document exists, False otherwise.\n        \"\"\"\nreturn bool(self._client.exists(self._prefix + doc_id))\n@staticmethod\ndef _generate_batches(data, batch_size):\nfor i in range(0, len(data), batch_size):\nyield data[i : i + batch_size]\ndef _get_items(\nself, doc_ids: Sequence[str]\n) -&gt; Union[Sequence[TSchema], Sequence[Dict[str, Any]]]:\n\"\"\"\n        Fetches the documents from the index based on document ids.\n        :param doc_ids: A sequence of document ids.\n        :return: A sequence of documents from the index.\n        \"\"\"\nif not doc_ids:\nreturn []\ndocs: List[Dict[str, Any]] = []\nfor batch in self._generate_batches(\ndoc_ids, batch_size=self._runtime_config.batch_size\n):\nids = [self._prefix + id for id in batch]\nretrieved_docs = self._client.json().mget(ids, '$')\ndocs.extend(doc[0] for doc in retrieved_docs if doc)\nif not docs:\nraise KeyError(f'No document with id {doc_ids} found')\nreturn docs\ndef execute_query(self, query: Any, *args: Any, **kwargs: Any) -&gt; Any:\n\"\"\"\n        Executes a hybrid query on the index.\n        :param query: Query to execute on the index.\n        :return: Query results.\n        \"\"\"\ncomponents: Dict[str, List[Dict[str, Any]]] = {}\nfor component, value in query:\nif component not in components:\ncomponents[component] = []\ncomponents[component].append(value)\nif (\nlen(components) != 2\nor len(components.get('find', [])) != 1\nor len(components.get('filter', [])) != 1\n):\nraise ValueError(\n'The query must contain exactly one \"find\" and \"filter\" components.'\n)\nfilter_query = components['filter'][0]['filter_query']\nquery = components['find'][0]['query']\nsearch_field = components['find'][0]['search_field']\nlimit = (\ncomponents['find'][0].get('limit')\nor components['filter'][0].get('limit')\nor 10\n)\ndocs, scores = self._hybrid_search(\nquery=query,\nfilter_query=filter_query,\nsearch_field=search_field,\nlimit=limit,\n)\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef _hybrid_search(\nself, query: np.ndarray, filter_query: str, search_field: str, limit: int\n) -&gt; _FindResult:\n\"\"\"\n        Conducts a hybrid search (a combination of vector search and filter-based search) on the index.\n        :param query: The query to search.\n        :param filter_query: The filter condition.\n        :param search_field: The vector field to search on.\n        :param limit: The maximum number of results to return.\n        :return: Query results.\n        \"\"\"\nredis_query = (\nQuery(f'{filter_query}=&gt;[KNN {limit} @{search_field} $vec AS vector_score]')\n.sort_by('vector_score')\n.paging(0, limit)\n.dialect(2)\n)\nquery_params: Mapping[str, bytes] = {\n'vec': np.array(query, dtype=np.float32).tobytes()\n}\nresults = (\nself._client.ft(self.index_name).search(redis_query, query_params).docs  # type: ignore[arg-type]\n)\nscores: NdArray = NdArray._docarray_from_native(\nnp.array([document['vector_score'] for document in results])\n)\ndocs = []\nfor out_doc in results:\ndoc_dict = json.loads(out_doc.json)\ndocs.append(doc_dict)\nreturn _FindResult(documents=docs, scores=scores)\ndef _find(\nself, query: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResult:\n\"\"\"\n        Conducts a search on the index.\n        :param query: The vector query to search.\n        :param limit: The maximum number of results to return.\n        :param search_field: The field to search the query.\n        :return: Search results.\n        \"\"\"\nreturn self._hybrid_search(\nquery=query, filter_query='*', search_field=search_field, limit=limit\n)\ndef _find_batched(\nself, queries: np.ndarray, limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\n\"\"\"\n        Conducts a batched search on the index.\n        :param queries: The queries to search.\n        :param limit: The maximum number of results to return for each query.\n        :param search_field: The field to search the queries.\n        :return: Search results.\n        \"\"\"\ndocs, scores = [], []\nfor query in queries:\nresults = self._find(query=query, search_field=search_field, limit=limit)\ndocs.append(results.documents)\nscores.append(results.scores)\nreturn _FindResultBatched(documents=docs, scores=scores)\ndef _filter(self, filter_query: Any, limit: int) -&gt; Union[DocList, List[Dict]]:\n\"\"\"\n        Filters the index based on the given filter query.\n        :param filter_query: The filter condition.\n        :param limit: The maximum number of results to return.\n        :return: Filter results.\n        \"\"\"\nq = Query(filter_query)\nq.paging(0, limit)\nresults = self._client.ft(index_name=self.index_name).search(q).docs\ndocs = [json.loads(doc.json) for doc in results]\nreturn docs\ndef _filter_batched(\nself, filter_queries: Any, limit: int\n) -&gt; Union[List[DocList], List[List[Dict]]]:\n\"\"\"\n        Filters the index based on the given batch of filter queries.\n        :param filter_queries: The filter conditions.\n        :param limit: The maximum number of results to return for each filter query.\n        :return: Filter results.\n        \"\"\"\nresults = []\nfor query in filter_queries:\nresults.append(self._filter(filter_query=query, limit=limit))\nreturn results\ndef _filter_by_parent_id(self, id: str) -&gt; Optional[List[str]]:\n\"\"\"Filter the ids of the subindex documents given id of root document.\n        :param id: the root document id to filter by\n        :return: a list of ids of the subindex documents\n        \"\"\"\ndocs = self._filter(filter_query=f'@parent_id:{{{id}}}', limit=self.num_docs())\nreturn [doc['id'] for doc in docs]\ndef _text_search(\nself, query: str, limit: int, search_field: str = ''\n) -&gt; _FindResult:\n\"\"\"\n        Conducts a text-based search on the index.\n        :param query: The query to search.\n        :param limit: The maximum number of results to return.\n        :param search_field: The field to search the query.\n        :return: Search results.\n        \"\"\"\nquery_str = '|'.join(query.split(' '))\nq = (\nQuery(f'@{search_field}:{query_str}')\n.scorer(self._text_scorer)\n.with_scores()\n.paging(0, limit)\n)\nresults = self._client.ft(index_name=self.index_name).search(q).docs\nscores: NdArray = NdArray._docarray_from_native(\nnp.array([document['score'] for document in results])\n)\ndocs = [json.loads(doc.json) for doc in results]\nreturn _FindResult(documents=docs, scores=scores)\ndef _text_search_batched(\nself, queries: Sequence[str], limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\n\"\"\"\n        Conducts a batched text-based search on the index.\n        :param queries: The queries to search.\n        :param limit: The maximum number of results to return for each query.\n        :param search_field: The field to search the queries.\n        :return: Search results.\n        \"\"\"\ndocs, scores = [], []\nfor query in queries:\nresults = self._text_search(\nquery=query, search_field=search_field, limit=limit\n)\ndocs.append(results.documents)\nscores.append(results.scores)\nreturn _FindResultBatched(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.out_schema","title":"<code>out_schema: Type[BaseDoc]</code>  <code>property</code>","text":"<p>Return the real schema of the index.</p>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of RedisDocumentIndex.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host address for the Redis server. Default is 'localhost'.</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>The port number for the Redis server. Default is 6379.</p> <code>6379</code> <code>index_name</code> <code>Optional[str]</code> <p>The name of the index in the Redis database. If not provided, default index name will be used.</p> <code>None</code> <code>username</code> <code>Optional[str]</code> <p>The username for the Redis server. Default is None.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>The password for the Redis server. Default is None.</p> <code>None</code> <code>text_scorer</code> <code>str</code> <p>The method for scoring text during text search. Default is 'BM25'.</p> <code>field(default='BM25')</code> <code>default_column_config</code> <code>Dict[Type, Dict[str, Any]]</code> <p>Default configuration for columns.</p> <code>field(default_factory=lambda : defaultdict(dict, {VectorField: {'algorithm': 'FLAT', 'distance': 'COSINE', 'ef_construction': None, 'm': None, 'ef_runtime': None, 'initial_cap': None}}))</code> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of RedisDocumentIndex.\n    :param host: The host address for the Redis server. Default is 'localhost'.\n    :param port: The port number for the Redis server. Default is 6379.\n    :param index_name: The name of the index in the Redis database.\n        If not provided, default index name will be used.\n    :param username: The username for the Redis server. Default is None.\n    :param password: The password for the Redis server. Default is None.\n    :param text_scorer: The method for scoring text during text search.\n        Default is 'BM25'.\n    :param default_column_config: Default configuration for columns.\n    \"\"\"\nhost: str = 'localhost'\nport: int = 6379\nindex_name: Optional[str] = None\nusername: Optional[str] = None\npassword: Optional[str] = None\ntext_scorer: str = field(default='BM25')\ndefault_column_config: Dict[Type, Dict[str, Any]] = field(\ndefault_factory=lambda: defaultdict(\ndict,\n{\nVectorField: {\n'algorithm': 'FLAT',\n'distance': 'COSINE',\n'ef_construction': None,\n'm': None,\n'ef_runtime': None,\n'initial_cap': None,\n},\n},\n)\n)\ndef __post_init__(self):\nself.text_scorer = self.text_scorer.upper()\nif self.text_scorer not in VALID_TEXT_SCORERS:\nraise ValueError(\nf\"Invalid text scorer '{self.text_scorer}' provided. \"\nf\"Must be one of: {', '.join(VALID_TEXT_SCORERS)}\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, query: Optional[List[Tuple[str, Dict]]] = None):\nsuper().__init__()\n# list of tuples (method name, kwargs)\nself._queries: List[Tuple[str, Dict]] = query or []\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\nfind = _collect_query_args('find')\nfilter = _collect_query_args('filter')\ntext_search = _raise_not_composable('text_search')\nfind_batched = _raise_not_composable('find_batched')\nfilter_batched = _raise_not_composable('filter_batched')\ntext_search_batched = _raise_not_composable('text_search_batched')\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the query object.</p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nreturn self._queries\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for index/get/del.</p> <code>100</code> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of RedisDocumentIndex.\n    :param batch_size: Batch size for index/get/del.\n    \"\"\"\nbatch_size: int = 100\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize RedisDocumentIndex</p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>def __init__(self, db_config=None, **kwargs):\n\"\"\"Initialize RedisDocumentIndex\"\"\"\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config = cast(RedisDocumentIndex.DBConfig, self._db_config)\nself._runtime_config: RedisDocumentIndex.RuntimeConfig = cast(\nRedisDocumentIndex.RuntimeConfig, self._runtime_config\n)\nself._prefix = self.index_name + ':'\nself._text_scorer = self._db_config.text_scorer\n# initialize Redis client\nself._client = redis.Redis(\nhost=self._db_config.host,\nport=self._db_config.port,\nusername=self._db_config.username,\npassword=self._db_config.password,\ndecode_responses=False,\n)\nself._create_index()\nself._logger.info(f'{self.__class__.__name__} has been initialized')\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for this DocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>a new <code>QueryBuilder</code> object for this DocumentIndex</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def build_query(self) -&gt; QueryBuilder:\n\"\"\"\n    Build a query for this DocumentIndex.\n    :return: a new `QueryBuilder` object for this DocumentIndex\n    \"\"\"\nreturn self.QueryBuilder()  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the DocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/abstract.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs):\n\"\"\"\n    Configure the DocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nif runtime_config is None:\nself._runtime_config = replace(self._runtime_config, **kwargs)\nelse:\nif not isinstance(runtime_config, self.RuntimeConfig):\nraise ValueError(f'runtime_config must be of type {self.RuntimeConfig}')\nself._runtime_config = runtime_config\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Executes a hybrid query on the index.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Any</code> <p>Query to execute on the index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Query results.</p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>def execute_query(self, query: Any, *args: Any, **kwargs: Any) -&gt; Any:\n\"\"\"\n    Executes a hybrid query on the index.\n    :param query: Query to execute on the index.\n    :return: Query results.\n    \"\"\"\ncomponents: Dict[str, List[Dict[str, Any]]] = {}\nfor component, value in query:\nif component not in components:\ncomponents[component] = []\ncomponents[component].append(value)\nif (\nlen(components) != 2\nor len(components.get('find', [])) != 1\nor len(components.get('filter', [])) != 1\n):\nraise ValueError(\n'The query must contain exactly one \"find\" and \"filter\" components.'\n)\nfilter_query = components['filter'][0]['filter_query']\nquery = components['find'][0]['query']\nsearch_field = components['find'][0]['search_field']\nlimit = (\ncomponents['find'][0].get('limit')\nor components['filter'][0].get('limit')\nor 10\n)\ndocs, scores = self._hybrid_search(\nquery=query,\nfilter_query=filter_query,\nsearch_field=search_field,\nlimit=limit,\n)\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `find_batched` for search field {search_field}')\nif search_field:\nif '__' in search_field:\nfields = search_field.split('__')\nif safe_issubclass(self._schema._get_field_annotation(fields[0]), AnyDocArray):  # type: ignore\nreturn self._subindices[fields[0]].find_batched(\nqueries,\nsearch_field='__'.join(fields[1:]),\nlimit=limit,\n**kwargs,\n)\nself._validate_search_field(search_field)\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, search_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif (\nlen(da_list) &gt; 0\nand isinstance(da_list[0], List)\nand not isinstance(da_list[0], DocList)\n):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Fetch the number of documents in the index.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of documents in the index.</p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Fetch the number of documents in the index.\n    :return: Number of documents in the index.\n    \"\"\"\nnum_docs = self._client.ft(self.index_name).info()['num_docs']\nreturn int(num_docs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python types to corresponding Redis types.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>Python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Corresponding Redis type.</p> Source code in <code>docarray/index/backends/redis.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"\n    Map python types to corresponding Redis types.\n    :param python_type: Python type.\n    :return: Corresponding Redis type.\n    \"\"\"\ntype_map = {\nint: NumericField,\nfloat: NumericField,\nstr: TextField,\nbytes: TextField,\nnp.ndarray: VectorField,\nlist: VectorField,\nAbstractTensor: VectorField,\n}\nfor py_type, redis_type in type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn redis_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/redis/#docarray.index.backends.redis.RedisDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/","title":"WeaviateDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/weaviate/#weaviatedocumentindex","title":"WeaviateDocumentIndex","text":""},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex","title":"<code>docarray.index.backends.weaviate.WeaviateDocumentIndex</code>","text":"<p>             Bases: <code>BaseDocIndex</code>, <code>Generic[TSchema]</code></p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>class WeaviateDocumentIndex(BaseDocIndex, Generic[TSchema]):\ndef __init__(self, db_config=None, **kwargs) -&gt; None:\n\"\"\"Initialize WeaviateDocumentIndex\"\"\"\nself.embedding_column: Optional[str] = None\nself.properties: Optional[List[str]] = None\n# keep track of the column name that contains the bytes\n# type because we will store them as a base64 encoded string\n# in weaviate\nself.bytes_columns: List[str] = []\n# keep track of the array columns that are not embeddings because we will\n# convert them to python lists before uploading to weaviate\nself.nonembedding_array_columns: List[str] = []\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: WeaviateDocumentIndex.DBConfig = cast(\nWeaviateDocumentIndex.DBConfig, self._db_config\n)\nself._runtime_config: WeaviateDocumentIndex.RuntimeConfig = cast(\nWeaviateDocumentIndex.RuntimeConfig, self._runtime_config\n)\nif self._db_config.embedded_options:\nself._client = weaviate.Client(\nembedded_options=self._db_config.embedded_options\n)\nelse:\nself._client = weaviate.Client(\nself._db_config.host, auth_client_secret=self._build_auth_credentials()\n)\nself._configure_client()\nself._validate_columns()\nself._set_embedding_column()\nself._set_properties()\nself._create_schema()\n@property\ndef index_name(self):\ndefault_index_name = self._schema.__name__ if self._schema is not None else None\nif default_index_name is None:\nraise ValueError(\n'A WeaviateDocumentIndex must be typed with a Document type.'\n'To do so, use the syntax: WeaviateDocumentIndex[DocumentType]'\n)\nreturn self._db_config.index_name or default_index_name\ndef _set_properties(self) -&gt; None:\nfield_overwrites = {\"id\": DOCUMENTID}\nself.properties = [\nfield_overwrites.get(k, k)\nfor k, v in self._column_infos.items()\nif v.config.get('is_embedding', False) is False\nand not safe_issubclass(v.docarray_type, AnyDocArray)\n]\ndef _validate_columns(self) -&gt; None:\n# must have at most one column with property is_embedding=True\n# and that column must be of type WEAVIATE_PY_VEC_TYPES\n# TODO: update when https://github.com/weaviate/weaviate/issues/2424\n# is implemented and discuss best interface to signal which column(s)\n# should be used for embeddings\nnum_embedding_columns = 0\nfor column_name, column_info in self._column_infos.items():\nif column_info.config.get('is_embedding', False):\nnum_embedding_columns += 1\n# if db_type is not 'number[]', then that means the type of the column in\n# the given schema is not one of WEAVIATE_PY_VEC_TYPES\n# note: the mapping between a column's type in the schema to a weaviate type\n# is handled by the python_type_to_db_type method\nif column_info.db_type != 'number[]':\nraise ValueError(\nf'Column {column_name} is marked as embedding but is not of type {WEAVIATE_PY_VEC_TYPES}'\n)\nif num_embedding_columns &gt; 1:\nraise ValueError(\nf'Only one column can be marked as embedding but found {num_embedding_columns} columns marked as embedding'\n)\ndef _set_embedding_column(self) -&gt; None:\nfor column_name, column_info in self._column_infos.items():\nif column_info.config.get('is_embedding', False):\nself.embedding_column = column_name\nbreak\ndef _configure_client(self) -&gt; None:\nself._client.batch.configure(**self._runtime_config.batch_config)\ndef _build_auth_credentials(self):\ndbconfig = self._db_config\nif dbconfig.auth_api_key:\nreturn weaviate.auth.AuthApiKey(api_key=dbconfig.auth_api_key)\nelif dbconfig.username and dbconfig.password:\nreturn weaviate.auth.AuthClientPassword(\ndbconfig.username, dbconfig.password, dbconfig.scopes\n)\nelse:\nreturn None\ndef configure(self, runtime_config=None, **kwargs) -&gt; None:\n\"\"\"\n        Configure the WeaviateDocumentIndex.\n        You can either pass a config object to `config` or pass individual config\n        parameters as keyword arguments.\n        If a configuration object is passed, it will replace the current configuration.\n        If keyword arguments are passed, they will update the current configuration.\n        :param runtime_config: the configuration to apply\n        :param kwargs: individual configuration parameters\n        \"\"\"\nsuper().configure(runtime_config, **kwargs)\nself._configure_client()\ndef _create_schema(self) -&gt; None:\nschema: Dict[str, Any] = {}\nproperties = []\ncolumn_infos = self._column_infos\nfor column_name, column_info in column_infos.items():\n# in weaviate, we do not create a property for the doc's embeddings\nif safe_issubclass(column_info.docarray_type, AnyDocArray):\ncontinue\nif column_name == self.embedding_column:\ncontinue\nif column_info.db_type == 'blob':\nself.bytes_columns.append(column_name)\nif column_info.db_type == 'number[]':\nself.nonembedding_array_columns.append(column_name)\nprop = {\n\"name\": column_name\nif column_name != 'id'\nelse DOCUMENTID,  # in weaviate, id and _id is a reserved keyword\n\"dataType\": [column_info.db_type],\n}\nproperties.append(prop)\n# TODO: What is the best way to specify other config that is part of schema?\n# e.g. invertedIndexConfig, shardingConfig, moduleConfig, vectorIndexConfig\n#       and configure replication\n# we will update base on user feedback\nschema[\"properties\"] = properties\nschema[\"class\"] = self.index_name\nif self._client.schema.exists(self.index_name):\nlogging.warning(\nf\"Found index {self.index_name} with schema {schema}. Will reuse existing schema.\"\n)\nelse:\nself._client.schema.create_class(schema)\n@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of WeaviateDocumentIndex.\"\"\"\nhost: str = 'http://localhost:8080'\nindex_name: Optional[str] = None\nusername: Optional[str] = None\npassword: Optional[str] = None\nscopes: List[str] = field(default_factory=lambda: [\"offline_access\"])\nauth_api_key: Optional[str] = None\nembedded_options: Optional[EmbeddedOptions] = None\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(\ndefault_factory=lambda: {\nnp.ndarray: {},\ndocarray.typing.ID: {},\n'string': {},\n'text': {},\n'int': {},\n'number': {},\n'boolean': {},\n'number[]': {},\n'blob': {},\n}\n)\ndef __post_init__(self):\n# To prevent errors, it is important to capitalize the provided index name\n# when working with Weaviate, as it stores index names in a capitalized format.\n# Can't use .capitalize() because it modifies the whole string (See test).\nself.index_name = (\nself.index_name[0].upper() + self.index_name[1:]\nif self.index_name\nelse None\n)\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of WeaviateDocumentIndex.\"\"\"\nbatch_config: Dict[str, Any] = field(\ndefault_factory=lambda: DEFAULT_BATCH_CONFIG\n)\ndef _del_items(self, doc_ids: Sequence[str]):\nhas_matches = True\noperands = [\n{\"path\": [DOCUMENTID], \"operator\": \"Equal\", \"valueString\": doc_id}\nfor doc_id in doc_ids\n]\nwhere_filter = {\n\"operator\": \"Or\",\n\"operands\": operands,\n}\n# do a loop because there is a limit to how many objects can be deleted at\n# in a single query\n# see: https://weaviate.io/developers/weaviate/api/rest/batch#maximum-number-of-deletes-per-query\nwhile has_matches:\nresults = self._client.batch.delete_objects(\nclass_name=self.index_name,\nwhere=where_filter,\n)\nhas_matches = results[\"results\"][\"matches\"]\ndef _filter(self, filter_query: Any, limit: int) -&gt; Union[DocList, List[Dict]]:\nself._overwrite_id(filter_query)\nresults = (\nself._client.query.get(self.index_name, self.properties)\n.with_additional(\"vector\")\n.with_where(filter_query)\n.with_limit(limit)\n.do()\n)\ndocs = results[\"data\"][\"Get\"][self.index_name]\nreturn [self._parse_weaviate_result(doc) for doc in docs]\ndef _filter_batched(\nself, filter_queries: Any, limit: int\n) -&gt; Union[List[DocList], List[List[Dict]]]:\nfor filter_query in filter_queries:\nself._overwrite_id(filter_query)\nqs = [\nself._client.query.get(self.index_name, self.properties)\n.with_additional(\"vector\")\n.with_where(filter_query)\n.with_limit(limit)\n.with_alias(f'query_{i}')\nfor i, filter_query in enumerate(filter_queries)\n]\nbatched_results = self._client.query.multi_get(qs).do()\nreturn [\n[self._parse_weaviate_result(doc) for doc in batched_result]\nfor batched_result in batched_results[\"data\"][\"Get\"].values()\n]\ndef find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n):\n\"\"\"\n        Find k-nearest neighbors of the query.\n        :param query: query vector for KNN/ANN search. Has single axis.\n        :param search_field: name of the field to search on\n        :param limit: maximum number of documents to return per query\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug('Executing `find`')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for WeaviateDocumentIndex.\\nSet search_field to an empty string to proceed.'\n)\nembedding_field = self._get_embedding_field()\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], embedding_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\ndef _overwrite_id(self, where_filter):\n\"\"\"\n        Overwrite the id field in the where filter to DOCUMENTID\n        if the \"id\" field is present in the path\n        \"\"\"\nfor key, value in where_filter.items():\nif key == \"path\" and value == [\"id\"]:\nwhere_filter[key] = [DOCUMENTID]\nelif isinstance(value, dict):\nself._overwrite_id(value)\nelif isinstance(value, list):\nfor item in value:\nif isinstance(item, dict):\nself._overwrite_id(item)\ndef _find(\nself,\nquery: np.ndarray,\nlimit: int,\nsearch_field: str = '',\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n) -&gt; _FindResult:\nindex_name = self.index_name\nif search_field:\nlogging.warning(\n'The search_field argument is not supported for the WeaviateDocumentIndex and will be ignored.'\n)\nnear_vector: Dict[str, Any] = {\n\"vector\": query,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nresults = (\nself._client.query.get(index_name, self.properties)\n.with_near_vector(\nnear_vector,\n)\n.with_limit(limit)\n.with_additional([score_name, \"vector\"])\n.do()\n)\ndocs, scores = self._format_response(\nresults[\"data\"][\"Get\"][index_name], score_name\n)\nreturn _FindResult(docs, parse_obj_as(NdArray, scores))\ndef _format_response(\nself, results, score_name\n) -&gt; Tuple[List[Dict[Any, Any]], List[Any]]:\n\"\"\"\n        Format the response from Weaviate into a Tuple of DocList and scores\n        \"\"\"\ndocuments = []\nscores = []\nfor result in results:\nscore = result[\"_additional\"][score_name]\nscores.append(score)\ndocument = self._parse_weaviate_result(result)\ndocuments.append(document)\nreturn documents, scores\ndef find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs: Any,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n        :param queries: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n            or a DocList.\n            If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n        :param search_field: name of the field to search on.\n            Documents in the index are retrieved based on this similarity\n            of this field to the query.\n        :param limit: maximum number of documents to return per query\n        :return: a named tuple containing `documents` and `scores`\n        \"\"\"\nself._logger.debug('Executing `find_batched`')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for WeaviateDocumentIndex.\\nSet search_field to an empty string to proceed.'\n)\nembedding_field = self._get_embedding_field()\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, embedding_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\ndef _find_batched(\nself,\nqueries: np.ndarray,\nlimit: int,\nsearch_field: str = '',\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n) -&gt; _FindResultBatched:\nqs = []\nfor i, query in enumerate(queries):\nnear_vector: Dict[str, Any] = {\"vector\": query}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nq = (\nself._client.query.get(self.index_name, self.properties)\n.with_near_vector(near_vector)\n.with_limit(limit)\n.with_additional([score_name, \"vector\"])\n.with_alias(f'query_{i}')\n)\nqs.append(q)\nresults = self._client.query.multi_get(qs).do()\ndocs_and_scores = [\nself._format_response(result, score_name)\nfor result in results[\"data\"][\"Get\"].values()\n]\ndocs, scores = zip(*docs_and_scores)\nreturn _FindResultBatched(list(docs), list(scores))\ndef _get_items(self, doc_ids: Sequence[str]) -&gt; List[Dict]:\n# TODO: warn when doc_ids &gt; QUERY_MAXIMUM_RESULTS after\n#       https://github.com/weaviate/weaviate/issues/2792\n#       is implemented\noperands = [\n{\"path\": [DOCUMENTID], \"operator\": \"Equal\", \"valueString\": doc_id}\nfor doc_id in doc_ids\n]\nwhere_filter = {\n\"operator\": \"Or\",\n\"operands\": operands,\n}\nresults = (\nself._client.query.get(self.index_name, self.properties)\n.with_where(where_filter)\n.with_additional(\"vector\")\n.do()\n)\ndocs = [\nself._parse_weaviate_result(doc)\nfor doc in results[\"data\"][\"Get\"][self.index_name]\n]\nreturn docs\ndef _rewrite_documentid(self, document: Dict):\ndoc = document.copy()\n# rewrite the id to DOCUMENTID\ndocument_id = doc.pop('id')\ndoc[DOCUMENTID] = document_id\nreturn doc\ndef _parse_weaviate_result(self, result: Dict) -&gt; Dict:\n\"\"\"\n        Parse the result from weaviate to a format that is compatible with the schema\n        that was used to initialize weaviate with.\n        \"\"\"\nresult = result.copy()\n# rewrite the DOCUMENTID to id\nif DOCUMENTID in result:\nresult['id'] = result.pop(DOCUMENTID)\n# take the vector from the _additional field\nif '_additional' in result and self.embedding_column:\nadditional_fields = result.pop('_additional')\nif 'vector' in additional_fields:\nresult[self.embedding_column] = additional_fields['vector']\n# convert any base64 encoded bytes column to bytes\nself._decode_base64_properties_to_bytes(result)\nreturn result\ndef _index(self, column_to_data: Dict[str, Generator[Any, None, None]]):\nself._index_subindex(column_to_data)\ndocs = self._transpose_col_value_dict(column_to_data)\nindex_name = self.index_name\nwith self._client.batch as batch:\nfor doc in docs:\nparsed_doc = self._rewrite_documentid(doc)\nself._encode_bytes_columns_to_base64(parsed_doc)\nself._convert_nonembedding_array_to_list(parsed_doc)\nvector = (\nparsed_doc.pop(self.embedding_column)\nif self.embedding_column\nelse None\n)\nbatch.add_data_object(\nuuid=weaviate.util.generate_uuid5(parsed_doc, index_name),\ndata_object=parsed_doc,\nclass_name=index_name,\nvector=vector,\n)\ndef _text_search(\nself, query: str, limit: int, search_field: str = ''\n) -&gt; _FindResult:\nindex_name = self.index_name\nbm25 = {\"query\": query, \"properties\": [search_field]}\nresults = (\nself._client.query.get(index_name, self.properties)\n.with_bm25(**bm25)\n.with_limit(limit)\n.with_additional([\"score\", \"vector\"])\n.do()\n)\ndocs, scores = self._format_response(\nresults[\"data\"][\"Get\"][index_name], \"score\"\n)\nreturn _FindResult(documents=docs, scores=parse_obj_as(NdArray, scores))\ndef _text_search_batched(\nself, queries: Sequence[str], limit: int, search_field: str = ''\n) -&gt; _FindResultBatched:\nqs = []\nfor i, query in enumerate(queries):\nbm25 = {\"query\": query, \"properties\": [search_field]}\nq = (\nself._client.query.get(self.index_name, self.properties)\n.with_bm25(**bm25)\n.with_limit(limit)\n.with_additional([\"score\", \"vector\"])\n.with_alias(f'query_{i}')\n)\nqs.append(q)\nresults = self._client.query.multi_get(qs).do()\ndocs_and_scores = [\nself._format_response(result, \"score\")\nfor result in results[\"data\"][\"Get\"].values()\n]\ndocs, scores = zip(*docs_and_scores)\nreturn _FindResultBatched(list(docs), list(scores))\ndef execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n        Execute a query on the WeaviateDocumentIndex.\n        Can take two kinds of inputs:\n        1. A native query of the underlying database. This is meant as a passthrough so that you\n        can enjoy any functionality that is not available through the Document index API.\n        2. The output of this Document index' `QueryBuilder.build()` method.\n        :param query: the query to execute\n        :param args: positional arguments to pass to the query\n        :param kwargs: keyword arguments to pass to the query\n        :return: the result of the query\n        \"\"\"\nda_class = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nif isinstance(query, self.QueryBuilder):\nbatched_results = self._client.query.multi_get(query._queries).do()\nbatched_docs = batched_results[\"data\"][\"Get\"].values()\ndef f(doc):\n# TODO: use\n# return self._schema(**self._parse_weaviate_result(doc))\n# when https://github.com/weaviate/weaviate/issues/2858\n# is fixed\nreturn self._schema.from_view(self._parse_weaviate_result(doc))  # type: ignore\nresults = [\nda_class([f(doc) for doc in batched_doc])\nfor batched_doc in batched_docs\n]\nreturn results if len(results) &gt; 1 else results[0]\n# TODO: validate graphql query string before sending it to weaviate\nif isinstance(query, str):\nreturn self._client.query.raw(query)\ndef num_docs(self) -&gt; int:\n\"\"\"\n        Get the number of documents.\n        \"\"\"\nindex_name = self.index_name\nresult = self._client.query.aggregate(index_name).with_meta_count().do()\n# TODO: decorator to check for errors\ntotal_docs = result[\"data\"][\"Aggregate\"][index_name][0][\"meta\"][\"count\"]\nreturn total_docs\ndef python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n        Takes any python type and returns the corresponding database column type.\n        :param python_type: a python type.\n        :return: the corresponding database column type,\n            or None if ``python_type`` is not supported.\n        \"\"\"\nfor allowed_type in WEAVIATE_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nreturn 'number[]'\npy_weaviate_type_map = {\ndocarray.typing.ID: 'string',\nstr: 'text',\nint: 'int',\nfloat: 'number',\nbool: 'boolean',\nnp.ndarray: 'number[]',\nbytes: 'blob',\n}\nfor py_type, weaviate_type in py_weaviate_type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn weaviate_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\ndef build_query(self) -&gt; BaseDocIndex.QueryBuilder:\n\"\"\"\n        Build a query for WeaviateDocumentIndex.\n        :return: QueryBuilder object\n        \"\"\"\nreturn self.QueryBuilder(self)\ndef _get_embedding_field(self):\nfor colname, colinfo in self._column_infos.items():\n# no need to check for missing is_embedding attribute because this check\n# is done when the index is created\nif colinfo.config.get('is_embedding', None):\nreturn colname\n# just to pass mypy\nreturn \"\"\ndef _encode_bytes_columns_to_base64(self, doc):\nfor column in self.bytes_columns:\nif doc[column] is not None:\ndoc[column] = base64.b64encode(doc[column]).decode(\"utf-8\")\ndef _decode_base64_properties_to_bytes(self, doc):\nfor column in self.bytes_columns:\nif doc[column] is not None:\ndoc[column] = base64.b64decode(doc[column])\ndef _convert_nonembedding_array_to_list(self, doc):\nfor column in self.nonembedding_array_columns:\nif doc[column] is not None:\ndoc[column] = doc[column].tolist()\ndef _filter_by_parent_id(self, id: str) -&gt; Optional[List[str]]:\nresults = (\nself._client.query.get(self._db_config.index_name, ['docarrayid'])\n.with_where(\n{'path': ['parent_id'], 'operator': 'Equal', 'valueString': f'{id}'}\n)\n.do()\n)\nids = [\nres['docarrayid']\nfor res in results['data']['Get'][self._db_config.index_name]\n]\nreturn ids\ndef _doc_exists(self, doc_id: str) -&gt; bool:\nresult = (\nself._client.query.get(self.index_name, ['docarrayid'])\n.with_where(\n{\n\"path\": ['docarrayid'],\n\"operator\": \"Equal\",\n\"valueString\": f'{doc_id}',\n}\n)\n.do()\n)\ndocs = result[\"data\"][\"Get\"][self.index_name]\nreturn docs is not None and len(docs) &gt; 0\nclass QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, document_index):\nself._queries = [\ndocument_index._client.query.get(\ndocument_index.index_name, document_index.properties\n)\n]\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nnum_queries = len(self._queries)\nfor i in range(num_queries):\nq = self._queries[i]\nif self._is_hybrid_query(q):\nself._make_proper_hybrid_query(q)\nq.with_additional([\"vector\"]).with_alias(f'query_{i}')\nreturn self\ndef _is_hybrid_query(self, query: weaviate.gql.get.GetBuilder) -&gt; bool:\n\"\"\"\n            Checks if a query has been composed with both a with_bm25 and a with_near_vector verb\n            \"\"\"\nif not query._near_ask:\nreturn False\nelse:\nreturn query._bm25 and query._near_ask._content.get(\"vector\", None)\ndef _make_proper_hybrid_query(\nself, query: weaviate.gql.get.GetBuilder\n) -&gt; weaviate.gql.get.GetBuilder:\n\"\"\"\n            Modifies a query to be a proper hybrid query.\n            In weaviate, a query with with_bm25 and with_near_vector verb is not a hybrid query.\n            We need to use the with_hybrid verb to make it a hybrid query.\n            \"\"\"\ntext_query = query._bm25.query\nvector_query = query._near_ask._content[\"vector\"]\nhybrid_query = weaviate.gql.get.Hybrid(\nquery=text_query, vector=vector_query, alpha=0.5\n)\nquery._bm25 = None\nquery._near_ask = None\nquery._hybrid = hybrid_query\ndef _overwrite_id(self, where_filter):\n\"\"\"\n            Overwrite the id field in the where filter to DOCUMENTID\n            if the \"id\" field is present in the path\n            \"\"\"\nfor key, value in where_filter.items():\nif key == \"path\" and value == [\"id\"]:\nwhere_filter[key] = [DOCUMENTID]\nelif isinstance(value, dict):\nself._overwrite_id(value)\nelif isinstance(value, list):\nfor item in value:\nif isinstance(item, dict):\nself._overwrite_id(item)\ndef find(\nself,\nquery,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n**kwargs,\n) -&gt; Any:\n\"\"\"\n            Find k-nearest neighbors of the query.\n            :param query: query vector for search. Has single axis.\n            :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n            :param score_threshold: the threshold of the score\n            :return: self\n            \"\"\"\nif kwargs.get('search_field'):\nlogging.warning(\n'The search_field argument is not supported for the WeaviateDocumentIndex and will be ignored.'\n)\nnear_vector = {\n\"vector\": query,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nself._queries[0] = self._queries[0].with_near_vector(near_vector)\nreturn self\ndef find_batched(\nself,\nqueries,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n) -&gt; Any:\n\"\"\"Find k-nearest neighbors of the query vectors.\n            :param queries: query vector for KNN/ANN search.\n                Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n                or a DocList.\n                If a tensor-like is passed, it should have shape `(batch_size, vector_dim)`\n            :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n            :param score_threshold: the threshold of the score\n            :return: self\n            \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nnear_vector = {\n\"vector\": clause,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nnew_queries.append(query.with_near_vector(near_vector))\nself._queries = new_queries\nreturn self\ndef filter(self, where_filter: Any) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n            :param where_filter: a filter\n            :return: self\n            \"\"\"\nwhere_filter = where_filter.copy()\nself._overwrite_id(where_filter)\nself._queries[0] = self._queries[0].with_where(where_filter)\nreturn self\ndef filter_batched(self, filters) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n            :param filters: filters\n            :return: self\n            \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, filters\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nclause = clause.copy()\nself._overwrite_id(clause)\nnew_queries.append(query.with_where(clause))\nself._queries = new_queries\nreturn self\ndef text_search(self, query: str, search_field: Optional[str] = None) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n            :param query: The text to search for\n            :param search_field: name of the field to search on\n            :return: self\n            \"\"\"\nbm25: Dict[str, Any] = {\"query\": query}\nif search_field:\nbm25[\"properties\"] = [search_field]\nself._queries[0] = self._queries[0].with_bm25(**bm25)\nreturn self\ndef text_search_batched(\nself, queries: Sequence[str], search_field: Optional[str] = None\n) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n            :param queries: The texts to search for\n            :param search_field: name of the field to search on\n            :return: self\n            \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nbm25 = {\"query\": clause}\nif search_field:\nbm25[\"properties\"] = [search_field]\nnew_queries.append(query.with_bm25(**bm25))\nself._queries = new_queries\nreturn self\ndef limit(self, limit: int) -&gt; Any:\nself._queries = [query.with_limit(limit) for query in self._queries]\nreturn self\ndef _resize_queries_and_clauses(self, queries, clauses):\n\"\"\"\n            Adjust the length and content of queries and clauses so that we can compose\n            them element-wise\n            \"\"\"\nnum_clauses = len(clauses)\nnum_queries = len(queries)\n# if there's only one clause, then we assume that it should be applied\n# to every query\nif num_clauses == 1:\nreturn queries, clauses * num_queries\n# if there's only one query, then we can lengthen it to match the number\n# of clauses\nelif num_queries == 1:\nreturn [copy.deepcopy(queries[0]) for _ in range(num_clauses)], clauses\n# if the number of queries and clauses is the same, then we can just\n# return them as-is\nelif num_clauses == num_queries:\nreturn queries, clauses\nelse:\nraise ValueError(\nf\"Can't compose {num_clauses} clauses with {num_queries} queries\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.DBConfig","title":"<code>DBConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>DBConfig</code></p> <p>Dataclass that contains all \"static\" configurations of WeaviateDocumentIndex.</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\n\"\"\"Dataclass that contains all \"static\" configurations of WeaviateDocumentIndex.\"\"\"\nhost: str = 'http://localhost:8080'\nindex_name: Optional[str] = None\nusername: Optional[str] = None\npassword: Optional[str] = None\nscopes: List[str] = field(default_factory=lambda: [\"offline_access\"])\nauth_api_key: Optional[str] = None\nembedded_options: Optional[EmbeddedOptions] = None\ndefault_column_config: Dict[Any, Dict[str, Any]] = field(\ndefault_factory=lambda: {\nnp.ndarray: {},\ndocarray.typing.ID: {},\n'string': {},\n'text': {},\n'int': {},\n'number': {},\n'boolean': {},\n'number[]': {},\n'blob': {},\n}\n)\ndef __post_init__(self):\n# To prevent errors, it is important to capitalize the provided index name\n# when working with Weaviate, as it stores index names in a capitalized format.\n# Can't use .capitalize() because it modifies the whole string (See test).\nself.index_name = (\nself.index_name[0].upper() + self.index_name[1:]\nif self.index_name\nelse None\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder","title":"<code>QueryBuilder</code>","text":"<p>             Bases: <code>QueryBuilder</code></p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\ndef __init__(self, document_index):\nself._queries = [\ndocument_index._client.query.get(\ndocument_index.index_name, document_index.properties\n)\n]\ndef build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nnum_queries = len(self._queries)\nfor i in range(num_queries):\nq = self._queries[i]\nif self._is_hybrid_query(q):\nself._make_proper_hybrid_query(q)\nq.with_additional([\"vector\"]).with_alias(f'query_{i}')\nreturn self\ndef _is_hybrid_query(self, query: weaviate.gql.get.GetBuilder) -&gt; bool:\n\"\"\"\n        Checks if a query has been composed with both a with_bm25 and a with_near_vector verb\n        \"\"\"\nif not query._near_ask:\nreturn False\nelse:\nreturn query._bm25 and query._near_ask._content.get(\"vector\", None)\ndef _make_proper_hybrid_query(\nself, query: weaviate.gql.get.GetBuilder\n) -&gt; weaviate.gql.get.GetBuilder:\n\"\"\"\n        Modifies a query to be a proper hybrid query.\n        In weaviate, a query with with_bm25 and with_near_vector verb is not a hybrid query.\n        We need to use the with_hybrid verb to make it a hybrid query.\n        \"\"\"\ntext_query = query._bm25.query\nvector_query = query._near_ask._content[\"vector\"]\nhybrid_query = weaviate.gql.get.Hybrid(\nquery=text_query, vector=vector_query, alpha=0.5\n)\nquery._bm25 = None\nquery._near_ask = None\nquery._hybrid = hybrid_query\ndef _overwrite_id(self, where_filter):\n\"\"\"\n        Overwrite the id field in the where filter to DOCUMENTID\n        if the \"id\" field is present in the path\n        \"\"\"\nfor key, value in where_filter.items():\nif key == \"path\" and value == [\"id\"]:\nwhere_filter[key] = [DOCUMENTID]\nelif isinstance(value, dict):\nself._overwrite_id(value)\nelif isinstance(value, list):\nfor item in value:\nif isinstance(item, dict):\nself._overwrite_id(item)\ndef find(\nself,\nquery,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n**kwargs,\n) -&gt; Any:\n\"\"\"\n        Find k-nearest neighbors of the query.\n        :param query: query vector for search. Has single axis.\n        :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n        :param score_threshold: the threshold of the score\n        :return: self\n        \"\"\"\nif kwargs.get('search_field'):\nlogging.warning(\n'The search_field argument is not supported for the WeaviateDocumentIndex and will be ignored.'\n)\nnear_vector = {\n\"vector\": query,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nself._queries[0] = self._queries[0].with_near_vector(near_vector)\nreturn self\ndef find_batched(\nself,\nqueries,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n) -&gt; Any:\n\"\"\"Find k-nearest neighbors of the query vectors.\n        :param queries: query vector for KNN/ANN search.\n            Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n            or a DocList.\n            If a tensor-like is passed, it should have shape `(batch_size, vector_dim)`\n        :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n        :param score_threshold: the threshold of the score\n        :return: self\n        \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nnear_vector = {\n\"vector\": clause,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nnew_queries.append(query.with_near_vector(near_vector))\nself._queries = new_queries\nreturn self\ndef filter(self, where_filter: Any) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n        :param where_filter: a filter\n        :return: self\n        \"\"\"\nwhere_filter = where_filter.copy()\nself._overwrite_id(where_filter)\nself._queries[0] = self._queries[0].with_where(where_filter)\nreturn self\ndef filter_batched(self, filters) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n        :param filters: filters\n        :return: self\n        \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, filters\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nclause = clause.copy()\nself._overwrite_id(clause)\nnew_queries.append(query.with_where(clause))\nself._queries = new_queries\nreturn self\ndef text_search(self, query: str, search_field: Optional[str] = None) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n        :param query: The text to search for\n        :param search_field: name of the field to search on\n        :return: self\n        \"\"\"\nbm25: Dict[str, Any] = {\"query\": query}\nif search_field:\nbm25[\"properties\"] = [search_field]\nself._queries[0] = self._queries[0].with_bm25(**bm25)\nreturn self\ndef text_search_batched(\nself, queries: Sequence[str], search_field: Optional[str] = None\n) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n        :param queries: The texts to search for\n        :param search_field: name of the field to search on\n        :return: self\n        \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nbm25 = {\"query\": clause}\nif search_field:\nbm25[\"properties\"] = [search_field]\nnew_queries.append(query.with_bm25(**bm25))\nself._queries = new_queries\nreturn self\ndef limit(self, limit: int) -&gt; Any:\nself._queries = [query.with_limit(limit) for query in self._queries]\nreturn self\ndef _resize_queries_and_clauses(self, queries, clauses):\n\"\"\"\n        Adjust the length and content of queries and clauses so that we can compose\n        them element-wise\n        \"\"\"\nnum_clauses = len(clauses)\nnum_queries = len(queries)\n# if there's only one clause, then we assume that it should be applied\n# to every query\nif num_clauses == 1:\nreturn queries, clauses * num_queries\n# if there's only one query, then we can lengthen it to match the number\n# of clauses\nelif num_queries == 1:\nreturn [copy.deepcopy(queries[0]) for _ in range(num_clauses)], clauses\n# if the number of queries and clauses is the same, then we can just\n# return them as-is\nelif num_clauses == num_queries:\nreturn queries, clauses\nelse:\nraise ValueError(\nf\"Can't compose {num_clauses} clauses with {num_queries} queries\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.build","title":"<code>build(*args, **kwargs)</code>","text":"<p>Build the query object.</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def build(self, *args, **kwargs) -&gt; Any:\n\"\"\"Build the query object.\"\"\"\nnum_queries = len(self._queries)\nfor i in range(num_queries):\nq = self._queries[i]\nif self._is_hybrid_query(q):\nself._make_proper_hybrid_query(q)\nq.with_additional([\"vector\"]).with_alias(f'query_{i}')\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.filter","title":"<code>filter(where_filter)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>where_filter</code> <code>Any</code> <p>a filter</p> required <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def filter(self, where_filter: Any) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n    :param where_filter: a filter\n    :return: self\n    \"\"\"\nwhere_filter = where_filter.copy()\nself._overwrite_id(where_filter)\nself._queries[0] = self._queries[0].with_where(where_filter)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.filter_batched","title":"<code>filter_batched(filters)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <p>filters</p> required <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def filter_batched(self, filters) -&gt; Any:\n\"\"\"Find documents in the index based on a filter query\n    :param filters: filters\n    :return: self\n    \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, filters\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nclause = clause.copy()\nself._overwrite_id(clause)\nnew_queries.append(query.with_where(clause))\nself._queries = new_queries\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.find","title":"<code>find(query, score_name='certainty', score_threshold=None, **kwargs)</code>","text":"<p>Find k-nearest neighbors of the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query vector for search. Has single axis.</p> required <code>score_name</code> <code>Literal['certainty', 'distance']</code> <p>either <code>\"certainty\"</code> (default) or <code>\"distance\"</code></p> <code>'certainty'</code> <code>score_threshold</code> <code>Optional[float]</code> <p>the threshold of the score</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def find(\nself,\nquery,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n**kwargs,\n) -&gt; Any:\n\"\"\"\n    Find k-nearest neighbors of the query.\n    :param query: query vector for search. Has single axis.\n    :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n    :param score_threshold: the threshold of the score\n    :return: self\n    \"\"\"\nif kwargs.get('search_field'):\nlogging.warning(\n'The search_field argument is not supported for the WeaviateDocumentIndex and will be ignored.'\n)\nnear_vector = {\n\"vector\": query,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nself._queries[0] = self._queries[0].with_near_vector(near_vector)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.find_batched","title":"<code>find_batched(queries, score_name='certainty', score_threshold=None)</code>","text":"<p>Find k-nearest neighbors of the query vectors.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape <code>(batch_size, vector_dim)</code></p> required <code>score_name</code> <code>Literal['certainty', 'distance']</code> <p>either <code>\"certainty\"</code> (default) or <code>\"distance\"</code></p> <code>'certainty'</code> <code>score_threshold</code> <code>Optional[float]</code> <p>the threshold of the score</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def find_batched(\nself,\nqueries,\nscore_name: Literal[\"certainty\", \"distance\"] = \"certainty\",\nscore_threshold: Optional[float] = None,\n) -&gt; Any:\n\"\"\"Find k-nearest neighbors of the query vectors.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape `(batch_size, vector_dim)`\n    :param score_name: either `\"certainty\"` (default) or `\"distance\"`\n    :param score_threshold: the threshold of the score\n    :return: self\n    \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nnear_vector = {\n\"vector\": clause,\n}\nif score_threshold:\nnear_vector[score_name] = score_threshold\nnew_queries.append(query.with_near_vector(near_vector))\nself._queries = new_queries\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.text_search","title":"<code>text_search(query, search_field=None)</code>","text":"<p>Find documents in the index based on a text search query</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The text to search for</p> required <code>search_field</code> <code>Optional[str]</code> <p>name of the field to search on</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def text_search(self, query: str, search_field: Optional[str] = None) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :return: self\n    \"\"\"\nbm25: Dict[str, Any] = {\"query\": query}\nif search_field:\nbm25[\"properties\"] = [search_field]\nself._queries[0] = self._queries[0].with_bm25(**bm25)\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.QueryBuilder.text_search_batched","title":"<code>text_search_batched(queries, search_field=None)</code>","text":"<p>Find documents in the index based on a text search query</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Sequence[str]</code> <p>The texts to search for</p> required <code>search_field</code> <code>Optional[str]</code> <p>name of the field to search on</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def text_search_batched(\nself, queries: Sequence[str], search_field: Optional[str] = None\n) -&gt; Any:\n\"\"\"Find documents in the index based on a text search query\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :return: self\n    \"\"\"\nadj_queries, adj_clauses = self._resize_queries_and_clauses(\nself._queries, queries\n)\nnew_queries = []\nfor query, clause in zip(adj_queries, adj_clauses):\nbm25 = {\"query\": clause}\nif search_field:\nbm25[\"properties\"] = [search_field]\nnew_queries.append(query.with_bm25(**bm25))\nself._queries = new_queries\nreturn self\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.RuntimeConfig","title":"<code>RuntimeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>RuntimeConfig</code></p> <p>Dataclass that contains all \"dynamic\" configurations of WeaviateDocumentIndex.</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n\"\"\"Dataclass that contains all \"dynamic\" configurations of WeaviateDocumentIndex.\"\"\"\nbatch_config: Dict[str, Any] = field(\ndefault_factory=lambda: DEFAULT_BATCH_CONFIG\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Checks if a given document exists in the index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>The document to check. It must be an instance of BaseDoc or its subclass.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the document exists in the index, False otherwise.</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def __contains__(self, item: BaseDoc) -&gt; bool:\n\"\"\"\n    Checks if a given document exists in the index.\n    :param item: The document to check.\n        It must be an instance of BaseDoc or its subclass.\n    :return: True if the document exists in the index, False otherwise.\n    \"\"\"\nif safe_issubclass(type(item), BaseDoc):\nreturn self._doc_exists(str(item.id))\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete one or multiple Documents from the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to delete from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __delitem__(self, key: Union[str, Sequence[str]]):\n\"\"\"Delete one or multiple Documents from the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to delete from the Document index\n    \"\"\"\nself._logger.info(f'Deleting documents with id(s) {key} from the index')\nif isinstance(key, str):\nkey = [key]\n# delete nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray):\nfor doc_id in key:\nnested_docs_id = self._subindices[field_name]._filter_by_parent_id(\ndoc_id\n)\nif nested_docs_id:\ndel self._subindices[field_name][nested_docs_id]\n# delete data\nself._del_items(key)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get one or multiple Documents into the index, by <code>id</code>. If no document is found, a KeyError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, Sequence[str]]</code> <p>id or ids to get from the Document index</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def __getitem__(\nself, key: Union[str, Sequence[str]]\n) -&gt; Union[TSchema, DocList[TSchema]]:\n\"\"\"Get one or multiple Documents into the index, by `id`.\n    If no document is found, a KeyError is raised.\n    :param key: id or ids to get from the Document index\n    \"\"\"\n# normalize input\nif isinstance(key, str):\nreturn_singleton = True\nkey = [key]\nelse:\nreturn_singleton = False\n# retrieve data\ndoc_sequence = self._get_items(key)\n# check data\nif len(doc_sequence) == 0:\nraise KeyError(f'No document with id {key} found')\n# retrieve nested data\nfor field_name, type_, _ in self._flatten_schema(\ncast(Type[BaseDoc], self._schema)\n):\nif safe_issubclass(type_, AnyDocArray) and isinstance(\ndoc_sequence[0], Dict\n):\nfor doc in doc_sequence:\nself._get_subindex_doclist(doc, field_name)  # type: ignore\n# cast output\nif isinstance(doc_sequence, DocList):\nout_docs: DocList[TSchema] = doc_sequence\nelif isinstance(doc_sequence[0], Dict):\nout_docs = self._dict_list_to_docarray(doc_sequence)  # type: ignore\nelse:\ndocs_cls = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nout_docs = docs_cls(doc_sequence)\nreturn out_docs[0] if return_singleton else out_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.__init__","title":"<code>__init__(db_config=None, **kwargs)</code>","text":"<p>Initialize WeaviateDocumentIndex</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def __init__(self, db_config=None, **kwargs) -&gt; None:\n\"\"\"Initialize WeaviateDocumentIndex\"\"\"\nself.embedding_column: Optional[str] = None\nself.properties: Optional[List[str]] = None\n# keep track of the column name that contains the bytes\n# type because we will store them as a base64 encoded string\n# in weaviate\nself.bytes_columns: List[str] = []\n# keep track of the array columns that are not embeddings because we will\n# convert them to python lists before uploading to weaviate\nself.nonembedding_array_columns: List[str] = []\nsuper().__init__(db_config=db_config, **kwargs)\nself._db_config: WeaviateDocumentIndex.DBConfig = cast(\nWeaviateDocumentIndex.DBConfig, self._db_config\n)\nself._runtime_config: WeaviateDocumentIndex.RuntimeConfig = cast(\nWeaviateDocumentIndex.RuntimeConfig, self._runtime_config\n)\nif self._db_config.embedded_options:\nself._client = weaviate.Client(\nembedded_options=self._db_config.embedded_options\n)\nelse:\nself._client = weaviate.Client(\nself._db_config.host, auth_client_secret=self._build_auth_credentials()\n)\nself._configure_client()\nself._validate_columns()\nself._set_embedding_column()\nself._set_properties()\nself._create_schema()\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.build_query","title":"<code>build_query()</code>","text":"<p>Build a query for WeaviateDocumentIndex.</p> <p>Returns:</p> Type Description <code>QueryBuilder</code> <p>QueryBuilder object</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def build_query(self) -&gt; BaseDocIndex.QueryBuilder:\n\"\"\"\n    Build a query for WeaviateDocumentIndex.\n    :return: QueryBuilder object\n    \"\"\"\nreturn self.QueryBuilder(self)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.configure","title":"<code>configure(runtime_config=None, **kwargs)</code>","text":"<p>Configure the WeaviateDocumentIndex. You can either pass a config object to <code>config</code> or pass individual config parameters as keyword arguments. If a configuration object is passed, it will replace the current configuration. If keyword arguments are passed, they will update the current configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_config</code> <p>the configuration to apply</p> <code>None</code> <code>kwargs</code> <p>individual configuration parameters</p> <code>{}</code> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def configure(self, runtime_config=None, **kwargs) -&gt; None:\n\"\"\"\n    Configure the WeaviateDocumentIndex.\n    You can either pass a config object to `config` or pass individual config\n    parameters as keyword arguments.\n    If a configuration object is passed, it will replace the current configuration.\n    If keyword arguments are passed, they will update the current configuration.\n    :param runtime_config: the configuration to apply\n    :param kwargs: individual configuration parameters\n    \"\"\"\nsuper().configure(runtime_config, **kwargs)\nself._configure_client()\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.execute_query","title":"<code>execute_query(query, *args, **kwargs)</code>","text":"<p>Execute a query on the WeaviateDocumentIndex.</p> <p>Can take two kinds of inputs:</p> <ol> <li>A native query of the underlying database. This is meant as a passthrough so that you can enjoy any functionality that is not available through the Document index API.</li> <li>The output of this Document index' <code>QueryBuilder.build()</code> method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Any</code> <p>the query to execute</p> required <code>args</code> <p>positional arguments to pass to the query</p> <code>()</code> <code>kwargs</code> <p>keyword arguments to pass to the query</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the result of the query</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def execute_query(self, query: Any, *args, **kwargs) -&gt; Any:\n\"\"\"\n    Execute a query on the WeaviateDocumentIndex.\n    Can take two kinds of inputs:\n    1. A native query of the underlying database. This is meant as a passthrough so that you\n    can enjoy any functionality that is not available through the Document index API.\n    2. The output of this Document index' `QueryBuilder.build()` method.\n    :param query: the query to execute\n    :param args: positional arguments to pass to the query\n    :param kwargs: keyword arguments to pass to the query\n    :return: the result of the query\n    \"\"\"\nda_class = DocList.__class_getitem__(cast(Type[BaseDoc], self._schema))\nif isinstance(query, self.QueryBuilder):\nbatched_results = self._client.query.multi_get(query._queries).do()\nbatched_docs = batched_results[\"data\"][\"Get\"].values()\ndef f(doc):\n# TODO: use\n# return self._schema(**self._parse_weaviate_result(doc))\n# when https://github.com/weaviate/weaviate/issues/2858\n# is fixed\nreturn self._schema.from_view(self._parse_weaviate_result(doc))  # type: ignore\nresults = [\nda_class([f(doc) for doc in batched_doc])\nfor batched_doc in batched_docs\n]\nreturn results if len(results) &gt; 1 else results[0]\n# TODO: validate graphql query string before sending it to weaviate\nif isinstance(query, str):\nreturn self._client.query.raw(query)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.filter","title":"<code>filter(filter_query, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter(\nself,\nfilter_query: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in the index based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(f'Executing `filter` for the query {filter_query}')\ndocs = self._filter(filter_query, limit=limit, **kwargs)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.filter_batched","title":"<code>filter_batched(filter_queries, limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on multiple filter queries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_queries</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[DocList]</code> <p>a DocList containing the documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_batched(\nself,\nfilter_queries: Any,\nlimit: int = 10,\n**kwargs,\n) -&gt; List[DocList]:\n\"\"\"Find documents in the index based on multiple filter queries.\n    :param filter_queries: the DB specific filter query to execute\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter_batched` for the queries {filter_queries}'\n)\nda_list = self._filter_batched(filter_queries, limit=limit, **kwargs)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn da_list  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.filter_subindex","title":"<code>filter_subindex(filter_query, subindex, limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level based on a filter query</p> <p>Parameters:</p> Name Type Description Default <code>filter_query</code> <code>Any</code> <p>the DB specific filter query to execute</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> required <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a DocList containing the subindex level documents that match the filter query</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def filter_subindex(\nself,\nfilter_query: Any,\nsubindex: str,\nlimit: int = 10,\n**kwargs,\n) -&gt; DocList:\n\"\"\"Find documents in subindex level based on a filter query\n    :param filter_query: the DB specific filter query to execute\n    :param subindex: name of the subindex to search on\n    :param limit: maximum number of documents to return\n    :return: a DocList containing the subindex level documents that match the filter query\n    \"\"\"\nself._logger.debug(\nf'Executing `filter` for the query {filter_query} in subindex {subindex}'\n)\nif '__' in subindex:\nfields = subindex.split('__')\nreturn self._subindices[fields[0]].filter_subindex(\nfilter_query, '__'.join(fields[1:]), limit=limit, **kwargs\n)\nelse:\nreturn self._subindices[subindex].filter(\nfilter_query, limit=limit, **kwargs\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.find","title":"<code>find(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find k-nearest neighbors of the query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Has single axis.</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def find(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n):\n\"\"\"\n    Find k-nearest neighbors of the query.\n    :param query: query vector for KNN/ANN search. Has single axis.\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug('Executing `find`')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for WeaviateDocumentIndex.\\nSet search_field to an empty string to proceed.'\n)\nembedding_field = self._get_embedding_field()\nif isinstance(query, BaseDoc):\nquery_vec = self._get_values_by_column([query], embedding_field)[0]\nelse:\nquery_vec = query\nquery_vec_np = self._to_numpy(query_vec)\ndocs, scores = self._find(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.find_batched","title":"<code>find_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index using nearest neighbor search.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[AnyTensor, DocList]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a, or a DocList. If a tensor-like is passed, it should have shape (batch_size, vector_dim)</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on. Documents in the index are retrieved based on this similarity of this field to the query.</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return per query</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def find_batched(\nself,\nqueries: Union[AnyTensor, DocList],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs: Any,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index using nearest neighbor search.\n    :param queries: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.) with a,\n        or a DocList.\n        If a tensor-like is passed, it should have shape (batch_size, vector_dim)\n    :param search_field: name of the field to search on.\n        Documents in the index are retrieved based on this similarity\n        of this field to the query.\n    :param limit: maximum number of documents to return per query\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug('Executing `find_batched`')\nif search_field != '':\nraise ValueError(\n'Argument search_field is not supported for WeaviateDocumentIndex.\\nSet search_field to an empty string to proceed.'\n)\nembedding_field = self._get_embedding_field()\nif isinstance(queries, Sequence):\nquery_vec_list = self._get_values_by_column(queries, embedding_field)\nquery_vec_np = np.stack(\ntuple(self._to_numpy(query_vec) for query_vec in query_vec_list)\n)\nelse:\nquery_vec_np = self._to_numpy(queries)\nda_list, scores = self._find_batched(\nquery_vec_np, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\nda_list = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=da_list, scores=scores)  # type: ignore\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.find_subindex","title":"<code>find_subindex(query, subindex='', search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in subindex level.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>query vector for KNN/ANN search. Can be either a tensor-like (np.array, torch.Tensor, etc.) with a single axis, or a Document</p> required <code>subindex</code> <code>str</code> <p>name of the subindex to search on</p> <code>''</code> <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>SubindexFindResult</code> <p>a named tuple containing root docs, subindex docs and scores</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def find_subindex(\nself,\nquery: Union[AnyTensor, BaseDoc],\nsubindex: str = '',\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; SubindexFindResult:\n\"\"\"Find documents in subindex level.\n    :param query: query vector for KNN/ANN search.\n        Can be either a tensor-like (np.array, torch.Tensor, etc.)\n        with a single axis, or a Document\n    :param subindex: name of the subindex to search on\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing root docs, subindex docs and scores\n    \"\"\"\nself._logger.debug(f'Executing `find_subindex` for search field {search_field}')\nsub_docs, scores = self._find_subdocs(\nquery, subindex=subindex, search_field=search_field, limit=limit, **kwargs\n)\nfields = subindex.split('__')\nroot_ids = [\nself._get_root_doc_id(doc.id, fields[0], '__'.join(fields[1:]))\nfor doc in sub_docs\n]\nroot_docs = DocList[self._schema]()  # type: ignore\nfor id in root_ids:\nroot_docs.append(self[id])\nreturn SubindexFindResult(\nroot_documents=root_docs, sub_documents=sub_docs, scores=scores  # type: ignore\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.index","title":"<code>index(docs, **kwargs)</code>","text":"<p>index Documents into the index.</p> <p>Note</p> <p>Passing a sequence of Documents that is not a DocList (such as a List of Docs) comes at a performance penalty. This is because the Index needs to check compatibility between itself and the data. With a DocList as input this is a single check; for other inputs compatibility needs to be checked for every Document individually.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[BaseDoc, Sequence[BaseDoc]]</code> <p>Documents to index.</p> required Source code in <code>docarray/index/abstract.py</code> <pre><code>def index(self, docs: Union[BaseDoc, Sequence[BaseDoc]], **kwargs):\n\"\"\"index Documents into the index.\n    !!! note\n        Passing a sequence of Documents that is not a DocList\n        (such as a List of Docs) comes at a performance penalty.\n        This is because the Index needs to check compatibility between itself and\n        the data. With a DocList as input this is a single check; for other inputs\n        compatibility needs to be checked for every Document individually.\n    :param docs: Documents to index.\n    \"\"\"\nn_docs = 1 if isinstance(docs, BaseDoc) else len(docs)\nself._logger.debug(f'Indexing {n_docs} documents')\ndocs_validated = self._validate_docs(docs)\nself._update_subindex_data(docs_validated)\ndata_by_columns = self._get_col_value_dict(docs_validated)\nself._index(data_by_columns, **kwargs)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.num_docs","title":"<code>num_docs()</code>","text":"<p>Get the number of documents.</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def num_docs(self) -&gt; int:\n\"\"\"\n    Get the number of documents.\n    \"\"\"\nindex_name = self.index_name\nresult = self._client.query.aggregate(index_name).with_meta_count().do()\n# TODO: decorator to check for errors\ntotal_docs = result[\"data\"][\"Aggregate\"][index_name][0][\"meta\"][\"count\"]\nreturn total_docs\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.python_type_to_db_type","title":"<code>python_type_to_db_type(python_type)</code>","text":"<p>Map python type to database type. Takes any python type and returns the corresponding database column type.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type</code> <p>a python type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>the corresponding database column type, or None if <code>python_type</code> is not supported.</p> Source code in <code>docarray/index/backends/weaviate.py</code> <pre><code>def python_type_to_db_type(self, python_type: Type) -&gt; Any:\n\"\"\"Map python type to database type.\n    Takes any python type and returns the corresponding database column type.\n    :param python_type: a python type.\n    :return: the corresponding database column type,\n        or None if ``python_type`` is not supported.\n    \"\"\"\nfor allowed_type in WEAVIATE_PY_VEC_TYPES:\nif safe_issubclass(python_type, allowed_type):\nreturn 'number[]'\npy_weaviate_type_map = {\ndocarray.typing.ID: 'string',\nstr: 'text',\nint: 'int',\nfloat: 'number',\nbool: 'boolean',\nnp.ndarray: 'number[]',\nbytes: 'blob',\n}\nfor py_type, weaviate_type in py_weaviate_type_map.items():\nif safe_issubclass(python_type, py_type):\nreturn weaviate_type\nraise ValueError(f'Unsupported column type for {type(self)}: {python_type}')\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.subindex_contains","title":"<code>subindex_contains(item)</code>","text":"<p>Checks if a given BaseDoc item is contained in the index or any of its subindices.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>BaseDoc</code> <p>the given BaseDoc</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if the given BaseDoc item is contained in the index/subindices</p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def subindex_contains(self, item: BaseDoc) -&gt; bool:\n\"\"\"Checks if a given BaseDoc item is contained in the index or any of its subindices.\n    :param item: the given BaseDoc\n    :return: if the given BaseDoc item is contained in the index/subindices\n    \"\"\"\nif self._is_index_empty:\nreturn False\nif safe_issubclass(type(item), BaseDoc):\nreturn self.__contains__(item) or any(\nindex.subindex_contains(item) for index in self._subindices.values()\n)\nelse:\nraise TypeError(\nf\"item must be an instance of BaseDoc or its subclass, not '{type(item).__name__}'\"\n)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.text_search","title":"<code>text_search(query, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, BaseDoc]</code> <p>The text to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search(\nself,\nquery: Union[str, BaseDoc],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResult:\n\"\"\"Find documents in the index based on a text search query.\n    :param query: The text to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(f'Executing `text_search` for search field {search_field}')\nself._validate_search_field(search_field)\nif isinstance(query, BaseDoc):\nquery_text = self._get_values_by_column([query], search_field)[0]\nelse:\nquery_text = query\ndocs, scores = self._text_search(\nquery_text, search_field=search_field, limit=limit, **kwargs\n)\nif isinstance(docs, List) and not isinstance(docs, DocList):\ndocs = self._dict_list_to_docarray(docs)\nreturn FindResult(documents=docs, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_index/backends/weaviate/#docarray.index.backends.weaviate.WeaviateDocumentIndex.text_search_batched","title":"<code>text_search_batched(queries, search_field='', limit=10, **kwargs)</code>","text":"<p>Find documents in the index based on a text search query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[Sequence[str], Sequence[BaseDoc]]</code> <p>The texts to search for</p> required <code>search_field</code> <code>str</code> <p>name of the field to search on</p> <code>''</code> <code>limit</code> <code>int</code> <p>maximum number of documents to return</p> <code>10</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>a named tuple containing <code>documents</code> and <code>scores</code></p> Source code in <code>docarray/index/abstract.py</code> <pre><code>def text_search_batched(\nself,\nqueries: Union[Sequence[str], Sequence[BaseDoc]],\nsearch_field: str = '',\nlimit: int = 10,\n**kwargs,\n) -&gt; FindResultBatched:\n\"\"\"Find documents in the index based on a text search query.\n    :param queries: The texts to search for\n    :param search_field: name of the field to search on\n    :param limit: maximum number of documents to return\n    :return: a named tuple containing `documents` and `scores`\n    \"\"\"\nself._logger.debug(\nf'Executing `text_search_batched` for search field {search_field}'\n)\nself._validate_search_field(search_field)\nif isinstance(queries[0], BaseDoc):\nquery_docs: Sequence[BaseDoc] = cast(Sequence[BaseDoc], queries)\nquery_texts: Sequence[str] = self._get_values_by_column(\nquery_docs, search_field\n)\nelse:\nquery_texts = cast(Sequence[str], queries)\nda_list, scores = self._text_search_batched(\nquery_texts, search_field=search_field, limit=limit, **kwargs\n)\nif len(da_list) &gt; 0 and isinstance(da_list[0], List):\ndocs = [self._dict_list_to_docarray(docs) for docs in da_list]\nreturn FindResultBatched(documents=docs, scores=scores)\nda_list_ = cast(List[DocList], da_list)\nreturn FindResultBatched(documents=da_list_, scores=scores)\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/","title":"DocStore","text":""},{"location":"API_reference/doc_store/doc_store/#docstore","title":"DocStore","text":""},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore","title":"<code>docarray.store.abstract_doc_store.AbstractDocStore</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>class AbstractDocStore(ABC):\n@staticmethod\n@abstractmethod\ndef list(namespace: str, show_table: bool) -&gt; List[str]:\n\"\"\"List all DocLists in the specified backend at the namespace.\n        :param namespace: The namespace to list\n        :param show_table: If true, a table is printed to the console\n        :return: A list of DocList names\n        \"\"\"\n...\n@staticmethod\n@abstractmethod\ndef delete(name: str, missing_ok: bool) -&gt; bool:\n\"\"\"Delete the DocList object at the specified name\n        :param name: The name of the DocList to delete\n        :param missing_ok: If true, no error will be raised if the DocList does not exist.\n        :return: True if the DocList was deleted, False if it did not exist.\n        \"\"\"\n...\n@staticmethod\n@abstractmethod\ndef push(\ndocs: 'DocList',\nname: str,\nshow_progress: bool,\n) -&gt; Dict:\n\"\"\"Push this DocList to the specified name.\n        :param docs: The DocList to push\n        :param name: The name to push to\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\n...\n@staticmethod\n@abstractmethod\ndef push_stream(\ndocs: Iterator['BaseDoc'],\nurl: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified name.\n        :param docs: a stream of documents\n        :param url: The name to push to\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\n...\n@staticmethod\n@abstractmethod\ndef pull(\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; 'DocList':\n\"\"\"Pull a DocList from the specified name.\n        :param docs_cls: The DocList class to instantiate\n        :param name: The name to pull from\n        :param show_progress: If true, a progress bar will be displayed.\n        :param local_cache: If true, the DocList will be cached locally\n        :return: A DocList\n        \"\"\"\n...\n@staticmethod\n@abstractmethod\ndef pull_stream(\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of documents from the specified name.\n        :param docs_cls: The DocList class to instantiate\n        :param name: The name to pull from\n        :param show_progress: If true, a progress bar will be displayed.\n        :param local_cache: If true, the DocList will be cached locally\n        :return: An iterator of documents\"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.delete","title":"<code>delete(name, missing_ok)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Delete the DocList object at the specified name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the DocList to delete</p> required <code>missing_ok</code> <code>bool</code> <p>If true, no error will be raised if the DocList does not exist.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the DocList was deleted, False if it did not exist.</p> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef delete(name: str, missing_ok: bool) -&gt; bool:\n\"\"\"Delete the DocList object at the specified name\n    :param name: The name of the DocList to delete\n    :param missing_ok: If true, no error will be raised if the DocList does not exist.\n    :return: True if the DocList was deleted, False if it did not exist.\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.list","title":"<code>list(namespace, show_table)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>List all DocLists in the specified backend at the namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>The namespace to list</p> required <code>show_table</code> <code>bool</code> <p>If true, a table is printed to the console</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of DocList names</p> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef list(namespace: str, show_table: bool) -&gt; List[str]:\n\"\"\"List all DocLists in the specified backend at the namespace.\n    :param namespace: The namespace to list\n    :param show_table: If true, a table is printed to the console\n    :return: A list of DocList names\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.pull","title":"<code>pull(docs_cls, name, show_progress, local_cache)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Pull a DocList from the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>docs_cls</code> <code>Type[DocList]</code> <p>The DocList class to instantiate</p> required <code>name</code> <code>str</code> <p>The name to pull from</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> required <code>local_cache</code> <code>bool</code> <p>If true, the DocList will be cached locally</p> required <p>Returns:</p> Type Description <code>DocList</code> <p>A DocList</p> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef pull(\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; 'DocList':\n\"\"\"Pull a DocList from the specified name.\n    :param docs_cls: The DocList class to instantiate\n    :param name: The name to pull from\n    :param show_progress: If true, a progress bar will be displayed.\n    :param local_cache: If true, the DocList will be cached locally\n    :return: A DocList\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.pull_stream","title":"<code>pull_stream(docs_cls, name, show_progress, local_cache)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Pull a stream of documents from the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>docs_cls</code> <code>Type[DocList]</code> <p>The DocList class to instantiate</p> required <code>name</code> <code>str</code> <p>The name to pull from</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> required <code>local_cache</code> <code>bool</code> <p>If true, the DocList will be cached locally</p> required <p>Returns:</p> Type Description <code>Iterator[BaseDoc]</code> <p>An iterator of documents</p> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef pull_stream(\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of documents from the specified name.\n    :param docs_cls: The DocList class to instantiate\n    :param name: The name to pull from\n    :param show_progress: If true, a progress bar will be displayed.\n    :param local_cache: If true, the DocList will be cached locally\n    :return: An iterator of documents\"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.push","title":"<code>push(docs, name, show_progress)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Push this DocList to the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>DocList</code> <p>The DocList to push</p> required <code>name</code> <code>str</code> <p>The name to push to</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> required Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef push(\ndocs: 'DocList',\nname: str,\nshow_progress: bool,\n) -&gt; Dict:\n\"\"\"Push this DocList to the specified name.\n    :param docs: The DocList to push\n    :param name: The name to push to\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/doc_store/#docarray.store.abstract_doc_store.AbstractDocStore.push_stream","title":"<code>push_stream(docs, url, show_progress=False)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Push a stream of documents to the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[BaseDoc]</code> <p>a stream of documents</p> required <code>url</code> <code>str</code> <p>The name to push to</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/store/abstract_doc_store.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef push_stream(\ndocs: Iterator['BaseDoc'],\nurl: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified name.\n    :param docs: a stream of documents\n    :param url: The name to push to\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/","title":"FileDocStore","text":""},{"location":"API_reference/doc_store/file_doc_store/#filedocstore","title":"FileDocStore","text":""},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore","title":"<code>docarray.store.file.FileDocStore</code>","text":"<p>             Bases: <code>AbstractDocStore</code></p> <p>Class to push and pull <code>DocList</code> on-disk.</p> Source code in <code>docarray/store/file.py</code> <pre><code>class FileDocStore(AbstractDocStore):\n\"\"\"Class to push and pull [`DocList`][docarray.DocList] on-disk.\"\"\"\n@staticmethod\ndef _abs_filepath(name: str) -&gt; Path:\n\"\"\"Resolve a name to an absolute path.\n        :param name: If it is not a path, the cache directory is prepended.\n            If it is a path, it is resolved to an absolute path.\n        :return: Path\n        \"\"\"\nif not (name.startswith('/') or name.startswith('~') or name.startswith('.')):\nname = str(_get_cache_path() / name)\nif name.startswith('~'):\nname = str(Path.home() / name[2:])\nreturn Path(name).resolve()\n@classmethod\ndef list(\ncls: Type[SelfFileDocStore], namespace: str, show_table: bool\n) -&gt; List[str]:\n\"\"\"List all [`DocList`s][docarray.DocList] in a directory.\n        :param namespace: The directory to list.\n        :param show_table: If True, print a table of the files in the directory.\n        :return: A list of the names of the `DocLists` in the directory.\n        \"\"\"\nnamespace_dir = cls._abs_filepath(namespace)\nif not namespace_dir.exists():\nraise FileNotFoundError(f'Directory {namespace} does not exist')\nda_files = [dafile for dafile in namespace_dir.glob('*.docs')]\nif show_table:\nfrom datetime import datetime\nfrom rich import box, filesize\nfrom rich.console import Console\nfrom rich.table import Table\ntable = Table(\ntitle=f'You have {len(da_files)} DocLists in file://{namespace_dir}',\nbox=box.SIMPLE,\nhighlight=True,\n)\ntable.add_column('Name')\ntable.add_column('Last Modified', justify='center')\ntable.add_column('Size')\nfor da_file in da_files:\ntable.add_row(\nda_file.stem,\nstr(datetime.fromtimestamp(int(da_file.stat().st_ctime))),\nstr(filesize.decimal(da_file.stat().st_size)),\n)\nConsole().print(table)\nreturn [dafile.stem for dafile in da_files]\n@classmethod\ndef delete(\ncls: Type[SelfFileDocStore], name: str, missing_ok: bool = False\n) -&gt; bool:\n\"\"\"Delete a [`DocList`][docarray.DocList] from the local filesystem.\n        :param name: The name of the `DocList` to delete.\n        :param missing_ok: If True, do not raise an exception if the file does not exist. Defaults to False.\n        :return: True if the file was deleted, False if it did not exist.\n        \"\"\"\npath = cls._abs_filepath(name)\ntry:\npath.with_suffix('.docs').unlink()\nreturn True\nexcept FileNotFoundError:\nif not missing_ok:\nraise\nreturn False\n@classmethod\ndef push(\ncls: Type[SelfFileDocStore],\ndocs: 'DocList',\nname: str,\nshow_progress: bool,\n) -&gt; Dict:\n\"\"\"Push this [`DocList`][docarray.DocList] object to the specified file path.\n        :param docs: The `DocList` to push.\n        :param name: The file path to push to.\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nreturn cls.push_stream(iter(docs), name, show_progress)\n@classmethod\ndef push_stream(\ncls: Type[SelfFileDocStore],\ndocs: Iterator['BaseDoc'],\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified file path.\n        :param docs: a stream of documents\n        :param name: The file path to push to.\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nsource = _to_binary_stream(\ndocs, protocol='protobuf', compress='gzip', show_progress=show_progress\n)\npath = cls._abs_filepath(name).with_suffix('.docs.tmp')\nif path.exists():\nraise ConcurrentPushException(f'File {path} already exists.')\nwith open(path, 'wb') as f:\nwhile True:\ntry:\nf.write(next(source))\nexcept StopIteration:\nbreak\npath.rename(path.with_suffix(''))\nreturn {}\n@classmethod\ndef pull(\ncls: Type[SelfFileDocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; 'DocList':\n\"\"\"Pull a [`DocList`][docarray.DocList] from the specified url.\n        :param name: The file path to pull from.\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: store the downloaded `DocList` to local folder\n        :return: a `DocList` object\n        \"\"\"\nreturn docs_cls(\ncls.pull_stream(\ndocs_cls, name, show_progress=show_progress, local_cache=local_cache\n)\n)\n@classmethod\ndef pull_stream(\ncls: Type[SelfFileDocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified file.\n        :param name: The file path to pull from.\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: Not used by the ``file`` protocol.\n        :return: Iterator of Documents\n        \"\"\"\nif local_cache:\nlogging.warning('local_cache is not supported for \"file\" protocol')\npath = cls._abs_filepath(name).with_suffix('.docs')\nsource = open(path, 'rb')\nreturn _from_binary_stream(\ndocs_cls.doc_type,\nsource,\nprotocol='protobuf',\ncompress='gzip',\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.delete","title":"<code>delete(name, missing_ok=False)</code>  <code>classmethod</code>","text":"<p>Delete a <code>DocList</code> from the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the <code>DocList</code> to delete.</p> required <code>missing_ok</code> <code>bool</code> <p>If True, do not raise an exception if the file does not exist. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was deleted, False if it did not exist.</p> Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef delete(\ncls: Type[SelfFileDocStore], name: str, missing_ok: bool = False\n) -&gt; bool:\n\"\"\"Delete a [`DocList`][docarray.DocList] from the local filesystem.\n    :param name: The name of the `DocList` to delete.\n    :param missing_ok: If True, do not raise an exception if the file does not exist. Defaults to False.\n    :return: True if the file was deleted, False if it did not exist.\n    \"\"\"\npath = cls._abs_filepath(name)\ntry:\npath.with_suffix('.docs').unlink()\nreturn True\nexcept FileNotFoundError:\nif not missing_ok:\nraise\nreturn False\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.list","title":"<code>list(namespace, show_table)</code>  <code>classmethod</code>","text":"<p>List all <code>DocList</code>s in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>The directory to list.</p> required <code>show_table</code> <code>bool</code> <p>If True, print a table of the files in the directory.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of the names of the <code>DocLists</code> in the directory.</p> Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef list(\ncls: Type[SelfFileDocStore], namespace: str, show_table: bool\n) -&gt; List[str]:\n\"\"\"List all [`DocList`s][docarray.DocList] in a directory.\n    :param namespace: The directory to list.\n    :param show_table: If True, print a table of the files in the directory.\n    :return: A list of the names of the `DocLists` in the directory.\n    \"\"\"\nnamespace_dir = cls._abs_filepath(namespace)\nif not namespace_dir.exists():\nraise FileNotFoundError(f'Directory {namespace} does not exist')\nda_files = [dafile for dafile in namespace_dir.glob('*.docs')]\nif show_table:\nfrom datetime import datetime\nfrom rich import box, filesize\nfrom rich.console import Console\nfrom rich.table import Table\ntable = Table(\ntitle=f'You have {len(da_files)} DocLists in file://{namespace_dir}',\nbox=box.SIMPLE,\nhighlight=True,\n)\ntable.add_column('Name')\ntable.add_column('Last Modified', justify='center')\ntable.add_column('Size')\nfor da_file in da_files:\ntable.add_row(\nda_file.stem,\nstr(datetime.fromtimestamp(int(da_file.stat().st_ctime))),\nstr(filesize.decimal(da_file.stat().st_size)),\n)\nConsole().print(table)\nreturn [dafile.stem for dafile in da_files]\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.pull","title":"<code>pull(docs_cls, name, show_progress, local_cache)</code>  <code>classmethod</code>","text":"<p>Pull a <code>DocList</code> from the specified url.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The file path to pull from.</p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> required <code>local_cache</code> <code>bool</code> <p>store the downloaded <code>DocList</code> to local folder</p> required <p>Returns:</p> Type Description <code>DocList</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef pull(\ncls: Type[SelfFileDocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; 'DocList':\n\"\"\"Pull a [`DocList`][docarray.DocList] from the specified url.\n    :param name: The file path to pull from.\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded `DocList` to local folder\n    :return: a `DocList` object\n    \"\"\"\nreturn docs_cls(\ncls.pull_stream(\ndocs_cls, name, show_progress=show_progress, local_cache=local_cache\n)\n)\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.pull_stream","title":"<code>pull_stream(docs_cls, name, show_progress, local_cache)</code>  <code>classmethod</code>","text":"<p>Pull a stream of Documents from the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The file path to pull from.</p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> required <code>local_cache</code> <code>bool</code> <p>Not used by the <code>file</code> protocol.</p> required <p>Returns:</p> Type Description <code>Iterator[BaseDoc]</code> <p>Iterator of Documents</p> Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef pull_stream(\ncls: Type[SelfFileDocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified file.\n    :param name: The file path to pull from.\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: Not used by the ``file`` protocol.\n    :return: Iterator of Documents\n    \"\"\"\nif local_cache:\nlogging.warning('local_cache is not supported for \"file\" protocol')\npath = cls._abs_filepath(name).with_suffix('.docs')\nsource = open(path, 'rb')\nreturn _from_binary_stream(\ndocs_cls.doc_type,\nsource,\nprotocol='protobuf',\ncompress='gzip',\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.push","title":"<code>push(docs, name, show_progress)</code>  <code>classmethod</code>","text":"<p>Push this <code>DocList</code> object to the specified file path.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>DocList</code> <p>The <code>DocList</code> to push.</p> required <code>name</code> <code>str</code> <p>The file path to push to.</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> required Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef push(\ncls: Type[SelfFileDocStore],\ndocs: 'DocList',\nname: str,\nshow_progress: bool,\n) -&gt; Dict:\n\"\"\"Push this [`DocList`][docarray.DocList] object to the specified file path.\n    :param docs: The `DocList` to push.\n    :param name: The file path to push to.\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nreturn cls.push_stream(iter(docs), name, show_progress)\n</code></pre>"},{"location":"API_reference/doc_store/file_doc_store/#docarray.store.file.FileDocStore.push_stream","title":"<code>push_stream(docs, name, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Push a stream of documents to the specified file path.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[BaseDoc]</code> <p>a stream of documents</p> required <code>name</code> <code>str</code> <p>The file path to push to.</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/store/file.py</code> <pre><code>@classmethod\ndef push_stream(\ncls: Type[SelfFileDocStore],\ndocs: Iterator['BaseDoc'],\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified file path.\n    :param docs: a stream of documents\n    :param name: The file path to push to.\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nsource = _to_binary_stream(\ndocs, protocol='protobuf', compress='gzip', show_progress=show_progress\n)\npath = cls._abs_filepath(name).with_suffix('.docs.tmp')\nif path.exists():\nraise ConcurrentPushException(f'File {path} already exists.')\nwith open(path, 'wb') as f:\nwhile True:\ntry:\nf.write(next(source))\nexcept StopIteration:\nbreak\npath.rename(path.with_suffix(''))\nreturn {}\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/","title":"S3DocStore","text":""},{"location":"API_reference/doc_store/s3_doc_store/#s3docstore","title":"S3DocStore","text":""},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore","title":"<code>docarray.store.s3.S3DocStore</code>","text":"<p>             Bases: <code>AbstractDocStore</code></p> <p>Class to push and pull <code>DocList</code> to and from S3.</p> Source code in <code>docarray/store/s3.py</code> <pre><code>class S3DocStore(AbstractDocStore):\n\"\"\"Class to push and pull [`DocList`][docarray.DocList] to and from S3.\"\"\"\n@staticmethod\ndef list(namespace: str, show_table: bool = False) -&gt; List[str]:\n\"\"\"List all [`DocList`s][docarray.DocList] in the specified bucket and namespace.\n        :param namespace: The bucket and namespace to list. e.g. my_bucket/my_namespace\n        :param show_table: If true, a rich table will be printed to the console.\n        :return: A list of `DocList` names.\n        \"\"\"\nbucket, namespace = namespace.split('/', 1)\ns3 = boto3.resource('s3')\ns3_bucket = s3.Bucket(bucket)\nda_files = [\nobj\nfor obj in s3_bucket.objects.all()\nif obj.key.startswith(namespace) and obj.key.endswith('.docs')\n]\nda_names = [f.key.split('/')[-1].split('.')[0] for f in da_files]\nif show_table:\nfrom rich import box, filesize\nfrom rich.console import Console\nfrom rich.table import Table\ntable = Table(\ntitle=f'You have {len(da_files)} DocLists in bucket s3://{bucket} under the namespace \"{namespace}\"',\nbox=box.SIMPLE,\nhighlight=True,\n)\ntable.add_column('Name')\ntable.add_column('Last Modified', justify='center')\ntable.add_column('Size')\nfor da_name, da_file in zip(da_names, da_files):\ntable.add_row(\nda_name,\nstr(da_file.last_modified),\nstr(filesize.decimal(da_file.size)),\n)\nConsole().print(table)\nreturn da_names\n@staticmethod\ndef delete(name: str, missing_ok: bool = True) -&gt; bool:\n\"\"\"Delete the [`DocList`][docarray.DocList] object at the specified bucket and key.\n        :param name: The bucket and key to delete. e.g. my_bucket/my_key\n        :param missing_ok: If true, no error will be raised if the object does not exist.\n        :return: True if the object was deleted, False if it did not exist.\n        \"\"\"\nbucket, name = name.split('/', 1)\ns3 = boto3.resource('s3')\nobject = s3.Object(bucket, name + '.docs')\ntry:\nobject.load()\nexcept botocore.exceptions.ClientError as e:\nif e.response['Error']['Code'] == \"404\":\nif missing_ok:\nreturn False\nelse:\nraise ValueError(f'Object {name} does not exist')\nelse:\nraise\nobject.delete()\nreturn True\n@classmethod\ndef push(\ncls: Type[SelfS3DocStore],\ndocs: 'DocList',\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push this [`DocList`][docarray.DocList] object to the specified bucket and key.\n        :param docs: The `DocList` to push.\n        :param name: The bucket and key to push to. e.g. my_bucket/my_key\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nreturn cls.push_stream(iter(docs), name, show_progress)\n@staticmethod\ndef push_stream(\ndocs: Iterator['BaseDoc'],\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified bucket and key.\n        :param docs: a stream of documents\n        :param name: The bucket and key to push to. e.g. my_bucket/my_key\n        :param show_progress: If true, a progress bar will be displayed.\n        \"\"\"\nbucket, name = name.split('/', 1)\nbinary_stream = _to_binary_stream(\ndocs, protocol='pickle', compress=None, show_progress=show_progress\n)\n# Upload to S3\nwith open(\nf\"s3://{bucket}/{name}.docs\",\n'wb',\ncompression='.gz',\ntransport_params={'multipart_upload': False},\n) as fout:\nwhile True:\ntry:\nfout.write(next(binary_stream))\nexcept StopIteration:\nbreak\nreturn {}\n@classmethod\ndef pull(\ncls: Type[SelfS3DocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool = False,\nlocal_cache: bool = False,\n) -&gt; 'DocList':\n\"\"\"Pull a [`DocList`][docarray.DocList] from the specified bucket and key.\n        :param name: The bucket and key to pull from. e.g. my_bucket/my_key\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: store the downloaded DocList to local cache\n        :return: a `DocList` object\n        \"\"\"\ndocs = docs_cls(  # type: ignore\ncls.pull_stream(\ndocs_cls, name, show_progress=show_progress, local_cache=local_cache\n)\n)\nreturn docs\n@classmethod\ndef pull_stream(\ncls: Type[SelfS3DocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified name.\n        Name is expected to be in the format of bucket/key.\n        :param name: The bucket and key to pull from. e.g. my_bucket/my_key\n        :param show_progress: if true, display a progress bar.\n        :param local_cache: store the downloaded DocList to local cache\n        :return: An iterator of Documents\n        \"\"\"\nbucket, name = name.split('/', 1)\nsave_name = name.replace('/', '_')\ncache_path = _get_cache_path() / f'{save_name}.docs'\nsource = _BufferedCachingReader(\nopen(f\"s3://{bucket}/{name}.docs\", 'rb', compression='.gz'),\ncache_path=cache_path if local_cache else None,\n)\nif local_cache:\nif cache_path.exists():\nobject_header = boto3.client('s3').head_object(\nBucket=bucket, Key=name + '.docs'\n)\nif cache_path.stat().st_size == object_header['ContentLength']:\nlogging.info(\nf'Using cached file for {name} (size: {cache_path.stat().st_size})'\n)\nsource = open(cache_path, 'rb')\nreturn _from_binary_stream(\ndocs_cls.doc_type,\nsource,\nprotocol='pickle',\ncompress=None,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.delete","title":"<code>delete(name, missing_ok=True)</code>  <code>staticmethod</code>","text":"<p>Delete the <code>DocList</code> object at the specified bucket and key.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The bucket and key to delete. e.g. my_bucket/my_key</p> required <code>missing_ok</code> <code>bool</code> <p>If true, no error will be raised if the object does not exist.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the object was deleted, False if it did not exist.</p> Source code in <code>docarray/store/s3.py</code> <pre><code>@staticmethod\ndef delete(name: str, missing_ok: bool = True) -&gt; bool:\n\"\"\"Delete the [`DocList`][docarray.DocList] object at the specified bucket and key.\n    :param name: The bucket and key to delete. e.g. my_bucket/my_key\n    :param missing_ok: If true, no error will be raised if the object does not exist.\n    :return: True if the object was deleted, False if it did not exist.\n    \"\"\"\nbucket, name = name.split('/', 1)\ns3 = boto3.resource('s3')\nobject = s3.Object(bucket, name + '.docs')\ntry:\nobject.load()\nexcept botocore.exceptions.ClientError as e:\nif e.response['Error']['Code'] == \"404\":\nif missing_ok:\nreturn False\nelse:\nraise ValueError(f'Object {name} does not exist')\nelse:\nraise\nobject.delete()\nreturn True\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.list","title":"<code>list(namespace, show_table=False)</code>  <code>staticmethod</code>","text":"<p>List all <code>DocList</code>s in the specified bucket and namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>The bucket and namespace to list. e.g. my_bucket/my_namespace</p> required <code>show_table</code> <code>bool</code> <p>If true, a rich table will be printed to the console.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of <code>DocList</code> names.</p> Source code in <code>docarray/store/s3.py</code> <pre><code>@staticmethod\ndef list(namespace: str, show_table: bool = False) -&gt; List[str]:\n\"\"\"List all [`DocList`s][docarray.DocList] in the specified bucket and namespace.\n    :param namespace: The bucket and namespace to list. e.g. my_bucket/my_namespace\n    :param show_table: If true, a rich table will be printed to the console.\n    :return: A list of `DocList` names.\n    \"\"\"\nbucket, namespace = namespace.split('/', 1)\ns3 = boto3.resource('s3')\ns3_bucket = s3.Bucket(bucket)\nda_files = [\nobj\nfor obj in s3_bucket.objects.all()\nif obj.key.startswith(namespace) and obj.key.endswith('.docs')\n]\nda_names = [f.key.split('/')[-1].split('.')[0] for f in da_files]\nif show_table:\nfrom rich import box, filesize\nfrom rich.console import Console\nfrom rich.table import Table\ntable = Table(\ntitle=f'You have {len(da_files)} DocLists in bucket s3://{bucket} under the namespace \"{namespace}\"',\nbox=box.SIMPLE,\nhighlight=True,\n)\ntable.add_column('Name')\ntable.add_column('Last Modified', justify='center')\ntable.add_column('Size')\nfor da_name, da_file in zip(da_names, da_files):\ntable.add_row(\nda_name,\nstr(da_file.last_modified),\nstr(filesize.decimal(da_file.size)),\n)\nConsole().print(table)\nreturn da_names\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.pull","title":"<code>pull(docs_cls, name, show_progress=False, local_cache=False)</code>  <code>classmethod</code>","text":"<p>Pull a <code>DocList</code> from the specified bucket and key.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The bucket and key to pull from. e.g. my_bucket/my_key</p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> <code>False</code> <code>local_cache</code> <code>bool</code> <p>store the downloaded DocList to local cache</p> <code>False</code> <p>Returns:</p> Type Description <code>DocList</code> <p>a <code>DocList</code> object</p> Source code in <code>docarray/store/s3.py</code> <pre><code>@classmethod\ndef pull(\ncls: Type[SelfS3DocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool = False,\nlocal_cache: bool = False,\n) -&gt; 'DocList':\n\"\"\"Pull a [`DocList`][docarray.DocList] from the specified bucket and key.\n    :param name: The bucket and key to pull from. e.g. my_bucket/my_key\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded DocList to local cache\n    :return: a `DocList` object\n    \"\"\"\ndocs = docs_cls(  # type: ignore\ncls.pull_stream(\ndocs_cls, name, show_progress=show_progress, local_cache=local_cache\n)\n)\nreturn docs\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.pull_stream","title":"<code>pull_stream(docs_cls, name, show_progress, local_cache)</code>  <code>classmethod</code>","text":"<p>Pull a stream of Documents from the specified name. Name is expected to be in the format of bucket/key.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The bucket and key to pull from. e.g. my_bucket/my_key</p> required <code>show_progress</code> <code>bool</code> <p>if true, display a progress bar.</p> required <code>local_cache</code> <code>bool</code> <p>store the downloaded DocList to local cache</p> required <p>Returns:</p> Type Description <code>Iterator[BaseDoc]</code> <p>An iterator of Documents</p> Source code in <code>docarray/store/s3.py</code> <pre><code>@classmethod\ndef pull_stream(\ncls: Type[SelfS3DocStore],\ndocs_cls: Type['DocList'],\nname: str,\nshow_progress: bool,\nlocal_cache: bool,\n) -&gt; Iterator['BaseDoc']:\n\"\"\"Pull a stream of Documents from the specified name.\n    Name is expected to be in the format of bucket/key.\n    :param name: The bucket and key to pull from. e.g. my_bucket/my_key\n    :param show_progress: if true, display a progress bar.\n    :param local_cache: store the downloaded DocList to local cache\n    :return: An iterator of Documents\n    \"\"\"\nbucket, name = name.split('/', 1)\nsave_name = name.replace('/', '_')\ncache_path = _get_cache_path() / f'{save_name}.docs'\nsource = _BufferedCachingReader(\nopen(f\"s3://{bucket}/{name}.docs\", 'rb', compression='.gz'),\ncache_path=cache_path if local_cache else None,\n)\nif local_cache:\nif cache_path.exists():\nobject_header = boto3.client('s3').head_object(\nBucket=bucket, Key=name + '.docs'\n)\nif cache_path.stat().st_size == object_header['ContentLength']:\nlogging.info(\nf'Using cached file for {name} (size: {cache_path.stat().st_size})'\n)\nsource = open(cache_path, 'rb')\nreturn _from_binary_stream(\ndocs_cls.doc_type,\nsource,\nprotocol='pickle',\ncompress=None,\nshow_progress=show_progress,\n)\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.push","title":"<code>push(docs, name, show_progress=False)</code>  <code>classmethod</code>","text":"<p>Push this <code>DocList</code> object to the specified bucket and key.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>DocList</code> <p>The <code>DocList</code> to push.</p> required <code>name</code> <code>str</code> <p>The bucket and key to push to. e.g. my_bucket/my_key</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/store/s3.py</code> <pre><code>@classmethod\ndef push(\ncls: Type[SelfS3DocStore],\ndocs: 'DocList',\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push this [`DocList`][docarray.DocList] object to the specified bucket and key.\n    :param docs: The `DocList` to push.\n    :param name: The bucket and key to push to. e.g. my_bucket/my_key\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nreturn cls.push_stream(iter(docs), name, show_progress)\n</code></pre>"},{"location":"API_reference/doc_store/s3_doc_store/#docarray.store.s3.S3DocStore.push_stream","title":"<code>push_stream(docs, name, show_progress=False)</code>  <code>staticmethod</code>","text":"<p>Push a stream of documents to the specified bucket and key.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[BaseDoc]</code> <p>a stream of documents</p> required <code>name</code> <code>str</code> <p>The bucket and key to push to. e.g. my_bucket/my_key</p> required <code>show_progress</code> <code>bool</code> <p>If true, a progress bar will be displayed.</p> <code>False</code> Source code in <code>docarray/store/s3.py</code> <pre><code>@staticmethod\ndef push_stream(\ndocs: Iterator['BaseDoc'],\nname: str,\nshow_progress: bool = False,\n) -&gt; Dict:\n\"\"\"Push a stream of documents to the specified bucket and key.\n    :param docs: a stream of documents\n    :param name: The bucket and key to push to. e.g. my_bucket/my_key\n    :param show_progress: If true, a progress bar will be displayed.\n    \"\"\"\nbucket, name = name.split('/', 1)\nbinary_stream = _to_binary_stream(\ndocs, protocol='pickle', compress=None, show_progress=show_progress\n)\n# Upload to S3\nwith open(\nf\"s3://{bucket}/{name}.docs\",\n'wb',\ncompression='.gz',\ntransport_params={'multipart_upload': False},\n) as fout:\nwhile True:\ntry:\nfout.write(next(binary_stream))\nexcept StopIteration:\nbreak\nreturn {}\n</code></pre>"},{"location":"API_reference/documents/documents/","title":"Documents","text":""},{"location":"API_reference/documents/documents/#documents","title":"Documents","text":""},{"location":"API_reference/documents/documents/#docarray.documents","title":"<code>docarray.documents</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc","title":"<code>AudioDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling audios.</p> <p>The Audio Document can contain:</p> <ul> <li>an <code>AudioUrl</code> (<code>AudioDoc.url</code>)</li> <li>an <code>AudioTensor</code> (<code>AudioDoc.tensor</code>)</li> <li>an <code>AnyEmbedding</code> (<code>AudioDoc.embedding</code>)</li> <li>an <code>AudioBytes</code> (<code>AudioDoc.bytes_</code>) object</li> <li>an integer representing the frame_rate (<code>AudioDoc.frame_rate</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import AudioDoc\n# use it directly\naudio = AudioDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n)\naudio.tensor, audio.frame_rate = audio.url.load()\n# model = MyEmbeddingModel()\n# audio.embedding = model(audio.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import AudioDoc, TextDoc\nfrom typing import Optional\n# extend it\nclass MyAudio(AudioDoc):\nname: Optional[TextDoc] = None\naudio = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n)\naudio.name = TextDoc(text='my first audio')\naudio.tensor, audio.frame_rate = audio.url.load()\n# model = MyEmbeddingModel()\n# audio.embedding = model(audio.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import AudioDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\naudio: AudioDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\naudio=AudioDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.url.load()\n# equivalent to\nmmdoc.audio.bytes_ = mmdoc.audio.url.load_bytes()\nmmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.bytes_.load()\n</code></pre> Source code in <code>docarray/documents/audio.py</code> <pre><code>class AudioDoc(BaseDoc):\n\"\"\"\n    Document for handling audios.\n    The Audio Document can contain:\n    - an [`AudioUrl`][docarray.typing.url.AudioUrl] (`AudioDoc.url`)\n    - an [`AudioTensor`](../../../api_references/typing/tensor/audio) (`AudioDoc.tensor`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`AudioDoc.embedding`)\n    - an [`AudioBytes`][docarray.typing.bytes.AudioBytes] (`AudioDoc.bytes_`) object\n    - an integer representing the frame_rate (`AudioDoc.frame_rate`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import AudioDoc\n    # use it directly\n    audio = AudioDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n    )\n    audio.tensor, audio.frame_rate = audio.url.load()\n    # model = MyEmbeddingModel()\n    # audio.embedding = model(audio.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import AudioDoc, TextDoc\n    from typing import Optional\n    # extend it\n    class MyAudio(AudioDoc):\n        name: Optional[TextDoc] = None\n    audio = MyAudio(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n    )\n    audio.name = TextDoc(text='my first audio')\n    audio.tensor, audio.frame_rate = audio.url.load()\n    # model = MyEmbeddingModel()\n    # audio.embedding = model(audio.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import AudioDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        audio: AudioDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        audio=AudioDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.url.load()\n    # equivalent to\n    mmdoc.audio.bytes_ = mmdoc.audio.url.load_bytes()\n    mmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.bytes_.load()\n    ```\n    \"\"\"\nurl: Optional[AudioUrl] = Field(\ndescription='The url to a (potentially remote) audio file that can be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true',\ndefault=None,\n)\ntensor: Optional[AudioTensor] = Field(\ndescription='Tensor object of the audio which can be specified to one of `AudioNdArray`, `AudioTorchTensor`, `AudioTensorFlowTensor`',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the audio.',\nexample=[0, 1, 0],\ndefault=None,\n)\nbytes_: Optional[AudioBytes] = Field(\ndescription='Bytes representation pf the audio',\ndefault=None,\n)\nframe_rate: Optional[int] = Field(\ndescription='An integer representing the frame rate of the audio.',\nexample=24,\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.AudioDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc","title":"<code>ImageDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling images.</p> <p>It can contain:</p> <ul> <li>an <code>ImageUrl</code> (<code>Image.url</code>)</li> <li>an <code>ImageTensor</code> (<code>Image.tensor</code>)</li> <li>an <code>AnyEmbedding</code> (<code>Image.embedding</code>)</li> <li>an <code>ImageBytes</code> object (<code>ImageDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import ImageDoc\n# use it directly\nimage = ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\nimage.tensor = image.url.load()\n# model = MyEmbeddingModel()\n# image.embedding = model(image.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import ImageDoc\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyImage(ImageDoc):\nsecond_embedding: Optional[AnyEmbedding] = None\nimage = MyImage(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\nimage.tensor = image.url.load()\n# model = MyEmbeddingModel()\n# image.embedding = model(image.tensor)\n# image.second_embedding = model(image.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nimage: ImageDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\nimage=ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.image.tensor = mmdoc.image.url.load()\n# or\nmmdoc.image.bytes_ = mmdoc.image.url.load_bytes()\nmmdoc.image.tensor = mmdoc.image.bytes_.load()\n</code></pre> Source code in <code>docarray/documents/image.py</code> <pre><code>class ImageDoc(BaseDoc):\n\"\"\"\n    Document for handling images.\n    It can contain:\n    - an [`ImageUrl`][docarray.typing.url.ImageUrl] (`Image.url`)\n    - an [`ImageTensor`](../../../api_references/typing/tensor/image) (`Image.tensor`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`Image.embedding`)\n    - an [`ImageBytes`][docarray.typing.bytes.ImageBytes] object (`ImageDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import ImageDoc\n    # use it directly\n    image = ImageDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n    )\n    image.tensor = image.url.load()\n    # model = MyEmbeddingModel()\n    # image.embedding = model(image.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import ImageDoc\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyImage(ImageDoc):\n        second_embedding: Optional[AnyEmbedding] = None\n    image = MyImage(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n    )\n    image.tensor = image.url.load()\n    # model = MyEmbeddingModel()\n    # image.embedding = model(image.tensor)\n    # image.second_embedding = model(image.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import ImageDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        image: ImageDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        image=ImageDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.image.tensor = mmdoc.image.url.load()\n    # or\n    mmdoc.image.bytes_ = mmdoc.image.url.load_bytes()\n    mmdoc.image.tensor = mmdoc.image.bytes_.load()\n    ```\n    \"\"\"\nurl: Optional[ImageUrl] = Field(\ndescription='URL to a (potentially remote) image file that needs to be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true',\ndefault=None,\n)\ntensor: Optional[ImageTensor] = Field(\ndescription='Tensor object of the image which can be specifed to one of `ImageNdArray`, `ImageTorchTensor`, `ImageTensorflowTensor`.',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the image.',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[ImageBytes] = Field(\ndescription='Bytes object of the image which is an instance of `ImageBytes`.',\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif (\nisinstance(value, (AbstractTensor, np.ndarray))\nor (torch is not None and isinstance(value, torch.Tensor))\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nelif isinstance(value, bytes):\nvalue = dict(byte=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.ImageDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D","title":"<code>Mesh3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling meshes for 3D data representation.</p> <p>A mesh is a representation for 3D data and contains vertices and faces information. Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3). Faces are triangular surfaces that can be defined by three points in 3D space, corresponding to the three vertices of a triangle. Faces can be represented as a tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a vertex in the tensor of vertices.</p> <p>The Mesh3D Document can contain:</p> <ul> <li>an <code>Mesh3DUrl</code> (<code>Mesh3D.url</code>)</li> <li> <p>a <code>VerticesAndFaces</code> object containing:</p> <ul> <li>an <code>AnyTensor</code> of vertices (<code>Mesh3D.tensors.vertices</code>)</li> <li>an <code>AnyTensor</code> of faces (<code>Mesh3D.tensors.faces</code>)</li> </ul> </li> <li> <p>an <code>AnyEmbedding</code> (<code>Mesh3D.embedding</code>)</p> </li> <li>a <code>bytes</code> object (<code>Mesh3D.bytes_</code>).</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import Mesh3D\n# use it directly\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.tensors.vertices)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import Mesh3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyMesh3D(Mesh3D):\nname: Optional[str] = None\nmesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.name = 'my first mesh'\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.vertices)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import Mesh3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nmesh: Mesh3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\nmesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.mesh.tensors = mmdoc.mesh.url.load()\n# or\nmmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n</code></pre> <p>You can display your 3D mesh in a notebook from either its url, or its tensors:</p> <pre><code>from docarray.documents import Mesh3D\n# display from url\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# mesh.url.display()\n# display from tensors\nmesh.tensors = mesh.url.load()\n# mesh.tensors.display()\n</code></pre> Source code in <code>docarray/documents/mesh/mesh_3d.py</code> <pre><code>class Mesh3D(BaseDoc):\n\"\"\"\n    Document for handling meshes for 3D data representation.\n    A mesh is a representation for 3D data and contains vertices and faces information.\n    Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3).\n    Faces are triangular surfaces that can be defined by three points in 3D space,\n    corresponding to the three vertices of a triangle. Faces can be represented as a\n    tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a\n    vertex in the tensor of vertices.\n    The Mesh3D Document can contain:\n    - an [`Mesh3DUrl`][docarray.typing.url.Mesh3DUrl] (`Mesh3D.url`)\n    - a [`VerticesAndFaces`][docarray.documents.mesh.vertices_and_faces.VerticesAndFaces]\n    object containing:\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of\n        vertices (`Mesh3D.tensors.vertices`)\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of faces (`Mesh3D.tensors.faces`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`Mesh3D.embedding`)\n    - a `bytes` object (`Mesh3D.bytes_`).\n    You can use this Document directly:\n    ```python\n    from docarray.documents import Mesh3D\n    # use it directly\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.tensors.vertices)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import Mesh3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyMesh3D(Mesh3D):\n        name: Optional[str] = None\n    mesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.name = 'my first mesh'\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.vertices)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import Mesh3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        mesh: Mesh3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        mesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.mesh.tensors = mmdoc.mesh.url.load()\n    # or\n    mmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n    ```\n    You can display your 3D mesh in a notebook from either its url, or its tensors:\n    ```python\n    from docarray.documents import Mesh3D\n    # display from url\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # mesh.url.display()\n    # display from tensors\n    mesh.tensors = mesh.url.load()\n    # mesh.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[Mesh3DUrl] = Field(\ndescription='URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[VerticesAndFaces] = Field(\ndescription='A tensor object of 3D mesh of type `VerticesAndFaces`.',\nexample=[[0, 1, 1], [1, 0, 1], [1, 1, 0]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the 3D mesh.',\ndefault=[1, 0, 1],\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D mesh.',\ndefault=None,\n)\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nif isinstance(value, str):\nreturn {'url': value}\nreturn value\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = cls(url=value)\nreturn super().validate(value)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.Mesh3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D","title":"<code>PointCloud3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling point clouds for 3D data representation.</p> <p>Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly sampling points within the surface of the 3D body. Compared to the mesh representation, the point cloud is a fixed size ndarray of shape <code>(n_samples, 3)</code> and hence easier for deep learning algorithms to handle.</p> <p>A PointCloud3D Document can contain:</p> <ul> <li>a <code>PointCloud3DUrl</code> (<code>PointCloud3D.url</code>)</li> <li>a <code>PointsAndColors</code> object (<code>PointCloud3D.tensors</code>)</li> <li>an <code>AnyEmbedding</code> (<code>PointCloud3D.embedding</code>)</li> <li>a <code>bytes</code> object (<code>PointCloud3D.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import PointCloud3D\n# use it directly\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import PointCloud3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyPointCloud3D(PointCloud3D):\nsecond_embedding: Optional[AnyEmbedding] = None\npc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n# pc.second_embedding = model(pc.tensors.colors)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import PointCloud3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\npoint_cloud: PointCloud3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\npoint_cloud=PointCloud3D(\nurl='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n# or\nmmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n</code></pre> <p>You can display your point cloud from either its url, or its tensors:</p> <pre><code>from docarray.documents import PointCloud3D\n# display from url\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# pc.url.display()\n# display from tensors\npc.tensors = pc.url.load(samples=10000)\n# pc.tensors.display()\n</code></pre> Source code in <code>docarray/documents/point_cloud/point_cloud_3d.py</code> <pre><code>class PointCloud3D(BaseDoc):\n\"\"\"\n    Document for handling point clouds for 3D data representation.\n    Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly\n    sampling points within the surface of the 3D body. Compared to the mesh\n    representation, the point cloud is a fixed size ndarray of shape `(n_samples, 3)` and\n    hence easier for deep learning algorithms to handle.\n    A PointCloud3D Document can contain:\n    - a [`PointCloud3DUrl`][docarray.typing.url.PointCloud3DUrl] (`PointCloud3D.url`)\n    - a [`PointsAndColors`][docarray.documents.point_cloud.points_and_colors.PointsAndColors] object (`PointCloud3D.tensors`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`PointCloud3D.embedding`)\n    - a `bytes` object (`PointCloud3D.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import PointCloud3D\n    # use it directly\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import PointCloud3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyPointCloud3D(PointCloud3D):\n        second_embedding: Optional[AnyEmbedding] = None\n    pc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    # pc.second_embedding = model(pc.tensors.colors)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        point_cloud: PointCloud3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        point_cloud=PointCloud3D(\n            url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n    # or\n    mmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n    ```\n    You can display your point cloud from either its url, or its tensors:\n    ```python\n    from docarray.documents import PointCloud3D\n    # display from url\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # pc.url.display()\n    # display from tensors\n    pc.tensors = pc.url.load(samples=10000)\n    # pc.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[PointCloud3DUrl] = Field(\ndescription='URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[PointsAndColors] = Field(\ndescription='A tensor object of 3D point cloud of type `PointsAndColors`.',\nexample=[[0, 0, 1], [1, 0, 1], [0, 1, 1]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of 3D point cloud.',\nexample=[1, 1, 1],\ndefault=None,\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D point cloud.',\ndefault=None,\n)\n@classmethod\ndef _validate(self, value: Union[str, AbstractTensor, Any]) -&gt; Any:\nif isinstance(value, str):\nvalue = {'url': value}\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = {'tensors': PointsAndColors(points=value)}\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointCloud3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors","title":"<code>PointsAndColors</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>PointCloud3D</code> object.</p> <p>A PointsAndColors Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the points in 3D space information (<code>PointsAndColors.points</code>)</li> <li>an <code>AnyTensor</code> containing the points' color information (<code>PointsAndColors.colors</code>)</li> </ul> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>class PointsAndColors(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`PointCloud3D`][docarray.documents.point_cloud.PointCloud3D] object.\n    A PointsAndColors Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points in 3D space information (`PointsAndColors.points`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points' color information (`PointsAndColors.colors`)\n    \"\"\"\npoints: AnyTensor\ncolors: Optional[AnyTensor] = None\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = cls(points=value)\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot point cloud consisting of points in 3D space and optionally colors.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.display","title":"<code>display()</code>","text":"<p>Plot point cloud consisting of points in 3D space and optionally colors.</p> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot point cloud consisting of points in 3D space and optionally colors.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.PointsAndColors.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc","title":"<code>TextDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling text.</p> <p>It can contain:</p> <ul> <li>a <code>TextUrl</code> (<code>TextDoc.url</code>)</li> <li>a <code>str</code> (<code>TextDoc.text</code>)</li> <li>an <code>AnyEmbedding</code> (<code>TextDoc.embedding</code>)</li> <li>a <code>bytes</code> object (<code>TextDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import TextDoc\n# use it directly\ntxt_doc = TextDoc(url='https://www.gutenberg.org/files/1065/1065-0.txt')\ntxt_doc.text = txt_doc.url.load()\n# model = MyEmbeddingModel()\n# txt_doc.embedding = model(txt_doc.text)\n</code></pre> <p>You can initialize directly from a string:</p> <pre><code>from docarray.documents import TextDoc\ntxt_doc = TextDoc('hello world')\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyText(TextDoc):\nsecond_embedding: Optional[AnyEmbedding] = None\ntxt_doc = MyText(url='https://www.gutenberg.org/files/1065/1065-0.txt')\ntxt_doc.text = txt_doc.url.load()\n# model = MyEmbeddingModel()\n# txt_doc.embedding = model(txt_doc.text)\n# txt_doc.second_embedding = model(txt_doc.text)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nimage_doc: ImageDoc\ntext_doc: TextDoc\nmmdoc = MultiModalDoc(\nimage_doc=ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n),\ntext_doc=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.image_doc.tensor = mmdoc.image_doc.url.load()\n# or\nmmdoc.image_doc.bytes_ = mmdoc.image_doc.url.load_bytes()\nmmdoc.image_doc.tensor = mmdoc.image_doc.bytes_.load()\n</code></pre> <p>This Document can be compared against another Document of the same type or a string. When compared against another object of the same type, the pydantic BaseModel equality check will apply which checks the equality of every attribute, excluding <code>id</code>. When compared against a str, it will check the equality of the <code>text</code> attribute against the given string.</p> <pre><code>from docarray.documents import TextDoc\ndoc = TextDoc(text='This is the main text', url='exampleurl.com/file')\ndoc2 = TextDoc(text='This is the main text', url='exampleurl.com/file')\ndoc == 'This is the main text'  # True\ndoc == doc2  # True\n</code></pre> Source code in <code>docarray/documents/text.py</code> <pre><code>class TextDoc(BaseDoc):\n\"\"\"\n    Document for handling text.\n    It can contain:\n    - a [`TextUrl`][docarray.typing.url.TextUrl] (`TextDoc.url`)\n    - a `str` (`TextDoc.text`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`TextDoc.embedding`)\n    - a `bytes` object (`TextDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import TextDoc\n    # use it directly\n    txt_doc = TextDoc(url='https://www.gutenberg.org/files/1065/1065-0.txt')\n    txt_doc.text = txt_doc.url.load()\n    # model = MyEmbeddingModel()\n    # txt_doc.embedding = model(txt_doc.text)\n    ```\n    You can initialize directly from a string:\n    ```python\n    from docarray.documents import TextDoc\n    txt_doc = TextDoc('hello world')\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import TextDoc\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyText(TextDoc):\n        second_embedding: Optional[AnyEmbedding] = None\n    txt_doc = MyText(url='https://www.gutenberg.org/files/1065/1065-0.txt')\n    txt_doc.text = txt_doc.url.load()\n    # model = MyEmbeddingModel()\n    # txt_doc.embedding = model(txt_doc.text)\n    # txt_doc.second_embedding = model(txt_doc.text)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import ImageDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        image_doc: ImageDoc\n        text_doc: TextDoc\n    mmdoc = MultiModalDoc(\n        image_doc=ImageDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n        ),\n        text_doc=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.image_doc.tensor = mmdoc.image_doc.url.load()\n    # or\n    mmdoc.image_doc.bytes_ = mmdoc.image_doc.url.load_bytes()\n    mmdoc.image_doc.tensor = mmdoc.image_doc.bytes_.load()\n    ```\n    This Document can be compared against another Document of the same type or a string.\n    When compared against another object of the same type, the pydantic BaseModel\n    equality check will apply which checks the equality of every attribute,\n    excluding `id`. When compared against a str, it will check the equality\n    of the `text` attribute against the given string.\n    ```python\n    from docarray.documents import TextDoc\n    doc = TextDoc(text='This is the main text', url='exampleurl.com/file')\n    doc2 = TextDoc(text='This is the main text', url='exampleurl.com/file')\n    doc == 'This is the main text'  # True\n    doc == doc2  # True\n    ```\n    \"\"\"\ntext: Optional[str] = Field(\ndescription='The text content stored in the document',\nexample='This is an example text content of the document',\ndefault=None,\n)\nurl: Optional[TextUrl] = Field(\ndescription='URL to a (potentially remote) text file that can be loaded',\nexample='https://www.w3.org/History/19921103-hypertext/hypertext/README.html',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the text',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of the text',\ndefault=None,\n)\ndef __init__(self, text: Optional[str] = None, **kwargs):\nif 'text' not in kwargs:\nkwargs['text'] = text\nsuper().__init__(**kwargs)\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, values):\nif isinstance(values, str):\nreturn {'text': values}\nelse:\nreturn values\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = cls(text=value)\nreturn super().validate(value)\ndef __eq__(self, other: Any) -&gt; bool:\nif isinstance(other, str):\nreturn self.text == other\nelse:\n# BaseModel has a default equality\nreturn super().__eq__(other)\ndef __contains__(self, item: str) -&gt; bool:\n\"\"\"\n        This method makes `TextDoc` behave the same as an `str`.\n        :param item: A string to be checked if is a substring of `text` attribute\n        :return: A boolean determining the presence of `item` as a substring in `text`\n        ```python\n        from docarray.documents import TextDoc\n        t = TextDoc(text='this is my text document')\n        assert 'text' in t\n        assert 'docarray' not in t\n        ```\n        \"\"\"\nif self.text is not None:\nreturn self.text.__contains__(item)\nelse:\nreturn False\ndef _get_string_for_regex_filter(self):\nreturn self.text\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.__contains__","title":"<code>__contains__(item)</code>","text":"<p>This method makes <code>TextDoc</code> behave the same as an <code>str</code>.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>A string to be checked if is a substring of <code>text</code> attribute</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean determining the presence of <code>item</code> as a substring in <code>text</code> <code>python from docarray.documents import TextDoc  t = TextDoc(text='this is my text document') assert 'text' in t assert 'docarray' not in t</code></p> Source code in <code>docarray/documents/text.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n\"\"\"\n    This method makes `TextDoc` behave the same as an `str`.\n    :param item: A string to be checked if is a substring of `text` attribute\n    :return: A boolean determining the presence of `item` as a substring in `text`\n    ```python\n    from docarray.documents import TextDoc\n    t = TextDoc(text='this is my text document')\n    assert 'text' in t\n    assert 'docarray' not in t\n    ```\n    \"\"\"\nif self.text is not None:\nreturn self.text.__contains__(item)\nelse:\nreturn False\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.TextDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces","title":"<code>VerticesAndFaces</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>Mesh3D</code> object.</p> <p>A VerticesAndFaces Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the vertices information (<code>VerticesAndFaces.vertices</code>)</li> <li>an <code>AnyTensor</code> containing the faces information (<code>VerticesAndFaces.faces</code>)</li> </ul> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>class VerticesAndFaces(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`Mesh3D`][docarray.documents.mesh.Mesh3D] object.\n    A VerticesAndFaces Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the vertices information (`VerticesAndFaces.vertices`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the faces information (`VerticesAndFaces.faces`)\n    \"\"\"\nvertices: AnyTensor\nfaces: AnyTensor\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh consisting of vertices and faces.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.display","title":"<code>display()</code>","text":"<p>Plot mesh consisting of vertices and faces.</p> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh consisting of vertices and faces.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VerticesAndFaces.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc","title":"<code>VideoDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling video.</p> <p>The Video Document can contain:</p> <ul> <li>a <code>VideoUrl</code> (<code>VideoDoc.url</code>)</li> <li>an <code>AudioDoc</code> (<code>VideoDoc.audio</code>)</li> <li>a <code>VideoTensor</code> (<code>VideoDoc.tensor</code>)</li> <li>an <code>AnyTensor</code> representing the indices of the video's key frames (<code>VideoDoc.key_frame_indices</code>)</li> <li>an <code>AnyEmbedding</code> (<code>VideoDoc.embedding</code>)</li> <li>a <code>VideoBytes</code> object (<code>VideoDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import VideoDoc, AudioDoc\n# use it directly\nvid = VideoDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ntensor, audio_tensor, key_frame_indices = vid.url.load()\nvid.tensor = tensor\nvid.audio = AudioDoc(tensor=audio_tensor)\nvid.key_frame_indices = key_frame_indices\n# model = MyEmbeddingModel()\n# vid.embedding = model(vid.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from typing import Optional\nfrom docarray.documents import TextDoc, VideoDoc\n# extend it\nclass MyVideo(VideoDoc):\nname: Optional[TextDoc] = None\nvideo = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\nvideo.name = TextDoc(text='my first video')\nvideo.tensor = video.url.load().video\n# model = MyEmbeddingModel()\n# video.embedding = model(video.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import TextDoc, VideoDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nvideo: VideoDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\nvideo=VideoDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.video.tensor = mmdoc.video.url.load().video\n# or\nmmdoc.video.bytes_ = mmdoc.video.url.load_bytes()\nmmdoc.video.tensor = mmdoc.video.bytes_.load().video\n</code></pre> Source code in <code>docarray/documents/video.py</code> <pre><code>class VideoDoc(BaseDoc):\n\"\"\"\n    Document for handling video.\n    The Video Document can contain:\n    - a [`VideoUrl`][docarray.typing.url.VideoUrl] (`VideoDoc.url`)\n    - an [`AudioDoc`][docarray.documents.AudioDoc] (`VideoDoc.audio`)\n    - a [`VideoTensor`](../../../api_references/typing/tensor/video) (`VideoDoc.tensor`)\n    - an [`AnyTensor`](../../../api_references/typing/tensor/tensor) representing the indices of the video's key frames (`VideoDoc.key_frame_indices`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`VideoDoc.embedding`)\n    - a [`VideoBytes`][docarray.typing.bytes.VideoBytes] object (`VideoDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import VideoDoc, AudioDoc\n    # use it directly\n    vid = VideoDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    tensor, audio_tensor, key_frame_indices = vid.url.load()\n    vid.tensor = tensor\n    vid.audio = AudioDoc(tensor=audio_tensor)\n    vid.key_frame_indices = key_frame_indices\n    # model = MyEmbeddingModel()\n    # vid.embedding = model(vid.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from typing import Optional\n    from docarray.documents import TextDoc, VideoDoc\n    # extend it\n    class MyVideo(VideoDoc):\n        name: Optional[TextDoc] = None\n    video = MyVideo(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    video.name = TextDoc(text='my first video')\n    video.tensor = video.url.load().video\n    # model = MyEmbeddingModel()\n    # video.embedding = model(video.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import TextDoc, VideoDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        video: VideoDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        video=VideoDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.video.tensor = mmdoc.video.url.load().video\n    # or\n    mmdoc.video.bytes_ = mmdoc.video.url.load_bytes()\n    mmdoc.video.tensor = mmdoc.video.bytes_.load().video\n    ```\n    \"\"\"\nurl: Optional[VideoUrl] = Field(\ndescription='URL to a (potentially remote) video file that needs to be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\ndefault=None,\n)\naudio: Optional[AudioDoc] = Field(\ndescription='Audio document associated with the video',\ndefault=None,\n)\ntensor: Optional[VideoTensor] = Field(\ndescription='Tensor object representing the video which be specified to one of `VideoNdArray`, `VideoTorchTensor`, `VideoTensorFlowTensor`',\ndefault=None,\n)\nkey_frame_indices: Optional[AnyTensor] = Field(\ndescription='List of all the key frames in the video',\nexample=[0, 1, 2, 3, 4],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the video',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[VideoBytes] = Field(\ndescription='Bytes representation of the video',\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.VideoDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio","title":"<code>audio</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc","title":"<code>AudioDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling audios.</p> <p>The Audio Document can contain:</p> <ul> <li>an <code>AudioUrl</code> (<code>AudioDoc.url</code>)</li> <li>an <code>AudioTensor</code> (<code>AudioDoc.tensor</code>)</li> <li>an <code>AnyEmbedding</code> (<code>AudioDoc.embedding</code>)</li> <li>an <code>AudioBytes</code> (<code>AudioDoc.bytes_</code>) object</li> <li>an integer representing the frame_rate (<code>AudioDoc.frame_rate</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import AudioDoc\n# use it directly\naudio = AudioDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n)\naudio.tensor, audio.frame_rate = audio.url.load()\n# model = MyEmbeddingModel()\n# audio.embedding = model(audio.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import AudioDoc, TextDoc\nfrom typing import Optional\n# extend it\nclass MyAudio(AudioDoc):\nname: Optional[TextDoc] = None\naudio = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n)\naudio.name = TextDoc(text='my first audio')\naudio.tensor, audio.frame_rate = audio.url.load()\n# model = MyEmbeddingModel()\n# audio.embedding = model(audio.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import AudioDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\naudio: AudioDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\naudio=AudioDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.url.load()\n# equivalent to\nmmdoc.audio.bytes_ = mmdoc.audio.url.load_bytes()\nmmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.bytes_.load()\n</code></pre> Source code in <code>docarray/documents/audio.py</code> <pre><code>class AudioDoc(BaseDoc):\n\"\"\"\n    Document for handling audios.\n    The Audio Document can contain:\n    - an [`AudioUrl`][docarray.typing.url.AudioUrl] (`AudioDoc.url`)\n    - an [`AudioTensor`](../../../api_references/typing/tensor/audio) (`AudioDoc.tensor`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`AudioDoc.embedding`)\n    - an [`AudioBytes`][docarray.typing.bytes.AudioBytes] (`AudioDoc.bytes_`) object\n    - an integer representing the frame_rate (`AudioDoc.frame_rate`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import AudioDoc\n    # use it directly\n    audio = AudioDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n    )\n    audio.tensor, audio.frame_rate = audio.url.load()\n    # model = MyEmbeddingModel()\n    # audio.embedding = model(audio.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import AudioDoc, TextDoc\n    from typing import Optional\n    # extend it\n    class MyAudio(AudioDoc):\n        name: Optional[TextDoc] = None\n    audio = MyAudio(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n    )\n    audio.name = TextDoc(text='my first audio')\n    audio.tensor, audio.frame_rate = audio.url.load()\n    # model = MyEmbeddingModel()\n    # audio.embedding = model(audio.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import AudioDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        audio: AudioDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        audio=AudioDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.wav?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.url.load()\n    # equivalent to\n    mmdoc.audio.bytes_ = mmdoc.audio.url.load_bytes()\n    mmdoc.audio.tensor, mmdoc.audio.frame_rate = mmdoc.audio.bytes_.load()\n    ```\n    \"\"\"\nurl: Optional[AudioUrl] = Field(\ndescription='The url to a (potentially remote) audio file that can be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true',\ndefault=None,\n)\ntensor: Optional[AudioTensor] = Field(\ndescription='Tensor object of the audio which can be specified to one of `AudioNdArray`, `AudioTorchTensor`, `AudioTensorFlowTensor`',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the audio.',\nexample=[0, 1, 0],\ndefault=None,\n)\nbytes_: Optional[AudioBytes] = Field(\ndescription='Bytes representation pf the audio',\ndefault=None,\n)\nframe_rate: Optional[int] = Field(\ndescription='An integer representing the frame rate of the audio.',\nexample=24,\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.audio.AudioDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.helper","title":"<code>helper</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.helper.create_doc","title":"<code>create_doc(__model_name, *, __config__=None, __base__=BaseDoc, __module__=__name__, __validators__=None, __cls_kwargs__=None, __slots__=None, **field_definitions)</code>","text":"<p>Dynamically create a subclass of BaseDoc. This is a wrapper around pydantic's create_model.</p> <p>Note</p> <p>To pickle a dynamically created BaseDoc subclass:</p> <ul> <li>the class must be defined globally</li> <li>it must provide <code>__module__</code></li> </ul> <pre><code>from docarray.documents import Audio\nfrom docarray.documents.helper import create_doc\nfrom docarray.typing.tensor.audio import AudioNdArray\nMyAudio = create_doc(\n'MyAudio',\n__base__=Audio,\ntitle=(str, ...),\ntensor=(AudioNdArray, ...),\n)\nassert safe_issubclass(MyAudio, BaseDoc)\nassert safe_issubclass(MyAudio, Audio)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>__model_name</code> <code>str</code> <p>name of the created model</p> required <code>__config__</code> <code>Optional[Type[BaseConfig]]</code> <p>config class to use for the new model</p> <code>None</code> <code>__base__</code> <code>Type[T_doc]</code> <p>base class for the new model to inherit from, must be BaseDoc or its subclass</p> <code>BaseDoc</code> <code>__module__</code> <code>str</code> <p>module of the created model</p> <code>__name__</code> <code>__validators__</code> <code>Dict[str, AnyClassMethod]</code> <p>a dict of method names and @validator class methods</p> <code>None</code> <code>__cls_kwargs__</code> <code>Dict[str, Any]</code> <p>a dict for class creation</p> <code>None</code> <code>__slots__</code> <code>Optional[Tuple[str, ...]]</code> <p>Deprecated, <code>__slots__</code> should not be passed to <code>create_model</code></p> <code>None</code> <code>field_definitions</code> <code>Any</code> <p>fields of the model (or extra fields if a base is supplied) in the format <code>&lt;name&gt;=(&lt;type&gt;, &lt;default default&gt;)</code> or <code>&lt;name&gt;=&lt;default value&gt;</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[T_doc]</code> <p>the new Document class</p> Source code in <code>docarray/documents/helper.py</code> <pre><code>def create_doc(\n__model_name: str,\n*,\n__config__: Optional[Type[BaseConfig]] = None,\n__base__: Type['T_doc'] = BaseDoc,  # type: ignore\n__module__: str = __name__,\n__validators__: Dict[str, 'AnyClassMethod'] = None,  # type: ignore\n__cls_kwargs__: Dict[str, Any] = None,  # type: ignore\n__slots__: Optional[Tuple[str, ...]] = None,\n**field_definitions: Any,\n) -&gt; Type['T_doc']:\n\"\"\"\n    Dynamically create a subclass of BaseDoc. This is a wrapper around pydantic's create_model.\n    !!! note\n        To pickle a dynamically created BaseDoc subclass:\n        - the class must be defined globally\n        - it must provide `__module__`\n    ```python\n    from docarray.documents import Audio\n    from docarray.documents.helper import create_doc\n    from docarray.typing.tensor.audio import AudioNdArray\n    MyAudio = create_doc(\n        'MyAudio',\n        __base__=Audio,\n        title=(str, ...),\n        tensor=(AudioNdArray, ...),\n    )\n    assert safe_issubclass(MyAudio, BaseDoc)\n    assert safe_issubclass(MyAudio, Audio)\n    ```\n    :param __model_name: name of the created model\n    :param __config__: config class to use for the new model\n    :param __base__: base class for the new model to inherit from, must be BaseDoc or its subclass\n    :param __module__: module of the created model\n    :param __validators__: a dict of method names and @validator class methods\n    :param __cls_kwargs__: a dict for class creation\n    :param __slots__: Deprecated, `__slots__` should not be passed to `create_model`\n    :param field_definitions: fields of the model (or extra fields if a base is supplied)\n        in the format `&lt;name&gt;=(&lt;type&gt;, &lt;default default&gt;)` or `&lt;name&gt;=&lt;default value&gt;`\n    :return: the new Document class\n    \"\"\"\nif not safe_issubclass(__base__, BaseDoc):\nraise ValueError(f'{type(__base__)} is not a BaseDoc or its subclass')\ndoc = create_model(\n__model_name,\n__config__=__config__,\n__base__=__base__,\n__module__=__module__,\n__validators__=__validators__,\n__cls_kwargs__=__cls_kwargs__,\n__slots__=__slots__,\n**field_definitions,\n)\nreturn doc\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.helper.create_doc_from_dict","title":"<code>create_doc_from_dict(model_name, data_dict)</code>","text":"<p>Create a subclass of BaseDoc based on example data given as a dictionary.</p> <p>In case the example contains None as a value, corresponding field will be viewed as the type Any.</p> <pre><code>import numpy as np\nfrom docarray.documents import ImageDoc\nfrom docarray.documents.helper import create_doc_from_dict\ndata_dict = {'image': ImageDoc(tensor=np.random.rand(3, 224, 224)), 'author': 'me'}\nMyDoc = create_doc_from_dict(model_name='MyDoc', data_dict=data_dict)\nassert safe_issubclass(MyDoc, BaseDoc)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the new Document class</p> required <code>data_dict</code> <code>Dict[str, Any]</code> <p>Dictionary of field types to their corresponding values.</p> required <p>Returns:</p> Type Description <code>Type[T_doc]</code> <p>the new Document class</p> Source code in <code>docarray/documents/helper.py</code> <pre><code>def create_doc_from_dict(model_name: str, data_dict: Dict[str, Any]) -&gt; Type['T_doc']:\n\"\"\"\n    Create a subclass of BaseDoc based on example data given as a dictionary.\n    In case the example contains None as a value,\n    corresponding field will be viewed as the type Any.\n    ---\n    ```python\n    import numpy as np\n    from docarray.documents import ImageDoc\n    from docarray.documents.helper import create_doc_from_dict\n    data_dict = {'image': ImageDoc(tensor=np.random.rand(3, 224, 224)), 'author': 'me'}\n    MyDoc = create_doc_from_dict(model_name='MyDoc', data_dict=data_dict)\n    assert safe_issubclass(MyDoc, BaseDoc)\n    ```\n    ---\n    :param model_name: Name of the new Document class\n    :param data_dict: Dictionary of field types to their corresponding values.\n    :return: the new Document class\n    \"\"\"\nif not data_dict:\nraise ValueError('`data_dict` should contain at least one item')\nfield_types = {\nfield: (type(value) if value else Any, ...)\nfor field, value in data_dict.items()\n}\nreturn create_doc(__model_name=model_name, **field_types)  # type: ignore\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.helper.create_doc_from_typeddict","title":"<code>create_doc_from_typeddict(typeddict_cls, **kwargs)</code>","text":"<p>Create a subclass of BaseDoc based on the fields of a <code>TypedDict</code>. This is a wrapper around pydantic's create_model_from_typeddict.</p> <pre><code>from typing_extensions import TypedDict\nfrom docarray import BaseDoc\nfrom docarray.documents import Audio\nfrom docarray.documents.helper import create_doc_from_typeddict\nfrom docarray.typing.tensor.audio import AudioNdArray\nclass MyAudio(TypedDict):\ntitle: str\ntensor: AudioNdArray\nDoc = create_doc_from_typeddict(MyAudio, __base__=Audio)\nassert safe_issubclass(Doc, BaseDoc)\nassert safe_issubclass(Doc, Audio)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>typeddict_cls</code> <code>Type[TypedDict]</code> <p>TypedDict class to use for the new Document class</p> required <code>kwargs</code> <code>Any</code> <p>extra arguments to pass to <code>create_model_from_typeddict</code></p> <code>{}</code> <p>Returns:</p> Type Description <p>the new Document class</p> Source code in <code>docarray/documents/helper.py</code> <pre><code>def create_doc_from_typeddict(\ntypeddict_cls: Type['TypedDict'],  # type: ignore\n**kwargs: Any,\n):\n\"\"\"\n    Create a subclass of BaseDoc based on the fields of a `TypedDict`. This is a wrapper around pydantic's create_model_from_typeddict.\n    ---\n    ```python\n    from typing_extensions import TypedDict\n    from docarray import BaseDoc\n    from docarray.documents import Audio\n    from docarray.documents.helper import create_doc_from_typeddict\n    from docarray.typing.tensor.audio import AudioNdArray\n    class MyAudio(TypedDict):\n        title: str\n        tensor: AudioNdArray\n    Doc = create_doc_from_typeddict(MyAudio, __base__=Audio)\n    assert safe_issubclass(Doc, BaseDoc)\n    assert safe_issubclass(Doc, Audio)\n    ```\n    ---\n    :param typeddict_cls: TypedDict class to use for the new Document class\n    :param kwargs: extra arguments to pass to `create_model_from_typeddict`\n    :return: the new Document class\n    \"\"\"\nif '__base__' in kwargs:\nif not safe_issubclass(kwargs['__base__'], BaseDoc):\nraise ValueError(f'{kwargs[\"__base__\"]} is not a BaseDoc or its subclass')\nelse:\nkwargs['__base__'] = BaseDoc\ndoc = create_model_from_typeddict(typeddict_cls, **kwargs)\nreturn doc\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image","title":"<code>image</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc","title":"<code>ImageDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling images.</p> <p>It can contain:</p> <ul> <li>an <code>ImageUrl</code> (<code>Image.url</code>)</li> <li>an <code>ImageTensor</code> (<code>Image.tensor</code>)</li> <li>an <code>AnyEmbedding</code> (<code>Image.embedding</code>)</li> <li>an <code>ImageBytes</code> object (<code>ImageDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import ImageDoc\n# use it directly\nimage = ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\nimage.tensor = image.url.load()\n# model = MyEmbeddingModel()\n# image.embedding = model(image.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import ImageDoc\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyImage(ImageDoc):\nsecond_embedding: Optional[AnyEmbedding] = None\nimage = MyImage(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\nimage.tensor = image.url.load()\n# model = MyEmbeddingModel()\n# image.embedding = model(image.tensor)\n# image.second_embedding = model(image.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nimage: ImageDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\nimage=ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.image.tensor = mmdoc.image.url.load()\n# or\nmmdoc.image.bytes_ = mmdoc.image.url.load_bytes()\nmmdoc.image.tensor = mmdoc.image.bytes_.load()\n</code></pre> Source code in <code>docarray/documents/image.py</code> <pre><code>class ImageDoc(BaseDoc):\n\"\"\"\n    Document for handling images.\n    It can contain:\n    - an [`ImageUrl`][docarray.typing.url.ImageUrl] (`Image.url`)\n    - an [`ImageTensor`](../../../api_references/typing/tensor/image) (`Image.tensor`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`Image.embedding`)\n    - an [`ImageBytes`][docarray.typing.bytes.ImageBytes] object (`ImageDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import ImageDoc\n    # use it directly\n    image = ImageDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n    )\n    image.tensor = image.url.load()\n    # model = MyEmbeddingModel()\n    # image.embedding = model(image.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import ImageDoc\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyImage(ImageDoc):\n        second_embedding: Optional[AnyEmbedding] = None\n    image = MyImage(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n    )\n    image.tensor = image.url.load()\n    # model = MyEmbeddingModel()\n    # image.embedding = model(image.tensor)\n    # image.second_embedding = model(image.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import ImageDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        image: ImageDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        image=ImageDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.image.tensor = mmdoc.image.url.load()\n    # or\n    mmdoc.image.bytes_ = mmdoc.image.url.load_bytes()\n    mmdoc.image.tensor = mmdoc.image.bytes_.load()\n    ```\n    \"\"\"\nurl: Optional[ImageUrl] = Field(\ndescription='URL to a (potentially remote) image file that needs to be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true',\ndefault=None,\n)\ntensor: Optional[ImageTensor] = Field(\ndescription='Tensor object of the image which can be specifed to one of `ImageNdArray`, `ImageTorchTensor`, `ImageTensorflowTensor`.',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the image.',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[ImageBytes] = Field(\ndescription='Bytes object of the image which is an instance of `ImageBytes`.',\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif (\nisinstance(value, (AbstractTensor, np.ndarray))\nor (torch is not None and isinstance(value, torch.Tensor))\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nelif isinstance(value, bytes):\nvalue = dict(byte=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.image.ImageDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy","title":"<code>legacy</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument","title":"<code>LegacyDocument</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>This Document is the LegacyDocument. It follows the same schema as in DocArray &lt;=0.21. It can be useful to start migrating a codebase from v1 to v2.</p> <p>Nevertheless, the API is not totally compatible with DocArray &lt;=0.21 <code>Document</code>. Indeed, none of the method associated with <code>Document</code> are present. Only the schema of the data is similar.</p> <pre><code>from docarray import DocList\nfrom docarray.documents.legacy import LegacyDocument\nimport numpy as np\ndoc = LegacyDocument(text='hello')\ndoc.url = 'http://myimg.png'\ndoc.tensor = np.zeros((3, 224, 224))\ndoc.embedding = np.zeros((100, 1))\ndoc.tags['price'] = 10\ndoc.chunks = DocList[Document]([Document() for _ in range(10)])\ndoc.chunks = DocList[Document]([Document() for _ in range(10)])\n</code></pre> Source code in <code>docarray/documents/legacy/legacy_document.py</code> <pre><code>class LegacyDocument(BaseDoc):\n\"\"\"\n    This Document is the LegacyDocument. It follows the same schema as in DocArray &lt;=0.21.\n    It can be useful to start migrating a codebase from v1 to v2.\n    Nevertheless, the API is not totally compatible with DocArray &lt;=0.21 `Document`.\n    Indeed, none of the method associated with `Document` are present. Only the schema\n    of the data is similar.\n    ```python\n    from docarray import DocList\n    from docarray.documents.legacy import LegacyDocument\n    import numpy as np\n    doc = LegacyDocument(text='hello')\n    doc.url = 'http://myimg.png'\n    doc.tensor = np.zeros((3, 224, 224))\n    doc.embedding = np.zeros((100, 1))\n    doc.tags['price'] = 10\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n    ```\n    \"\"\"\ntensor: Optional[AnyTensor] = None\nchunks: Optional[DocList[LegacyDocument]] = None\nmatches: Optional[DocList[LegacyDocument]] = None\nblob: Optional[bytes] = None\ntext: Optional[str] = None\nurl: Optional[str] = None\nembedding: Optional[AnyEmbedding] = None\ntags: Dict[str, Any] = dict()\nscores: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.LegacyDocument.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document","title":"<code>legacy_document</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument","title":"<code>LegacyDocument</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>This Document is the LegacyDocument. It follows the same schema as in DocArray &lt;=0.21. It can be useful to start migrating a codebase from v1 to v2.</p> <p>Nevertheless, the API is not totally compatible with DocArray &lt;=0.21 <code>Document</code>. Indeed, none of the method associated with <code>Document</code> are present. Only the schema of the data is similar.</p> <pre><code>from docarray import DocList\nfrom docarray.documents.legacy import LegacyDocument\nimport numpy as np\ndoc = LegacyDocument(text='hello')\ndoc.url = 'http://myimg.png'\ndoc.tensor = np.zeros((3, 224, 224))\ndoc.embedding = np.zeros((100, 1))\ndoc.tags['price'] = 10\ndoc.chunks = DocList[Document]([Document() for _ in range(10)])\ndoc.chunks = DocList[Document]([Document() for _ in range(10)])\n</code></pre> Source code in <code>docarray/documents/legacy/legacy_document.py</code> <pre><code>class LegacyDocument(BaseDoc):\n\"\"\"\n    This Document is the LegacyDocument. It follows the same schema as in DocArray &lt;=0.21.\n    It can be useful to start migrating a codebase from v1 to v2.\n    Nevertheless, the API is not totally compatible with DocArray &lt;=0.21 `Document`.\n    Indeed, none of the method associated with `Document` are present. Only the schema\n    of the data is similar.\n    ```python\n    from docarray import DocList\n    from docarray.documents.legacy import LegacyDocument\n    import numpy as np\n    doc = LegacyDocument(text='hello')\n    doc.url = 'http://myimg.png'\n    doc.tensor = np.zeros((3, 224, 224))\n    doc.embedding = np.zeros((100, 1))\n    doc.tags['price'] = 10\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n    ```\n    \"\"\"\ntensor: Optional[AnyTensor] = None\nchunks: Optional[DocList[LegacyDocument]] = None\nmatches: Optional[DocList[LegacyDocument]] = None\nblob: Optional[bytes] = None\ntext: Optional[str] = None\nurl: Optional[str] = None\nembedding: Optional[AnyEmbedding] = None\ntags: Dict[str, Any] = dict()\nscores: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.legacy.legacy_document.LegacyDocument.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh","title":"<code>mesh</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D","title":"<code>Mesh3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling meshes for 3D data representation.</p> <p>A mesh is a representation for 3D data and contains vertices and faces information. Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3). Faces are triangular surfaces that can be defined by three points in 3D space, corresponding to the three vertices of a triangle. Faces can be represented as a tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a vertex in the tensor of vertices.</p> <p>The Mesh3D Document can contain:</p> <ul> <li>an <code>Mesh3DUrl</code> (<code>Mesh3D.url</code>)</li> <li> <p>a <code>VerticesAndFaces</code> object containing:</p> <ul> <li>an <code>AnyTensor</code> of vertices (<code>Mesh3D.tensors.vertices</code>)</li> <li>an <code>AnyTensor</code> of faces (<code>Mesh3D.tensors.faces</code>)</li> </ul> </li> <li> <p>an <code>AnyEmbedding</code> (<code>Mesh3D.embedding</code>)</p> </li> <li>a <code>bytes</code> object (<code>Mesh3D.bytes_</code>).</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import Mesh3D\n# use it directly\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.tensors.vertices)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import Mesh3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyMesh3D(Mesh3D):\nname: Optional[str] = None\nmesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.name = 'my first mesh'\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.vertices)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import Mesh3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nmesh: Mesh3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\nmesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.mesh.tensors = mmdoc.mesh.url.load()\n# or\nmmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n</code></pre> <p>You can display your 3D mesh in a notebook from either its url, or its tensors:</p> <pre><code>from docarray.documents import Mesh3D\n# display from url\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# mesh.url.display()\n# display from tensors\nmesh.tensors = mesh.url.load()\n# mesh.tensors.display()\n</code></pre> Source code in <code>docarray/documents/mesh/mesh_3d.py</code> <pre><code>class Mesh3D(BaseDoc):\n\"\"\"\n    Document for handling meshes for 3D data representation.\n    A mesh is a representation for 3D data and contains vertices and faces information.\n    Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3).\n    Faces are triangular surfaces that can be defined by three points in 3D space,\n    corresponding to the three vertices of a triangle. Faces can be represented as a\n    tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a\n    vertex in the tensor of vertices.\n    The Mesh3D Document can contain:\n    - an [`Mesh3DUrl`][docarray.typing.url.Mesh3DUrl] (`Mesh3D.url`)\n    - a [`VerticesAndFaces`][docarray.documents.mesh.vertices_and_faces.VerticesAndFaces]\n    object containing:\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of\n        vertices (`Mesh3D.tensors.vertices`)\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of faces (`Mesh3D.tensors.faces`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`Mesh3D.embedding`)\n    - a `bytes` object (`Mesh3D.bytes_`).\n    You can use this Document directly:\n    ```python\n    from docarray.documents import Mesh3D\n    # use it directly\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.tensors.vertices)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import Mesh3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyMesh3D(Mesh3D):\n        name: Optional[str] = None\n    mesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.name = 'my first mesh'\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.vertices)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import Mesh3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        mesh: Mesh3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        mesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.mesh.tensors = mmdoc.mesh.url.load()\n    # or\n    mmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n    ```\n    You can display your 3D mesh in a notebook from either its url, or its tensors:\n    ```python\n    from docarray.documents import Mesh3D\n    # display from url\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # mesh.url.display()\n    # display from tensors\n    mesh.tensors = mesh.url.load()\n    # mesh.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[Mesh3DUrl] = Field(\ndescription='URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[VerticesAndFaces] = Field(\ndescription='A tensor object of 3D mesh of type `VerticesAndFaces`.',\nexample=[[0, 1, 1], [1, 0, 1], [1, 1, 0]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the 3D mesh.',\ndefault=[1, 0, 1],\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D mesh.',\ndefault=None,\n)\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nif isinstance(value, str):\nreturn {'url': value}\nreturn value\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = cls(url=value)\nreturn super().validate(value)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.Mesh3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces","title":"<code>VerticesAndFaces</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>Mesh3D</code> object.</p> <p>A VerticesAndFaces Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the vertices information (<code>VerticesAndFaces.vertices</code>)</li> <li>an <code>AnyTensor</code> containing the faces information (<code>VerticesAndFaces.faces</code>)</li> </ul> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>class VerticesAndFaces(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`Mesh3D`][docarray.documents.mesh.Mesh3D] object.\n    A VerticesAndFaces Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the vertices information (`VerticesAndFaces.vertices`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the faces information (`VerticesAndFaces.faces`)\n    \"\"\"\nvertices: AnyTensor\nfaces: AnyTensor\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh consisting of vertices and faces.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.display","title":"<code>display()</code>","text":"<p>Plot mesh consisting of vertices and faces.</p> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh consisting of vertices and faces.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.VerticesAndFaces.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d","title":"<code>mesh_3d</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D","title":"<code>Mesh3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling meshes for 3D data representation.</p> <p>A mesh is a representation for 3D data and contains vertices and faces information. Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3). Faces are triangular surfaces that can be defined by three points in 3D space, corresponding to the three vertices of a triangle. Faces can be represented as a tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a vertex in the tensor of vertices.</p> <p>The Mesh3D Document can contain:</p> <ul> <li>an <code>Mesh3DUrl</code> (<code>Mesh3D.url</code>)</li> <li> <p>a <code>VerticesAndFaces</code> object containing:</p> <ul> <li>an <code>AnyTensor</code> of vertices (<code>Mesh3D.tensors.vertices</code>)</li> <li>an <code>AnyTensor</code> of faces (<code>Mesh3D.tensors.faces</code>)</li> </ul> </li> <li> <p>an <code>AnyEmbedding</code> (<code>Mesh3D.embedding</code>)</p> </li> <li>a <code>bytes</code> object (<code>Mesh3D.bytes_</code>).</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import Mesh3D\n# use it directly\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.tensors.vertices)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import Mesh3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyMesh3D(Mesh3D):\nname: Optional[str] = None\nmesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\nmesh.name = 'my first mesh'\nmesh.tensors = mesh.url.load()\n# model = MyEmbeddingModel()\n# mesh.embedding = model(mesh.vertices)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import Mesh3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nmesh: Mesh3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\nmesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.mesh.tensors = mmdoc.mesh.url.load()\n# or\nmmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n</code></pre> <p>You can display your 3D mesh in a notebook from either its url, or its tensors:</p> <pre><code>from docarray.documents import Mesh3D\n# display from url\nmesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# mesh.url.display()\n# display from tensors\nmesh.tensors = mesh.url.load()\n# mesh.tensors.display()\n</code></pre> Source code in <code>docarray/documents/mesh/mesh_3d.py</code> <pre><code>class Mesh3D(BaseDoc):\n\"\"\"\n    Document for handling meshes for 3D data representation.\n    A mesh is a representation for 3D data and contains vertices and faces information.\n    Vertices are points in a 3D space, represented as a tensor of shape (n_points, 3).\n    Faces are triangular surfaces that can be defined by three points in 3D space,\n    corresponding to the three vertices of a triangle. Faces can be represented as a\n    tensor of shape (n_faces, 3). Each number in that tensor refers to an index of a\n    vertex in the tensor of vertices.\n    The Mesh3D Document can contain:\n    - an [`Mesh3DUrl`][docarray.typing.url.Mesh3DUrl] (`Mesh3D.url`)\n    - a [`VerticesAndFaces`][docarray.documents.mesh.vertices_and_faces.VerticesAndFaces]\n    object containing:\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of\n        vertices (`Mesh3D.tensors.vertices`)\n        - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor) of faces (`Mesh3D.tensors.faces`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`Mesh3D.embedding`)\n    - a `bytes` object (`Mesh3D.bytes_`).\n    You can use this Document directly:\n    ```python\n    from docarray.documents import Mesh3D\n    # use it directly\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.tensors.vertices)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import Mesh3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyMesh3D(Mesh3D):\n        name: Optional[str] = None\n    mesh = MyMesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    mesh.name = 'my first mesh'\n    mesh.tensors = mesh.url.load()\n    # model = MyEmbeddingModel()\n    # mesh.embedding = model(mesh.vertices)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import Mesh3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        mesh: Mesh3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        mesh=Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.mesh.tensors = mmdoc.mesh.url.load()\n    # or\n    mmdoc.mesh.bytes_ = mmdoc.mesh.url.load_bytes()\n    ```\n    You can display your 3D mesh in a notebook from either its url, or its tensors:\n    ```python\n    from docarray.documents import Mesh3D\n    # display from url\n    mesh = Mesh3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # mesh.url.display()\n    # display from tensors\n    mesh.tensors = mesh.url.load()\n    # mesh.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[Mesh3DUrl] = Field(\ndescription='URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[VerticesAndFaces] = Field(\ndescription='A tensor object of 3D mesh of type `VerticesAndFaces`.',\nexample=[[0, 1, 1], [1, 0, 1], [1, 1, 0]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the 3D mesh.',\ndefault=[1, 0, 1],\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D mesh.',\ndefault=None,\n)\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nif isinstance(value, str):\nreturn {'url': value}\nreturn value\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = cls(url=value)\nreturn super().validate(value)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.mesh_3d.Mesh3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces","title":"<code>vertices_and_faces</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces","title":"<code>VerticesAndFaces</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>Mesh3D</code> object.</p> <p>A VerticesAndFaces Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the vertices information (<code>VerticesAndFaces.vertices</code>)</li> <li>an <code>AnyTensor</code> containing the faces information (<code>VerticesAndFaces.faces</code>)</li> </ul> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>class VerticesAndFaces(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`Mesh3D`][docarray.documents.mesh.Mesh3D] object.\n    A VerticesAndFaces Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the vertices information (`VerticesAndFaces.vertices`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the faces information (`VerticesAndFaces.faces`)\n    \"\"\"\nvertices: AnyTensor\nfaces: AnyTensor\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh consisting of vertices and faces.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.display","title":"<code>display()</code>","text":"<p>Plot mesh consisting of vertices and faces.</p> Source code in <code>docarray/documents/mesh/vertices_and_faces.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh consisting of vertices and faces.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\nif self.vertices is None or self.faces is None:\nraise ValueError(\n'Can\\'t display mesh from tensors when the vertices and/or faces '\n'are None.'\n)\nmesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.mesh.vertices_and_faces.VerticesAndFaces.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud","title":"<code>point_cloud</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D","title":"<code>PointCloud3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling point clouds for 3D data representation.</p> <p>Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly sampling points within the surface of the 3D body. Compared to the mesh representation, the point cloud is a fixed size ndarray of shape <code>(n_samples, 3)</code> and hence easier for deep learning algorithms to handle.</p> <p>A PointCloud3D Document can contain:</p> <ul> <li>a <code>PointCloud3DUrl</code> (<code>PointCloud3D.url</code>)</li> <li>a <code>PointsAndColors</code> object (<code>PointCloud3D.tensors</code>)</li> <li>an <code>AnyEmbedding</code> (<code>PointCloud3D.embedding</code>)</li> <li>a <code>bytes</code> object (<code>PointCloud3D.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import PointCloud3D\n# use it directly\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import PointCloud3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyPointCloud3D(PointCloud3D):\nsecond_embedding: Optional[AnyEmbedding] = None\npc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n# pc.second_embedding = model(pc.tensors.colors)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import PointCloud3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\npoint_cloud: PointCloud3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\npoint_cloud=PointCloud3D(\nurl='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n# or\nmmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n</code></pre> <p>You can display your point cloud from either its url, or its tensors:</p> <pre><code>from docarray.documents import PointCloud3D\n# display from url\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# pc.url.display()\n# display from tensors\npc.tensors = pc.url.load(samples=10000)\n# pc.tensors.display()\n</code></pre> Source code in <code>docarray/documents/point_cloud/point_cloud_3d.py</code> <pre><code>class PointCloud3D(BaseDoc):\n\"\"\"\n    Document for handling point clouds for 3D data representation.\n    Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly\n    sampling points within the surface of the 3D body. Compared to the mesh\n    representation, the point cloud is a fixed size ndarray of shape `(n_samples, 3)` and\n    hence easier for deep learning algorithms to handle.\n    A PointCloud3D Document can contain:\n    - a [`PointCloud3DUrl`][docarray.typing.url.PointCloud3DUrl] (`PointCloud3D.url`)\n    - a [`PointsAndColors`][docarray.documents.point_cloud.points_and_colors.PointsAndColors] object (`PointCloud3D.tensors`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`PointCloud3D.embedding`)\n    - a `bytes` object (`PointCloud3D.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import PointCloud3D\n    # use it directly\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import PointCloud3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyPointCloud3D(PointCloud3D):\n        second_embedding: Optional[AnyEmbedding] = None\n    pc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    # pc.second_embedding = model(pc.tensors.colors)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        point_cloud: PointCloud3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        point_cloud=PointCloud3D(\n            url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n    # or\n    mmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n    ```\n    You can display your point cloud from either its url, or its tensors:\n    ```python\n    from docarray.documents import PointCloud3D\n    # display from url\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # pc.url.display()\n    # display from tensors\n    pc.tensors = pc.url.load(samples=10000)\n    # pc.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[PointCloud3DUrl] = Field(\ndescription='URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[PointsAndColors] = Field(\ndescription='A tensor object of 3D point cloud of type `PointsAndColors`.',\nexample=[[0, 0, 1], [1, 0, 1], [0, 1, 1]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of 3D point cloud.',\nexample=[1, 1, 1],\ndefault=None,\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D point cloud.',\ndefault=None,\n)\n@classmethod\ndef _validate(self, value: Union[str, AbstractTensor, Any]) -&gt; Any:\nif isinstance(value, str):\nvalue = {'url': value}\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = {'tensors': PointsAndColors(points=value)}\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointCloud3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors","title":"<code>PointsAndColors</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>PointCloud3D</code> object.</p> <p>A PointsAndColors Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the points in 3D space information (<code>PointsAndColors.points</code>)</li> <li>an <code>AnyTensor</code> containing the points' color information (<code>PointsAndColors.colors</code>)</li> </ul> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>class PointsAndColors(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`PointCloud3D`][docarray.documents.point_cloud.PointCloud3D] object.\n    A PointsAndColors Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points in 3D space information (`PointsAndColors.points`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points' color information (`PointsAndColors.colors`)\n    \"\"\"\npoints: AnyTensor\ncolors: Optional[AnyTensor] = None\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = cls(points=value)\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot point cloud consisting of points in 3D space and optionally colors.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.display","title":"<code>display()</code>","text":"<p>Plot point cloud consisting of points in 3D space and optionally colors.</p> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot point cloud consisting of points in 3D space and optionally colors.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.PointsAndColors.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d","title":"<code>point_cloud_3d</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D","title":"<code>PointCloud3D</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling point clouds for 3D data representation.</p> <p>Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly sampling points within the surface of the 3D body. Compared to the mesh representation, the point cloud is a fixed size ndarray of shape <code>(n_samples, 3)</code> and hence easier for deep learning algorithms to handle.</p> <p>A PointCloud3D Document can contain:</p> <ul> <li>a <code>PointCloud3DUrl</code> (<code>PointCloud3D.url</code>)</li> <li>a <code>PointsAndColors</code> object (<code>PointCloud3D.tensors</code>)</li> <li>an <code>AnyEmbedding</code> (<code>PointCloud3D.embedding</code>)</li> <li>a <code>bytes</code> object (<code>PointCloud3D.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import PointCloud3D\n# use it directly\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import PointCloud3D\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyPointCloud3D(PointCloud3D):\nsecond_embedding: Optional[AnyEmbedding] = None\npc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\npc.tensors = pc.url.load(samples=100)\n# model = MyEmbeddingModel()\n# pc.embedding = model(pc.tensors.points)\n# pc.second_embedding = model(pc.tensors.colors)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import PointCloud3D, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\npoint_cloud: PointCloud3D\ntext: TextDoc\nmmdoc = MultiModalDoc(\npoint_cloud=PointCloud3D(\nurl='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n# or\nmmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n</code></pre> <p>You can display your point cloud from either its url, or its tensors:</p> <pre><code>from docarray.documents import PointCloud3D\n# display from url\npc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n# pc.url.display()\n# display from tensors\npc.tensors = pc.url.load(samples=10000)\n# pc.tensors.display()\n</code></pre> Source code in <code>docarray/documents/point_cloud/point_cloud_3d.py</code> <pre><code>class PointCloud3D(BaseDoc):\n\"\"\"\n    Document for handling point clouds for 3D data representation.\n    Point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly\n    sampling points within the surface of the 3D body. Compared to the mesh\n    representation, the point cloud is a fixed size ndarray of shape `(n_samples, 3)` and\n    hence easier for deep learning algorithms to handle.\n    A PointCloud3D Document can contain:\n    - a [`PointCloud3DUrl`][docarray.typing.url.PointCloud3DUrl] (`PointCloud3D.url`)\n    - a [`PointsAndColors`][docarray.documents.point_cloud.points_and_colors.PointsAndColors] object (`PointCloud3D.tensors`)\n    - an [`AnyEmbedding`](../../../../api_references/typing/tensor/embedding) (`PointCloud3D.embedding`)\n    - a `bytes` object (`PointCloud3D.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import PointCloud3D\n    # use it directly\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import PointCloud3D\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyPointCloud3D(PointCloud3D):\n        second_embedding: Optional[AnyEmbedding] = None\n    pc = MyPointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    pc.tensors = pc.url.load(samples=100)\n    # model = MyEmbeddingModel()\n    # pc.embedding = model(pc.tensors.points)\n    # pc.second_embedding = model(pc.tensors.colors)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        point_cloud: PointCloud3D\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        point_cloud=PointCloud3D(\n            url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.point_cloud.tensors = mmdoc.point_cloud.url.load(samples=100)\n    # or\n    mmdoc.point_cloud.bytes_ = mmdoc.point_cloud.url.load_bytes()\n    ```\n    You can display your point cloud from either its url, or its tensors:\n    ```python\n    from docarray.documents import PointCloud3D\n    # display from url\n    pc = PointCloud3D(url='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj')\n    # pc.url.display()\n    # display from tensors\n    pc.tensors = pc.url.load(samples=10000)\n    # pc.tensors.display()\n    ```\n    \"\"\"\nurl: Optional[PointCloud3DUrl] = Field(\ndescription='URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.',\nexample='https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj',\ndefault=None,\n)\ntensors: Optional[PointsAndColors] = Field(\ndescription='A tensor object of 3D point cloud of type `PointsAndColors`.',\nexample=[[0, 0, 1], [1, 0, 1], [0, 1, 1]],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of 3D point cloud.',\nexample=[1, 1, 1],\ndefault=None,\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of 3D point cloud.',\ndefault=None,\n)\n@classmethod\ndef _validate(self, value: Union[str, AbstractTensor, Any]) -&gt; Any:\nif isinstance(value, str):\nvalue = {'url': value}\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = {'tensors': PointsAndColors(points=value)}\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.point_cloud_3d.PointCloud3D.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors","title":"<code>points_and_colors</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors","title":"<code>PointsAndColors</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling the tensor data of a <code>PointCloud3D</code> object.</p> <p>A PointsAndColors Document can contain:</p> <ul> <li>an <code>AnyTensor</code> containing the points in 3D space information (<code>PointsAndColors.points</code>)</li> <li>an <code>AnyTensor</code> containing the points' color information (<code>PointsAndColors.colors</code>)</li> </ul> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>class PointsAndColors(BaseDoc):\n\"\"\"\n    Document for handling the tensor data of a [`PointCloud3D`][docarray.documents.point_cloud.PointCloud3D] object.\n    A PointsAndColors Document can contain:\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points in 3D space information (`PointsAndColors.points`)\n    - an [`AnyTensor`](../../../../api_references/typing/tensor/tensor)\n    containing the points' color information (`PointsAndColors.colors`)\n    \"\"\"\npoints: AnyTensor\ncolors: Optional[AnyTensor] = None\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = cls(points=value)\nreturn super().validate(value)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot point cloud consisting of points in 3D space and optionally colors.\n        \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.display","title":"<code>display()</code>","text":"<p>Plot point cloud consisting of points in 3D space and optionally colors.</p> Source code in <code>docarray/documents/point_cloud/points_and_colors.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot point cloud consisting of points in 3D space and optionally colors.\n    \"\"\"\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nfrom IPython.display import display\ncolors = (\nself.colors\nif self.colors is not None\nelse np.tile(\nnp.array([0, 0, 0]),\n(self.points.get_comp_backend().shape(self.points)[0], 1),\n)\n)\npc = trimesh.points.PointCloud(vertices=self.points, colors=colors)\ns = trimesh.Scene(geometry=pc)\ndisplay(s.show())\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.point_cloud.points_and_colors.PointsAndColors.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text","title":"<code>text</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc","title":"<code>TextDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling text.</p> <p>It can contain:</p> <ul> <li>a <code>TextUrl</code> (<code>TextDoc.url</code>)</li> <li>a <code>str</code> (<code>TextDoc.text</code>)</li> <li>an <code>AnyEmbedding</code> (<code>TextDoc.embedding</code>)</li> <li>a <code>bytes</code> object (<code>TextDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import TextDoc\n# use it directly\ntxt_doc = TextDoc(url='https://www.gutenberg.org/files/1065/1065-0.txt')\ntxt_doc.text = txt_doc.url.load()\n# model = MyEmbeddingModel()\n# txt_doc.embedding = model(txt_doc.text)\n</code></pre> <p>You can initialize directly from a string:</p> <pre><code>from docarray.documents import TextDoc\ntxt_doc = TextDoc('hello world')\n</code></pre> <p>You can extend this Document:</p> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extend it\nclass MyText(TextDoc):\nsecond_embedding: Optional[AnyEmbedding] = None\ntxt_doc = MyText(url='https://www.gutenberg.org/files/1065/1065-0.txt')\ntxt_doc.text = txt_doc.url.load()\n# model = MyEmbeddingModel()\n# txt_doc.embedding = model(txt_doc.text)\n# txt_doc.second_embedding = model(txt_doc.text)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc, TextDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nimage_doc: ImageDoc\ntext_doc: TextDoc\nmmdoc = MultiModalDoc(\nimage_doc=ImageDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n),\ntext_doc=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.image_doc.tensor = mmdoc.image_doc.url.load()\n# or\nmmdoc.image_doc.bytes_ = mmdoc.image_doc.url.load_bytes()\nmmdoc.image_doc.tensor = mmdoc.image_doc.bytes_.load()\n</code></pre> <p>This Document can be compared against another Document of the same type or a string. When compared against another object of the same type, the pydantic BaseModel equality check will apply which checks the equality of every attribute, excluding <code>id</code>. When compared against a str, it will check the equality of the <code>text</code> attribute against the given string.</p> <pre><code>from docarray.documents import TextDoc\ndoc = TextDoc(text='This is the main text', url='exampleurl.com/file')\ndoc2 = TextDoc(text='This is the main text', url='exampleurl.com/file')\ndoc == 'This is the main text'  # True\ndoc == doc2  # True\n</code></pre> Source code in <code>docarray/documents/text.py</code> <pre><code>class TextDoc(BaseDoc):\n\"\"\"\n    Document for handling text.\n    It can contain:\n    - a [`TextUrl`][docarray.typing.url.TextUrl] (`TextDoc.url`)\n    - a `str` (`TextDoc.text`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`TextDoc.embedding`)\n    - a `bytes` object (`TextDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import TextDoc\n    # use it directly\n    txt_doc = TextDoc(url='https://www.gutenberg.org/files/1065/1065-0.txt')\n    txt_doc.text = txt_doc.url.load()\n    # model = MyEmbeddingModel()\n    # txt_doc.embedding = model(txt_doc.text)\n    ```\n    You can initialize directly from a string:\n    ```python\n    from docarray.documents import TextDoc\n    txt_doc = TextDoc('hello world')\n    ```\n    You can extend this Document:\n    ```python\n    from docarray.documents import TextDoc\n    from docarray.typing import AnyEmbedding\n    from typing import Optional\n    # extend it\n    class MyText(TextDoc):\n        second_embedding: Optional[AnyEmbedding] = None\n    txt_doc = MyText(url='https://www.gutenberg.org/files/1065/1065-0.txt')\n    txt_doc.text = txt_doc.url.load()\n    # model = MyEmbeddingModel()\n    # txt_doc.embedding = model(txt_doc.text)\n    # txt_doc.second_embedding = model(txt_doc.text)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import ImageDoc, TextDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        image_doc: ImageDoc\n        text_doc: TextDoc\n    mmdoc = MultiModalDoc(\n        image_doc=ImageDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n        ),\n        text_doc=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.image_doc.tensor = mmdoc.image_doc.url.load()\n    # or\n    mmdoc.image_doc.bytes_ = mmdoc.image_doc.url.load_bytes()\n    mmdoc.image_doc.tensor = mmdoc.image_doc.bytes_.load()\n    ```\n    This Document can be compared against another Document of the same type or a string.\n    When compared against another object of the same type, the pydantic BaseModel\n    equality check will apply which checks the equality of every attribute,\n    excluding `id`. When compared against a str, it will check the equality\n    of the `text` attribute against the given string.\n    ```python\n    from docarray.documents import TextDoc\n    doc = TextDoc(text='This is the main text', url='exampleurl.com/file')\n    doc2 = TextDoc(text='This is the main text', url='exampleurl.com/file')\n    doc == 'This is the main text'  # True\n    doc == doc2  # True\n    ```\n    \"\"\"\ntext: Optional[str] = Field(\ndescription='The text content stored in the document',\nexample='This is an example text content of the document',\ndefault=None,\n)\nurl: Optional[TextUrl] = Field(\ndescription='URL to a (potentially remote) text file that can be loaded',\nexample='https://www.w3.org/History/19921103-hypertext/hypertext/README.html',\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the text',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[bytes] = Field(\ndescription='Bytes representation of the text',\ndefault=None,\n)\ndef __init__(self, text: Optional[str] = None, **kwargs):\nif 'text' not in kwargs:\nkwargs['text'] = text\nsuper().__init__(**kwargs)\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, values):\nif isinstance(values, str):\nreturn {'text': values}\nelse:\nreturn values\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = cls(text=value)\nreturn super().validate(value)\ndef __eq__(self, other: Any) -&gt; bool:\nif isinstance(other, str):\nreturn self.text == other\nelse:\n# BaseModel has a default equality\nreturn super().__eq__(other)\ndef __contains__(self, item: str) -&gt; bool:\n\"\"\"\n        This method makes `TextDoc` behave the same as an `str`.\n        :param item: A string to be checked if is a substring of `text` attribute\n        :return: A boolean determining the presence of `item` as a substring in `text`\n        ```python\n        from docarray.documents import TextDoc\n        t = TextDoc(text='this is my text document')\n        assert 'text' in t\n        assert 'docarray' not in t\n        ```\n        \"\"\"\nif self.text is not None:\nreturn self.text.__contains__(item)\nelse:\nreturn False\ndef _get_string_for_regex_filter(self):\nreturn self.text\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.__contains__","title":"<code>__contains__(item)</code>","text":"<p>This method makes <code>TextDoc</code> behave the same as an <code>str</code>.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>A string to be checked if is a substring of <code>text</code> attribute</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean determining the presence of <code>item</code> as a substring in <code>text</code> <code>python from docarray.documents import TextDoc  t = TextDoc(text='this is my text document') assert 'text' in t assert 'docarray' not in t</code></p> Source code in <code>docarray/documents/text.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n\"\"\"\n    This method makes `TextDoc` behave the same as an `str`.\n    :param item: A string to be checked if is a substring of `text` attribute\n    :return: A boolean determining the presence of `item` as a substring in `text`\n    ```python\n    from docarray.documents import TextDoc\n    t = TextDoc(text='this is my text document')\n    assert 'text' in t\n    assert 'docarray' not in t\n    ```\n    \"\"\"\nif self.text is not None:\nreturn self.text.__contains__(item)\nelse:\nreturn False\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.text.TextDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video","title":"<code>video</code>","text":""},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc","title":"<code>VideoDoc</code>","text":"<p>             Bases: <code>BaseDoc</code></p> <p>Document for handling video.</p> <p>The Video Document can contain:</p> <ul> <li>a <code>VideoUrl</code> (<code>VideoDoc.url</code>)</li> <li>an <code>AudioDoc</code> (<code>VideoDoc.audio</code>)</li> <li>a <code>VideoTensor</code> (<code>VideoDoc.tensor</code>)</li> <li>an <code>AnyTensor</code> representing the indices of the video's key frames (<code>VideoDoc.key_frame_indices</code>)</li> <li>an <code>AnyEmbedding</code> (<code>VideoDoc.embedding</code>)</li> <li>a <code>VideoBytes</code> object (<code>VideoDoc.bytes_</code>)</li> </ul> <p>You can use this Document directly:</p> <pre><code>from docarray.documents import VideoDoc, AudioDoc\n# use it directly\nvid = VideoDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ntensor, audio_tensor, key_frame_indices = vid.url.load()\nvid.tensor = tensor\nvid.audio = AudioDoc(tensor=audio_tensor)\nvid.key_frame_indices = key_frame_indices\n# model = MyEmbeddingModel()\n# vid.embedding = model(vid.tensor)\n</code></pre> <p>You can extend this Document:</p> <pre><code>from typing import Optional\nfrom docarray.documents import TextDoc, VideoDoc\n# extend it\nclass MyVideo(VideoDoc):\nname: Optional[TextDoc] = None\nvideo = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\nvideo.name = TextDoc(text='my first video')\nvideo.tensor = video.url.load().video\n# model = MyEmbeddingModel()\n# video.embedding = model(video.tensor)\n</code></pre> <p>You can use this Document for composition:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import TextDoc, VideoDoc\n# compose it\nclass MultiModalDoc(BaseDoc):\nvideo: VideoDoc\ntext: TextDoc\nmmdoc = MultiModalDoc(\nvideo=VideoDoc(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n),\ntext=TextDoc(text='hello world, how are you doing?'),\n)\nmmdoc.video.tensor = mmdoc.video.url.load().video\n# or\nmmdoc.video.bytes_ = mmdoc.video.url.load_bytes()\nmmdoc.video.tensor = mmdoc.video.bytes_.load().video\n</code></pre> Source code in <code>docarray/documents/video.py</code> <pre><code>class VideoDoc(BaseDoc):\n\"\"\"\n    Document for handling video.\n    The Video Document can contain:\n    - a [`VideoUrl`][docarray.typing.url.VideoUrl] (`VideoDoc.url`)\n    - an [`AudioDoc`][docarray.documents.AudioDoc] (`VideoDoc.audio`)\n    - a [`VideoTensor`](../../../api_references/typing/tensor/video) (`VideoDoc.tensor`)\n    - an [`AnyTensor`](../../../api_references/typing/tensor/tensor) representing the indices of the video's key frames (`VideoDoc.key_frame_indices`)\n    - an [`AnyEmbedding`](../../../api_references/typing/tensor/embedding) (`VideoDoc.embedding`)\n    - a [`VideoBytes`][docarray.typing.bytes.VideoBytes] object (`VideoDoc.bytes_`)\n    You can use this Document directly:\n    ```python\n    from docarray.documents import VideoDoc, AudioDoc\n    # use it directly\n    vid = VideoDoc(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    tensor, audio_tensor, key_frame_indices = vid.url.load()\n    vid.tensor = tensor\n    vid.audio = AudioDoc(tensor=audio_tensor)\n    vid.key_frame_indices = key_frame_indices\n    # model = MyEmbeddingModel()\n    # vid.embedding = model(vid.tensor)\n    ```\n    You can extend this Document:\n    ```python\n    from typing import Optional\n    from docarray.documents import TextDoc, VideoDoc\n    # extend it\n    class MyVideo(VideoDoc):\n        name: Optional[TextDoc] = None\n    video = MyVideo(\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    video.name = TextDoc(text='my first video')\n    video.tensor = video.url.load().video\n    # model = MyEmbeddingModel()\n    # video.embedding = model(video.tensor)\n    ```\n    You can use this Document for composition:\n    ```python\n    from docarray import BaseDoc\n    from docarray.documents import TextDoc, VideoDoc\n    # compose it\n    class MultiModalDoc(BaseDoc):\n        video: VideoDoc\n        text: TextDoc\n    mmdoc = MultiModalDoc(\n        video=VideoDoc(\n            url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        ),\n        text=TextDoc(text='hello world, how are you doing?'),\n    )\n    mmdoc.video.tensor = mmdoc.video.url.load().video\n    # or\n    mmdoc.video.bytes_ = mmdoc.video.url.load_bytes()\n    mmdoc.video.tensor = mmdoc.video.bytes_.load().video\n    ```\n    \"\"\"\nurl: Optional[VideoUrl] = Field(\ndescription='URL to a (potentially remote) video file that needs to be loaded',\nexample='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\ndefault=None,\n)\naudio: Optional[AudioDoc] = Field(\ndescription='Audio document associated with the video',\ndefault=None,\n)\ntensor: Optional[VideoTensor] = Field(\ndescription='Tensor object representing the video which be specified to one of `VideoNdArray`, `VideoTorchTensor`, `VideoTensorFlowTensor`',\ndefault=None,\n)\nkey_frame_indices: Optional[AnyTensor] = Field(\ndescription='List of all the key frames in the video',\nexample=[0, 1, 2, 3, 4],\ndefault=None,\n)\nembedding: Optional[AnyEmbedding] = Field(\ndescription='Store an embedding: a vector representation of the video',\nexample=[1, 0, 1],\ndefault=None,\n)\nbytes_: Optional[VideoBytes] = Field(\ndescription='Bytes representation of the video',\ndefault=None,\n)\n@classmethod\ndef _validate(cls, value) -&gt; Dict[str, Any]:\nif isinstance(value, str):\nvalue = dict(url=value)\nelif isinstance(value, (AbstractTensor, np.ndarray)) or (\ntorch is not None\nand isinstance(value, torch.Tensor)\nor (tf is not None and isinstance(value, tf.Tensor))\n):\nvalue = dict(tensor=value)\nreturn value\nif is_pydantic_v2:\n@model_validator(mode='before')\n@classmethod\ndef validate_model_before(cls, value):\nreturn cls._validate(value)\nelse:\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[str, AbstractTensor, Any],\n) -&gt; T:\nreturn super().validate(cls._validate(value))\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def dict(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\n) -&gt; 'DictStrAny':\n\"\"\"\n    Generate a dictionary representation of the model, optionally specifying\n    which fields to include or exclude.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_doclist(\nexclude=exclude\n)\ndata = super().dict(\ninclude=include,\nexclude=exclude,\nby_alias=by_alias,\nskip_defaults=skip_defaults,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\nval = getattr(self, field)\ndata[field] = (\n[doc.dict() for doc in val] if val is not None else None\n)\nreturn data\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.from_base64","title":"<code>from_base64(data, protocol='pickle', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>a base64 encoded string</p> required <code>protocol</code> <code>Literal['pickle', 'protobuf']</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'pickle'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_base64(\ncls: Type[T],\ndata: str,\nprotocol: Literal['pickle', 'protobuf'] = 'pickle',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: a base64 encoded string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nreturn cls.from_bytes(base64.b64decode(data), protocol, compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.from_bytes","title":"<code>from_bytes(data, protocol='protobuf', compress=None)</code>  <code>classmethod</code>","text":"<p>Build Document object from binary bytes</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>binary bytes</p> required <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_bytes(\ncls: Type[T],\ndata: bytes,\nprotocol: ProtocolType = 'protobuf',\ncompress: Optional[str] = None,\n) -&gt; T:\n\"\"\"Build Document object from binary bytes\n    :param data: binary bytes\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a Document object\n    \"\"\"\nbstr = _decompress_bytes(data, algorithm=compress)\nif protocol == 'pickle':\nreturn pickle.loads(bstr)\nelif protocol == 'protobuf':\nfrom docarray.proto import DocProto\npb_msg = DocProto()\npb_msg.ParseFromString(bstr)\nreturn cls.from_protobuf(pb_msg)\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Build Document object from json data</p> <p>Returns:</p> Type Description <code>T</code> <p>a Document object</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_json(\ncls: Type[T],\ndata: str,\n) -&gt; T:\n\"\"\"Build Document object from json data\n    :return: a Document object\n    \"\"\"\n# TODO: add tests\nif is_pydantic_v2:\nreturn cls.model_validate_json(data)\nelse:\nreturn cls.parse_raw(data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>create a Document from a protobuf message</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>DocProto</code> <p>the proto message of the Document</p> required <p>Returns:</p> Type Description <code>T</code> <p>a Document initialize with the proto data</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'DocProto') -&gt; T:\n\"\"\"create a Document from a protobuf message\n    :param pb_msg: the proto message of the Document\n    :return: a Document initialize with the proto data\n    \"\"\"\nfields: Dict[str, Any] = {}\nload_extra_field = (\ncls.model_config['_load_extra_fields_from_protobuf']\nif is_pydantic_v2\nelse cls.Config._load_extra_fields_from_protobuf\n)\nfor field_name in pb_msg.data:\nif (\nnot (load_extra_field)\nand field_name not in cls._docarray_fields().keys()\n):\ncontinue  # optimization we don't even load the data if the key does not\n# match any field in the cls or in the mapping\nfields[field_name] = cls._get_content_from_node_proto(\npb_msg.data[field_name], field_name\n)\nreturn cls(**fields)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.json","title":"<code>json(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False, encoder=None, models_as_dict=True, **dumps_kwargs)</code>","text":"<p>Generate a JSON representation of the model, <code>include</code> and <code>exclude</code> arguments as per <code>dict()</code>.</p> <p><code>encoder</code> is an optional function to supply as <code>default</code> to json.dumps(), other arguments as per <code>json.dumps()</code>.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def json(\nself,\n*,\ninclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\nexclude: ExcludeType = None,\nby_alias: bool = False,\nskip_defaults: Optional[bool] = None,\nexclude_unset: bool = False,\nexclude_defaults: bool = False,\nexclude_none: bool = False,\nencoder: Optional[Callable[[Any], Any]] = None,\nmodels_as_dict: bool = True,\n**dumps_kwargs: Any,\n) -&gt; str:\n\"\"\"\n    Generate a JSON representation of the model, `include` and `exclude`\n    arguments as per `dict()`.\n    `encoder` is an optional function to supply as `default` to json.dumps(),\n    other arguments as per `json.dumps()`.\n    \"\"\"\nexclude, original_exclude, doclist_exclude_fields = self._exclude_docarray(\nexclude=exclude\n)\n# this is copy from pydantic code\nif skip_defaults is not None:\nwarnings.warn(\nf'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\nDeprecationWarning,\n)\nexclude_unset = skip_defaults\nencoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n# We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n# because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n# This allows users to write custom JSON encoders for given `BaseModel` classes.\ndata = dict(\nself._iter(\nto_dict=models_as_dict,\nby_alias=by_alias,\ninclude=include,\nexclude=exclude,\nexclude_unset=exclude_unset,\nexclude_defaults=exclude_defaults,\nexclude_none=exclude_none,\n)\n)\n# this is the custom part to deal with DocList\nfor field in doclist_exclude_fields:\n# we need to do this because pydantic will not recognize DocList correctly\noriginal_exclude = original_exclude or {}\nif field not in original_exclude:\ndata[field] = getattr(\nself, field\n)  # here we need to keep doclist as doclist otherwise if a user want to have a special json config it will not work\n# this is copy from pydantic code\nif self.__custom_root_type__:\ndata = data[ROOT_KEY]\nreturn self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.parse_raw","title":"<code>parse_raw(b, *, content_type=None, encoding='utf8', proto=None, allow_pickle=False)</code>  <code>classmethod</code>","text":"<p>Parse a raw string or bytes into a base doc</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>StrBytes</code> required <code>content_type</code> <code>str</code> <code>None</code> <code>encoding</code> <code>str</code> <p>the encoding to use when parsing a string, defaults to 'utf8'</p> <code>'utf8'</code> <code>proto</code> <code>Protocol</code> <p>protocol to use.</p> <code>None</code> <code>allow_pickle</code> <code>bool</code> <p>allow pickle protocol</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>a document</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@no_type_check\n@classmethod\ndef parse_raw(\ncls: Type[T],\nb: 'StrBytes',\n*,\ncontent_type: str = None,\nencoding: str = 'utf8',\nproto: 'Protocol' = None,\nallow_pickle: bool = False,\n) -&gt; T:\n\"\"\"\n    Parse a raw string or bytes into a base doc\n    :param b:\n    :param content_type:\n    :param encoding: the encoding to use when parsing a string, defaults to 'utf8'\n    :param proto: protocol to use.\n    :param allow_pickle: allow pickle protocol\n    :return: a document\n    \"\"\"\nreturn super(BaseDocWithoutId, cls).parse_raw(\nb,\ncontent_type=content_type,\nencoding=encoding,\nproto=proto,\nallow_pickle=allow_pickle,\n)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.schema_summary","title":"<code>schema_summary()</code>  <code>classmethod</code>","text":"<p>Print a summary of the Documents schema.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>@classmethod\ndef schema_summary(cls) -&gt; None:\n\"\"\"Print a summary of the Documents schema.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary.schema_summary(cls)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.summary","title":"<code>summary()</code>","text":"<p>Print non-empty fields and nested structure of this Document object.</p> Source code in <code>docarray/base_doc/doc.py</code> <pre><code>def summary(self) -&gt; None:\n\"\"\"Print non-empty fields and nested structure of this Document object.\"\"\"\nfrom docarray.display.document_summary import DocumentSummary\nDocumentSummary(doc=self).summary()\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.to_base64","title":"<code>to_base64(protocol='protobuf', compress=None)</code>","text":"<p>Serialize a Document object into as base64 string</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compress method to use</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a base64 encoded string</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_base64(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; str:\n\"\"\"Serialize a Document object into as base64 string\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compress method to use\n    :return: a base64 encoded string\n    \"\"\"\nreturn base64.b64encode(self.to_bytes(protocol, compress)).decode('utf-8')\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.to_bytes","title":"<code>to_bytes(protocol='protobuf', compress=None)</code>","text":"<p>Serialize itself into bytes.</p> <p>For more Pythonic code, please use <code>bytes(...)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>protocol</code> <code>ProtocolType</code> <p>protocol to use. It can be 'pickle' or 'protobuf'</p> <code>'protobuf'</code> <code>compress</code> <code>Optional[str]</code> <p>compression algorithm to use</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>the binary serialization in bytes</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_bytes(\nself, protocol: ProtocolType = 'protobuf', compress: Optional[str] = None\n) -&gt; bytes:\n\"\"\"Serialize itself into bytes.\n    For more Pythonic code, please use ``bytes(...)``.\n    :param protocol: protocol to use. It can be 'pickle' or 'protobuf'\n    :param compress: compression algorithm to use\n    :return: the binary serialization in bytes\n    \"\"\"\nimport pickle\nif protocol == 'pickle':\nbstr = pickle.dumps(self)\nelif protocol == 'protobuf':\nbstr = self.to_protobuf().SerializePartialToString()\nelse:\nraise ValueError(\nf'protocol={protocol} is not supported. Can be only `protobuf` or '\nf'pickle protocols 0-5.'\n)\nreturn _compress_bytes(bstr, algorithm=compress)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Convert Document into a Protobuf message.</p> <p>Returns:</p> Type Description <code>DocProto</code> <p>the protobuf message</p> Source code in <code>docarray/base_doc/mixins/io.py</code> <pre><code>def to_protobuf(self: T) -&gt; 'DocProto':\n\"\"\"Convert Document into a Protobuf message.\n    :return: the protobuf message\n    \"\"\"\nfrom docarray.proto import DocProto\ndata = {}\nfor field, value in self:\ntry:\ndata[field] = _type_to_protobuf(value)\nexcept RecursionError as ex:\nif len(ex.args) &gt;= 1:\nex.args = (\n(\nf'Field `{field}` contains cyclic reference in memory. '\n'Could it be your Document is referring to itself?'\n),\n)\nraise ex\nexcept Exception as ex:\nif len(ex.args) &gt;= 1:\nex.args = (f'Field `{field}` is problematic',) + ex.args\nraise ex\nreturn DocProto(data=data)\n</code></pre>"},{"location":"API_reference/documents/documents/#docarray.documents.video.VideoDoc.update","title":"<code>update(other)</code>","text":"<p>Updates self with the content of other. Changes are applied to self. Updating one Document with another consists in the following:</p> <ul> <li>Setting data properties of the second Document to the first Document  if they are not None</li> <li>Concatenating lists and updating sets</li> <li>Updating recursively Documents and DocLists</li> <li>Updating Dictionaries of the left with the right</li> </ul> <p>It behaves as an update operation for Dictionaries, except that since it is applied to a static schema type, the presence of the field is given by the field not having a None value and that DocLists, lists and sets are concatenated. It is worth mentioning that Tuples are not merged together since they are meant to be immutable, so they behave as regular types and the value of <code>self</code> is updated with the value of <code>other</code>.</p> <pre><code>from typing import List, Optional\nfrom docarray import BaseDoc\nclass MyDocument(BaseDoc):\ncontent: str\ntitle: Optional[str] = None\ntags_: List\ndoc1 = MyDocument(\ncontent='Core content of the document', title='Title', tags_=['python', 'AI']\n)\ndoc2 = MyDocument(content='Core content updated', tags_=['docarray'])\ndoc1.update(doc2)\nassert doc1.content == 'Core content updated'\nassert doc1.title == 'Title'\nassert doc1.tags_ == ['python', 'AI', 'docarray']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>T</code> <p>The Document with which to update the contents of this</p> required Source code in <code>docarray/base_doc/mixins/update.py</code> <pre><code>def update(self, other: T):\n\"\"\"\n    Updates self with the content of other. Changes are applied to self.\n    Updating one Document with another consists in the following:\n     - Setting data properties of the second Document to the first Document\n     if they are not None\n     - Concatenating lists and updating sets\n     - Updating recursively Documents and DocLists\n     - Updating Dictionaries of the left with the right\n    It behaves as an update operation for Dictionaries, except that since\n    it is applied to a static schema type, the presence of the field is\n    given by the field not having a None value and that DocLists,\n    lists and sets are concatenated. It is worth mentioning that Tuples\n    are not merged together since they are meant to be immutable,\n    so they behave as regular types and the value of `self` is updated\n    with the value of `other`.\n    ---\n    ```python\n    from typing import List, Optional\n    from docarray import BaseDoc\n    class MyDocument(BaseDoc):\n        content: str\n        title: Optional[str] = None\n        tags_: List\n    doc1 = MyDocument(\n        content='Core content of the document', title='Title', tags_=['python', 'AI']\n    )\n    doc2 = MyDocument(content='Core content updated', tags_=['docarray'])\n    doc1.update(doc2)\n    assert doc1.content == 'Core content updated'\n    assert doc1.title == 'Title'\n    assert doc1.tags_ == ['python', 'AI', 'docarray']\n    ```\n    ---\n    :param other: The Document with which to update the contents of this\n    \"\"\"\nif not _similar_schemas(self, other):\nraise Exception(\nf'Update operation can only be applied to '\nf'Documents of the same schema. '\nf'Trying to update Document of type '\nf'{type(self)} with Document of type '\nf'{type(other)}'\n)\nfrom collections import namedtuple\nfrom docarray import DocList\nfrom docarray.utils.reduce import reduce\n# Declaring namedtuple()\n_FieldGroups = namedtuple(\n'_FieldGroups',\n[\n'simple_non_empty_fields',\n'list_fields',\n'set_fields',\n'dict_fields',\n'nested_docarray_fields',\n'nested_docs_fields',\n],\n)\nFORBIDDEN_FIELDS_TO_UPDATE = ['ID']\ndef _group_fields(doc: 'UpdateMixin') -&gt; _FieldGroups:\nsimple_non_empty_fields: List[str] = []\nlist_fields: List[str] = []\nset_fields: List[str] = []\ndict_fields: List[str] = []\nnested_docs_fields: List[str] = []\nnested_docarray_fields: List[str] = []\nfor field_name, field in doc._docarray_fields().items():\nif field_name not in FORBIDDEN_FIELDS_TO_UPDATE:\nfield_type = doc._get_field_annotation(field_name)\nif isinstance(field_type, type) and safe_issubclass(\nfield_type, DocList\n):\nnested_docarray_fields.append(field_name)\nelse:\norigin = get_origin(field_type)\nif origin is list:\nlist_fields.append(field_name)\nelif origin is set:\nset_fields.append(field_name)\nelif origin is dict:\ndict_fields.append(field_name)\nelse:\nv = getattr(doc, field_name)\nif v is not None:\nif isinstance(v, UpdateMixin):\nnested_docs_fields.append(field_name)\nelse:\nsimple_non_empty_fields.append(field_name)\nreturn _FieldGroups(\nsimple_non_empty_fields,\nlist_fields,\nset_fields,\ndict_fields,\nnested_docarray_fields,\nnested_docs_fields,\n)\ndoc1_fields = _group_fields(self)\ndoc2_fields = _group_fields(other)\nfor field in doc2_fields.simple_non_empty_fields:\nsetattr(self, field, getattr(other, field))\nfor field in set(\ndoc1_fields.nested_docs_fields + doc2_fields.nested_docs_fields\n):\nsub_doc_1: T = getattr(self, field)\nsub_doc_2: T = getattr(other, field)\nsub_doc_1.update(sub_doc_2)\nsetattr(self, field, sub_doc_1)\nfor field in set(doc1_fields.list_fields + doc2_fields.list_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.extend(array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.set_fields + doc2_fields.set_fields):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1.update(array2)\nsetattr(self, field, array1)\nfor field in set(\ndoc1_fields.nested_docarray_fields + doc2_fields.nested_docarray_fields\n):\narray1 = getattr(self, field)\narray2 = getattr(other, field)\nif array1 is None and array2 is not None:\nsetattr(self, field, array2)\nelif array1 is not None and array2 is not None:\narray1 = reduce(array1, array2)\nsetattr(self, field, array1)\nfor field in set(doc1_fields.dict_fields + doc2_fields.dict_fields):\ndict1 = getattr(self, field)\ndict2 = getattr(other, field)\nif dict1 is None and dict2 is not None:\nsetattr(self, field, dict2)\nelif dict1 is not None and dict2 is not None:\ndict1.update(dict2)\nsetattr(self, field, dict1)\n</code></pre>"},{"location":"API_reference/typing/bytes/","title":"Bytes","text":""},{"location":"API_reference/typing/bytes/#bytes","title":"Bytes","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes","title":"<code>docarray.typing.bytes</code>","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.AudioBytes","title":"<code>AudioBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store an audio and that can be load into an Audio tensor</p> Source code in <code>docarray/typing/bytes/audio_bytes.py</code> <pre><code>@_register_proto(proto_type_name='audio_bytes')\nclass AudioBytes(BaseBytes):\n\"\"\"\n    Bytes that store an audio and that can be load into an Audio tensor\n    \"\"\"\ndef load(self) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n        Load the Audio from the [`AudioBytes`][docarray.typing.AudioBytes] into an\n        [`AudioNdArray`][docarray.typing.AudioNdArray].\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import AudioBytes, AudioNdArray, AudioUrl\n        class MyAudio(BaseDoc):\n            url: AudioUrl\n            tensor: Optional[AudioNdArray] = None\n            bytes_: Optional[AudioBytes] = None\n            frame_rate: Optional[float] = None\n        doc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\n        doc.bytes_ = doc.url.load_bytes()\n        doc.tensor, doc.frame_rate = doc.bytes_.load()\n        # Note this is equivalent to do\n        doc.tensor, doc.frame_rate = doc.url.load()\n        assert isinstance(doc.tensor, AudioNdArray)\n        ```\n        ---\n        :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing the\n            audio bytes content, and an integer representing the frame rate.\n        \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\nsegment = AudioSegment.from_file(io.BytesIO(self))\n# Convert to float32 using NumPy\nsamples = np.array(segment.get_array_of_samples())\n# Normalise float32 array so that values are between -1.0 and +1.0\nsamples_norm = samples / 2 ** (segment.sample_width * 8 - 1)\nreturn parse_obj_as(AudioNdArray, samples_norm), segment.frame_rate\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.AudioBytes.load","title":"<code>load()</code>","text":"<p>Load the Audio from the <code>AudioBytes</code> into an <code>AudioNdArray</code>.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioNdArray, AudioUrl\nclass MyAudio(BaseDoc):\nurl: AudioUrl\ntensor: Optional[AudioNdArray] = None\nbytes_: Optional[AudioBytes] = None\nframe_rate: Optional[float] = None\ndoc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\ndoc.bytes_ = doc.url.load_bytes()\ndoc.tensor, doc.frame_rate = doc.bytes_.load()\n# Note this is equivalent to do\ndoc.tensor, doc.frame_rate = doc.url.load()\nassert isinstance(doc.tensor, AudioNdArray)\n</code></pre> <p>Returns:</p> Type Description <code>Tuple[AudioNdArray, int]</code> <p>tuple of an <code>AudioNdArray</code> representing the audio bytes content, and an integer representing the frame rate.</p> Source code in <code>docarray/typing/bytes/audio_bytes.py</code> <pre><code>def load(self) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n    Load the Audio from the [`AudioBytes`][docarray.typing.AudioBytes] into an\n    [`AudioNdArray`][docarray.typing.AudioNdArray].\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import AudioBytes, AudioNdArray, AudioUrl\n    class MyAudio(BaseDoc):\n        url: AudioUrl\n        tensor: Optional[AudioNdArray] = None\n        bytes_: Optional[AudioBytes] = None\n        frame_rate: Optional[float] = None\n    doc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\n    doc.bytes_ = doc.url.load_bytes()\n    doc.tensor, doc.frame_rate = doc.bytes_.load()\n    # Note this is equivalent to do\n    doc.tensor, doc.frame_rate = doc.url.load()\n    assert isinstance(doc.tensor, AudioNdArray)\n    ```\n    ---\n    :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing the\n        audio bytes content, and an integer representing the frame rate.\n    \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\nsegment = AudioSegment.from_file(io.BytesIO(self))\n# Convert to float32 using NumPy\nsamples = np.array(segment.get_array_of_samples())\n# Normalise float32 array so that values are between -1.0 and +1.0\nsamples_norm = samples / 2 ** (segment.sample_width * 8 - 1)\nreturn parse_obj_as(AudioNdArray, samples_norm), segment.frame_rate\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.ImageBytes","title":"<code>ImageBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store an image and that can be load into an image tensor</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>@_register_proto(proto_type_name='image_bytes')\nclass ImageBytes(BaseBytes):\n\"\"\"\n    Bytes that store an image and that can be load into an image tensor\n    \"\"\"\ndef load_pil(\nself,\n) -&gt; 'PILImage.Image':\n\"\"\"\n        Load the image from the bytes into a `PIL.Image.Image` instance\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl\n        img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        img_url = parse_obj_as(ImageUrl, img_url)\n        img = img_url.load_pil()\n        from PIL.Image import Image\n        assert isinstance(img, Image)\n        ```\n        ---\n        :return: a Pillow image\n        \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nreturn PILImage.open(BytesIO(self))\ndef load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\n) -&gt; ImageNdArray:\n\"\"\"\n        Load the image from the [`ImageBytes`][docarray.typing.ImageBytes] into an\n        [`ImageNdArray`][docarray.typing.ImageNdArray].\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import ImageNdArray, ImageUrl\n        class MyDoc(BaseDoc):\n            img_url: ImageUrl\n        doc = MyDoc(\n            img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n            \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        )\n        img_tensor = doc.img_url.load()\n        assert isinstance(img_tensor, ImageNdArray)\n        img_tensor = doc.img_url.load(height=224, width=224)\n        assert img_tensor.shape == (224, 224, 3)\n        layout = ('C', 'W', 'H')\n        img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n        assert img_tensor.shape == (3, 200, 100)\n        ```\n        ---\n        :param width: width of the image tensor.\n        :param height: height of the image tensor.\n        :param axis_layout: ordering of the different image axes.\n            'H' = height, 'W' = width, 'C' = color channel\n        :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n        \"\"\"\nraw_img = self.load_pil()\nif width or height:\nnew_width = width or raw_img.width\nnew_height = height or raw_img.height\nraw_img = raw_img.resize((new_width, new_height))\ntry:\ntensor = np.array(raw_img.convert('RGB'))\nexcept Exception:\ntensor = np.array(raw_img)\nimg = self._move_channel_axis(tensor, axis_layout=axis_layout)\nreturn parse_obj_as(ImageNdArray, img)\n@staticmethod\ndef _move_channel_axis(\ntensor: np.ndarray, axis_layout: Tuple[str, str, str] = ('H', 'W', 'C')\n) -&gt; np.ndarray:\n\"\"\"Moves channel axis around.\"\"\"\nchannel_to_offset = {'H': 0, 'W': 1, 'C': 2}\npermutation = tuple(channel_to_offset[axis] for axis in axis_layout)\nreturn np.transpose(tensor, permutation)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.ImageBytes.load","title":"<code>load(width=None, height=None, axis_layout=('H', 'W', 'C'))</code>","text":"<p>Load the image from the <code>ImageBytes</code> into an <code>ImageNdArray</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageNdArray, ImageUrl\nclass MyDoc(BaseDoc):\nimg_url: ImageUrl\ndoc = MyDoc(\nimg_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n)\nimg_tensor = doc.img_url.load()\nassert isinstance(img_tensor, ImageNdArray)\nimg_tensor = doc.img_url.load(height=224, width=224)\nassert img_tensor.shape == (224, 224, 3)\nlayout = ('C', 'W', 'H')\nimg_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\nassert img_tensor.shape == (3, 200, 100)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[int]</code> <p>width of the image tensor.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>height of the image tensor.</p> <code>None</code> <code>axis_layout</code> <code>Tuple[str, str, str]</code> <p>ordering of the different image axes. 'H' = height, 'W' = width, 'C' = color channel</p> <code>('H', 'W', 'C')</code> <p>Returns:</p> Type Description <code>ImageNdArray</code> <p><code>ImageNdArray</code> representing the image as RGB values</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>def load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\n) -&gt; ImageNdArray:\n\"\"\"\n    Load the image from the [`ImageBytes`][docarray.typing.ImageBytes] into an\n    [`ImageNdArray`][docarray.typing.ImageNdArray].\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import ImageNdArray, ImageUrl\n    class MyDoc(BaseDoc):\n        img_url: ImageUrl\n    doc = MyDoc(\n        img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    )\n    img_tensor = doc.img_url.load()\n    assert isinstance(img_tensor, ImageNdArray)\n    img_tensor = doc.img_url.load(height=224, width=224)\n    assert img_tensor.shape == (224, 224, 3)\n    layout = ('C', 'W', 'H')\n    img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n    assert img_tensor.shape == (3, 200, 100)\n    ```\n    ---\n    :param width: width of the image tensor.\n    :param height: height of the image tensor.\n    :param axis_layout: ordering of the different image axes.\n        'H' = height, 'W' = width, 'C' = color channel\n    :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n    \"\"\"\nraw_img = self.load_pil()\nif width or height:\nnew_width = width or raw_img.width\nnew_height = height or raw_img.height\nraw_img = raw_img.resize((new_width, new_height))\ntry:\ntensor = np.array(raw_img.convert('RGB'))\nexcept Exception:\ntensor = np.array(raw_img)\nimg = self._move_channel_axis(tensor, axis_layout=axis_layout)\nreturn parse_obj_as(ImageNdArray, img)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.ImageBytes.load_pil","title":"<code>load_pil()</code>","text":"<p>Load the image from the bytes into a <code>PIL.Image.Image</code> instance</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageUrl\nimg_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\nimg_url = parse_obj_as(ImageUrl, img_url)\nimg = img_url.load_pil()\nfrom PIL.Image import Image\nassert isinstance(img, Image)\n</code></pre> <p>Returns:</p> Type Description <code>Image</code> <p>a Pillow image</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>def load_pil(\nself,\n) -&gt; 'PILImage.Image':\n\"\"\"\n    Load the image from the bytes into a `PIL.Image.Image` instance\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl\n    img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    img_url = parse_obj_as(ImageUrl, img_url)\n    img = img_url.load_pil()\n    from PIL.Image import Image\n    assert isinstance(img, Image)\n    ```\n    ---\n    :return: a Pillow image\n    \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nreturn PILImage.open(BytesIO(self))\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.VideoBytes","title":"<code>VideoBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store a video and that can be load into a video tensor</p> Source code in <code>docarray/typing/bytes/video_bytes.py</code> <pre><code>@_register_proto(proto_type_name='video_bytes')\nclass VideoBytes(BaseBytes):\n\"\"\"\n    Bytes that store a video and that can be load into a video tensor\n    \"\"\"\ndef load(self, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n        Load the video from the bytes into a VideoLoadResult object consisting of:\n        - a [`VideoNdArray`][docarray.typing.VideoNdArray] (`VideoLoadResult.video`)\n        - an [`AudioNdArray`][docarray.typing.AudioNdArray] (`VideoLoadResult.audio`)\n        - an [`NdArray`][docarray.typing.NdArray] containing the key frame indices (`VideoLoadResult.key_frame_indices`).\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\n        class MyDoc(BaseDoc):\n            video_url: VideoUrl\n        doc = MyDoc(\n            video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        )\n        video, audio, key_frame_indices = doc.video_url.load()\n        assert isinstance(video, VideoNdArray)\n        assert isinstance(audio, AudioNdArray)\n        assert isinstance(key_frame_indices, NdArray)\n        ```\n        ---\n        :param kwargs: supports all keyword arguments that are being supported by\n            av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n        :return: a `VideoLoadResult` instance with video, audio and keyframe indices\n        \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av')\nwith av.open(BytesIO(self), **kwargs) as container:\naudio_frames: List[np.ndarray] = []\nvideo_frames: List[np.ndarray] = []\nkeyframe_indices: List[int] = []\nfor frame in container.decode():\nif type(frame) == av.audio.frame.AudioFrame:\naudio_frames.append(frame.to_ndarray())\nelif type(frame) == av.video.frame.VideoFrame:\nif frame.key_frame == 1:\ncurr_index = len(video_frames)\nkeyframe_indices.append(curr_index)\nvideo_frames.append(frame.to_ndarray(format='rgb24'))\nif len(audio_frames) == 0:\naudio = parse_obj_as(AudioNdArray, np.array(audio_frames))\nelse:\naudio = parse_obj_as(AudioNdArray, np.stack(audio_frames))\nvideo = parse_obj_as(VideoNdArray, np.stack(video_frames))\nindices = parse_obj_as(NdArray, keyframe_indices)\nreturn VideoLoadResult(video=video, audio=audio, key_frame_indices=indices)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.VideoBytes.load","title":"<code>load(**kwargs)</code>","text":"<p>Load the video from the bytes into a VideoLoadResult object consisting of:</p> <ul> <li>a <code>VideoNdArray</code> (<code>VideoLoadResult.video</code>)</li> <li>an <code>AudioNdArray</code> (<code>VideoLoadResult.audio</code>)</li> <li>an <code>NdArray</code> containing the key frame indices (<code>VideoLoadResult.key_frame_indices</code>).</li> </ul> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\nclass MyDoc(BaseDoc):\nvideo_url: VideoUrl\ndoc = MyDoc(\nvideo_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\nvideo, audio, key_frame_indices = doc.video_url.load()\nassert isinstance(video, VideoNdArray)\nassert isinstance(audio, AudioNdArray)\nassert isinstance(key_frame_indices, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>supports all keyword arguments that are being supported by av.open() as described here</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoLoadResult</code> <p>a <code>VideoLoadResult</code> instance with video, audio and keyframe indices</p> Source code in <code>docarray/typing/bytes/video_bytes.py</code> <pre><code>def load(self, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n    Load the video from the bytes into a VideoLoadResult object consisting of:\n    - a [`VideoNdArray`][docarray.typing.VideoNdArray] (`VideoLoadResult.video`)\n    - an [`AudioNdArray`][docarray.typing.AudioNdArray] (`VideoLoadResult.audio`)\n    - an [`NdArray`][docarray.typing.NdArray] containing the key frame indices (`VideoLoadResult.key_frame_indices`).\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\n    class MyDoc(BaseDoc):\n        video_url: VideoUrl\n    doc = MyDoc(\n        video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    video, audio, key_frame_indices = doc.video_url.load()\n    assert isinstance(video, VideoNdArray)\n    assert isinstance(audio, AudioNdArray)\n    assert isinstance(key_frame_indices, NdArray)\n    ```\n    ---\n    :param kwargs: supports all keyword arguments that are being supported by\n        av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n    :return: a `VideoLoadResult` instance with video, audio and keyframe indices\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av')\nwith av.open(BytesIO(self), **kwargs) as container:\naudio_frames: List[np.ndarray] = []\nvideo_frames: List[np.ndarray] = []\nkeyframe_indices: List[int] = []\nfor frame in container.decode():\nif type(frame) == av.audio.frame.AudioFrame:\naudio_frames.append(frame.to_ndarray())\nelif type(frame) == av.video.frame.VideoFrame:\nif frame.key_frame == 1:\ncurr_index = len(video_frames)\nkeyframe_indices.append(curr_index)\nvideo_frames.append(frame.to_ndarray(format='rgb24'))\nif len(audio_frames) == 0:\naudio = parse_obj_as(AudioNdArray, np.array(audio_frames))\nelse:\naudio = parse_obj_as(AudioNdArray, np.stack(audio_frames))\nvideo = parse_obj_as(VideoNdArray, np.stack(video_frames))\nindices = parse_obj_as(NdArray, keyframe_indices)\nreturn VideoLoadResult(video=video, audio=audio, key_frame_indices=indices)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.audio_bytes","title":"<code>audio_bytes</code>","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.audio_bytes.AudioBytes","title":"<code>AudioBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store an audio and that can be load into an Audio tensor</p> Source code in <code>docarray/typing/bytes/audio_bytes.py</code> <pre><code>@_register_proto(proto_type_name='audio_bytes')\nclass AudioBytes(BaseBytes):\n\"\"\"\n    Bytes that store an audio and that can be load into an Audio tensor\n    \"\"\"\ndef load(self) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n        Load the Audio from the [`AudioBytes`][docarray.typing.AudioBytes] into an\n        [`AudioNdArray`][docarray.typing.AudioNdArray].\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import AudioBytes, AudioNdArray, AudioUrl\n        class MyAudio(BaseDoc):\n            url: AudioUrl\n            tensor: Optional[AudioNdArray] = None\n            bytes_: Optional[AudioBytes] = None\n            frame_rate: Optional[float] = None\n        doc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\n        doc.bytes_ = doc.url.load_bytes()\n        doc.tensor, doc.frame_rate = doc.bytes_.load()\n        # Note this is equivalent to do\n        doc.tensor, doc.frame_rate = doc.url.load()\n        assert isinstance(doc.tensor, AudioNdArray)\n        ```\n        ---\n        :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing the\n            audio bytes content, and an integer representing the frame rate.\n        \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\nsegment = AudioSegment.from_file(io.BytesIO(self))\n# Convert to float32 using NumPy\nsamples = np.array(segment.get_array_of_samples())\n# Normalise float32 array so that values are between -1.0 and +1.0\nsamples_norm = samples / 2 ** (segment.sample_width * 8 - 1)\nreturn parse_obj_as(AudioNdArray, samples_norm), segment.frame_rate\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.audio_bytes.AudioBytes.load","title":"<code>load()</code>","text":"<p>Load the Audio from the <code>AudioBytes</code> into an <code>AudioNdArray</code>.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioNdArray, AudioUrl\nclass MyAudio(BaseDoc):\nurl: AudioUrl\ntensor: Optional[AudioNdArray] = None\nbytes_: Optional[AudioBytes] = None\nframe_rate: Optional[float] = None\ndoc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\ndoc.bytes_ = doc.url.load_bytes()\ndoc.tensor, doc.frame_rate = doc.bytes_.load()\n# Note this is equivalent to do\ndoc.tensor, doc.frame_rate = doc.url.load()\nassert isinstance(doc.tensor, AudioNdArray)\n</code></pre> <p>Returns:</p> Type Description <code>Tuple[AudioNdArray, int]</code> <p>tuple of an <code>AudioNdArray</code> representing the audio bytes content, and an integer representing the frame rate.</p> Source code in <code>docarray/typing/bytes/audio_bytes.py</code> <pre><code>def load(self) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n    Load the Audio from the [`AudioBytes`][docarray.typing.AudioBytes] into an\n    [`AudioNdArray`][docarray.typing.AudioNdArray].\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import AudioBytes, AudioNdArray, AudioUrl\n    class MyAudio(BaseDoc):\n        url: AudioUrl\n        tensor: Optional[AudioNdArray] = None\n        bytes_: Optional[AudioBytes] = None\n        frame_rate: Optional[float] = None\n    doc = MyAudio(url='https://www.kozco.com/tech/piano2.wav')\n    doc.bytes_ = doc.url.load_bytes()\n    doc.tensor, doc.frame_rate = doc.bytes_.load()\n    # Note this is equivalent to do\n    doc.tensor, doc.frame_rate = doc.url.load()\n    assert isinstance(doc.tensor, AudioNdArray)\n    ```\n    ---\n    :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing the\n        audio bytes content, and an integer representing the frame rate.\n    \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\nsegment = AudioSegment.from_file(io.BytesIO(self))\n# Convert to float32 using NumPy\nsamples = np.array(segment.get_array_of_samples())\n# Normalise float32 array so that values are between -1.0 and +1.0\nsamples_norm = samples / 2 ** (segment.sample_width * 8 - 1)\nreturn parse_obj_as(AudioNdArray, samples_norm), segment.frame_rate\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.base_bytes","title":"<code>base_bytes</code>","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.base_bytes.BaseBytes","title":"<code>BaseBytes</code>","text":"<p>             Bases: <code>bytes</code>, <code>AbstractType</code></p> <p>Bytes type for docarray</p> Source code in <code>docarray/typing/bytes/base_bytes.py</code> <pre><code>class BaseBytes(bytes, AbstractType):\n\"\"\"\n    Bytes type for docarray\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Any,\n) -&gt; T:\nvalue = bytes_validator(value)\nreturn cls(value)\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: T) -&gt; T:\nreturn parse_obj_as(cls, pb_msg)\ndef _to_node_protobuf(self: T) -&gt; 'NodeProto':\nfrom docarray.proto import NodeProto\nreturn NodeProto(blob=self, type=self._proto_type_name)\nif is_pydantic_v2:\n@classmethod\n@abstractmethod\ndef __get_pydantic_core_schema__(\ncls, _source_type: Any, _handler: 'GetCoreSchemaHandler'\n) -&gt; 'core_schema.CoreSchema':\nreturn core_schema.general_after_validator_function(\ncls.validate,\ncore_schema.bytes_schema(),\n)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.image_bytes","title":"<code>image_bytes</code>","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.image_bytes.ImageBytes","title":"<code>ImageBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store an image and that can be load into an image tensor</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>@_register_proto(proto_type_name='image_bytes')\nclass ImageBytes(BaseBytes):\n\"\"\"\n    Bytes that store an image and that can be load into an image tensor\n    \"\"\"\ndef load_pil(\nself,\n) -&gt; 'PILImage.Image':\n\"\"\"\n        Load the image from the bytes into a `PIL.Image.Image` instance\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl\n        img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        img_url = parse_obj_as(ImageUrl, img_url)\n        img = img_url.load_pil()\n        from PIL.Image import Image\n        assert isinstance(img, Image)\n        ```\n        ---\n        :return: a Pillow image\n        \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nreturn PILImage.open(BytesIO(self))\ndef load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\n) -&gt; ImageNdArray:\n\"\"\"\n        Load the image from the [`ImageBytes`][docarray.typing.ImageBytes] into an\n        [`ImageNdArray`][docarray.typing.ImageNdArray].\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import ImageNdArray, ImageUrl\n        class MyDoc(BaseDoc):\n            img_url: ImageUrl\n        doc = MyDoc(\n            img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n            \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        )\n        img_tensor = doc.img_url.load()\n        assert isinstance(img_tensor, ImageNdArray)\n        img_tensor = doc.img_url.load(height=224, width=224)\n        assert img_tensor.shape == (224, 224, 3)\n        layout = ('C', 'W', 'H')\n        img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n        assert img_tensor.shape == (3, 200, 100)\n        ```\n        ---\n        :param width: width of the image tensor.\n        :param height: height of the image tensor.\n        :param axis_layout: ordering of the different image axes.\n            'H' = height, 'W' = width, 'C' = color channel\n        :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n        \"\"\"\nraw_img = self.load_pil()\nif width or height:\nnew_width = width or raw_img.width\nnew_height = height or raw_img.height\nraw_img = raw_img.resize((new_width, new_height))\ntry:\ntensor = np.array(raw_img.convert('RGB'))\nexcept Exception:\ntensor = np.array(raw_img)\nimg = self._move_channel_axis(tensor, axis_layout=axis_layout)\nreturn parse_obj_as(ImageNdArray, img)\n@staticmethod\ndef _move_channel_axis(\ntensor: np.ndarray, axis_layout: Tuple[str, str, str] = ('H', 'W', 'C')\n) -&gt; np.ndarray:\n\"\"\"Moves channel axis around.\"\"\"\nchannel_to_offset = {'H': 0, 'W': 1, 'C': 2}\npermutation = tuple(channel_to_offset[axis] for axis in axis_layout)\nreturn np.transpose(tensor, permutation)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.image_bytes.ImageBytes.load","title":"<code>load(width=None, height=None, axis_layout=('H', 'W', 'C'))</code>","text":"<p>Load the image from the <code>ImageBytes</code> into an <code>ImageNdArray</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageNdArray, ImageUrl\nclass MyDoc(BaseDoc):\nimg_url: ImageUrl\ndoc = MyDoc(\nimg_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n)\nimg_tensor = doc.img_url.load()\nassert isinstance(img_tensor, ImageNdArray)\nimg_tensor = doc.img_url.load(height=224, width=224)\nassert img_tensor.shape == (224, 224, 3)\nlayout = ('C', 'W', 'H')\nimg_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\nassert img_tensor.shape == (3, 200, 100)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[int]</code> <p>width of the image tensor.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>height of the image tensor.</p> <code>None</code> <code>axis_layout</code> <code>Tuple[str, str, str]</code> <p>ordering of the different image axes. 'H' = height, 'W' = width, 'C' = color channel</p> <code>('H', 'W', 'C')</code> <p>Returns:</p> Type Description <code>ImageNdArray</code> <p><code>ImageNdArray</code> representing the image as RGB values</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>def load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\n) -&gt; ImageNdArray:\n\"\"\"\n    Load the image from the [`ImageBytes`][docarray.typing.ImageBytes] into an\n    [`ImageNdArray`][docarray.typing.ImageNdArray].\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import ImageNdArray, ImageUrl\n    class MyDoc(BaseDoc):\n        img_url: ImageUrl\n    doc = MyDoc(\n        img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    )\n    img_tensor = doc.img_url.load()\n    assert isinstance(img_tensor, ImageNdArray)\n    img_tensor = doc.img_url.load(height=224, width=224)\n    assert img_tensor.shape == (224, 224, 3)\n    layout = ('C', 'W', 'H')\n    img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n    assert img_tensor.shape == (3, 200, 100)\n    ```\n    ---\n    :param width: width of the image tensor.\n    :param height: height of the image tensor.\n    :param axis_layout: ordering of the different image axes.\n        'H' = height, 'W' = width, 'C' = color channel\n    :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n    \"\"\"\nraw_img = self.load_pil()\nif width or height:\nnew_width = width or raw_img.width\nnew_height = height or raw_img.height\nraw_img = raw_img.resize((new_width, new_height))\ntry:\ntensor = np.array(raw_img.convert('RGB'))\nexcept Exception:\ntensor = np.array(raw_img)\nimg = self._move_channel_axis(tensor, axis_layout=axis_layout)\nreturn parse_obj_as(ImageNdArray, img)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.image_bytes.ImageBytes.load_pil","title":"<code>load_pil()</code>","text":"<p>Load the image from the bytes into a <code>PIL.Image.Image</code> instance</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageUrl\nimg_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\nimg_url = parse_obj_as(ImageUrl, img_url)\nimg = img_url.load_pil()\nfrom PIL.Image import Image\nassert isinstance(img, Image)\n</code></pre> <p>Returns:</p> Type Description <code>Image</code> <p>a Pillow image</p> Source code in <code>docarray/typing/bytes/image_bytes.py</code> <pre><code>def load_pil(\nself,\n) -&gt; 'PILImage.Image':\n\"\"\"\n    Load the image from the bytes into a `PIL.Image.Image` instance\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl\n    img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    img_url = parse_obj_as(ImageUrl, img_url)\n    img = img_url.load_pil()\n    from PIL.Image import Image\n    assert isinstance(img, Image)\n    ```\n    ---\n    :return: a Pillow image\n    \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nreturn PILImage.open(BytesIO(self))\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.video_bytes","title":"<code>video_bytes</code>","text":""},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.video_bytes.VideoBytes","title":"<code>VideoBytes</code>","text":"<p>             Bases: <code>BaseBytes</code></p> <p>Bytes that store a video and that can be load into a video tensor</p> Source code in <code>docarray/typing/bytes/video_bytes.py</code> <pre><code>@_register_proto(proto_type_name='video_bytes')\nclass VideoBytes(BaseBytes):\n\"\"\"\n    Bytes that store a video and that can be load into a video tensor\n    \"\"\"\ndef load(self, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n        Load the video from the bytes into a VideoLoadResult object consisting of:\n        - a [`VideoNdArray`][docarray.typing.VideoNdArray] (`VideoLoadResult.video`)\n        - an [`AudioNdArray`][docarray.typing.AudioNdArray] (`VideoLoadResult.audio`)\n        - an [`NdArray`][docarray.typing.NdArray] containing the key frame indices (`VideoLoadResult.key_frame_indices`).\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\n        class MyDoc(BaseDoc):\n            video_url: VideoUrl\n        doc = MyDoc(\n            video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        )\n        video, audio, key_frame_indices = doc.video_url.load()\n        assert isinstance(video, VideoNdArray)\n        assert isinstance(audio, AudioNdArray)\n        assert isinstance(key_frame_indices, NdArray)\n        ```\n        ---\n        :param kwargs: supports all keyword arguments that are being supported by\n            av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n        :return: a `VideoLoadResult` instance with video, audio and keyframe indices\n        \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av')\nwith av.open(BytesIO(self), **kwargs) as container:\naudio_frames: List[np.ndarray] = []\nvideo_frames: List[np.ndarray] = []\nkeyframe_indices: List[int] = []\nfor frame in container.decode():\nif type(frame) == av.audio.frame.AudioFrame:\naudio_frames.append(frame.to_ndarray())\nelif type(frame) == av.video.frame.VideoFrame:\nif frame.key_frame == 1:\ncurr_index = len(video_frames)\nkeyframe_indices.append(curr_index)\nvideo_frames.append(frame.to_ndarray(format='rgb24'))\nif len(audio_frames) == 0:\naudio = parse_obj_as(AudioNdArray, np.array(audio_frames))\nelse:\naudio = parse_obj_as(AudioNdArray, np.stack(audio_frames))\nvideo = parse_obj_as(VideoNdArray, np.stack(video_frames))\nindices = parse_obj_as(NdArray, keyframe_indices)\nreturn VideoLoadResult(video=video, audio=audio, key_frame_indices=indices)\n</code></pre>"},{"location":"API_reference/typing/bytes/#docarray.typing.bytes.video_bytes.VideoBytes.load","title":"<code>load(**kwargs)</code>","text":"<p>Load the video from the bytes into a VideoLoadResult object consisting of:</p> <ul> <li>a <code>VideoNdArray</code> (<code>VideoLoadResult.video</code>)</li> <li>an <code>AudioNdArray</code> (<code>VideoLoadResult.audio</code>)</li> <li>an <code>NdArray</code> containing the key frame indices (<code>VideoLoadResult.key_frame_indices</code>).</li> </ul> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\nclass MyDoc(BaseDoc):\nvideo_url: VideoUrl\ndoc = MyDoc(\nvideo_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\nvideo, audio, key_frame_indices = doc.video_url.load()\nassert isinstance(video, VideoNdArray)\nassert isinstance(audio, AudioNdArray)\nassert isinstance(key_frame_indices, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>supports all keyword arguments that are being supported by av.open() as described here</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoLoadResult</code> <p>a <code>VideoLoadResult</code> instance with video, audio and keyframe indices</p> Source code in <code>docarray/typing/bytes/video_bytes.py</code> <pre><code>def load(self, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n    Load the video from the bytes into a VideoLoadResult object consisting of:\n    - a [`VideoNdArray`][docarray.typing.VideoNdArray] (`VideoLoadResult.video`)\n    - an [`AudioNdArray`][docarray.typing.AudioNdArray] (`VideoLoadResult.audio`)\n    - an [`NdArray`][docarray.typing.NdArray] containing the key frame indices (`VideoLoadResult.key_frame_indices`).\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\n    class MyDoc(BaseDoc):\n        video_url: VideoUrl\n    doc = MyDoc(\n        video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    video, audio, key_frame_indices = doc.video_url.load()\n    assert isinstance(video, VideoNdArray)\n    assert isinstance(audio, AudioNdArray)\n    assert isinstance(key_frame_indices, NdArray)\n    ```\n    ---\n    :param kwargs: supports all keyword arguments that are being supported by\n        av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n    :return: a `VideoLoadResult` instance with video, audio and keyframe indices\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av')\nwith av.open(BytesIO(self), **kwargs) as container:\naudio_frames: List[np.ndarray] = []\nvideo_frames: List[np.ndarray] = []\nkeyframe_indices: List[int] = []\nfor frame in container.decode():\nif type(frame) == av.audio.frame.AudioFrame:\naudio_frames.append(frame.to_ndarray())\nelif type(frame) == av.video.frame.VideoFrame:\nif frame.key_frame == 1:\ncurr_index = len(video_frames)\nkeyframe_indices.append(curr_index)\nvideo_frames.append(frame.to_ndarray(format='rgb24'))\nif len(audio_frames) == 0:\naudio = parse_obj_as(AudioNdArray, np.array(audio_frames))\nelse:\naudio = parse_obj_as(AudioNdArray, np.stack(audio_frames))\nvideo = parse_obj_as(VideoNdArray, np.stack(video_frames))\nindices = parse_obj_as(NdArray, keyframe_indices)\nreturn VideoLoadResult(video=video, audio=audio, key_frame_indices=indices)\n</code></pre>"},{"location":"API_reference/typing/id/","title":"Id","text":""},{"location":"API_reference/typing/id/#id","title":"Id","text":""},{"location":"API_reference/typing/id/#docarray.typing.id","title":"<code>docarray.typing.id</code>","text":""},{"location":"API_reference/typing/id/#docarray.typing.id.ID","title":"<code>ID</code>","text":"<p>             Bases: <code>str</code>, <code>AbstractType</code></p> <p>Represent an unique ID</p> Source code in <code>docarray/typing/id.py</code> <pre><code>@_register_proto(proto_type_name='id')\nclass ID(str, AbstractType):\n\"\"\"\n    Represent an unique ID\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[str, int, UUID],\n) -&gt; T:\ntry:\nid: str = str(value)\nreturn cls(id)\nexcept Exception:\nraise ValueError(f'Expected a str, int or UUID, got {type(value)}')\ndef _to_node_protobuf(self) -&gt; 'NodeProto':\n\"\"\"Convert an ID into a NodeProto message. This function should\n        be called when the self is nested into another Document that need to be\n        converted into a protobuf\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nreturn NodeProto(text=self, type=self._proto_type_name)\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n        read ndarray from a proto msg\n        :param pb_msg:\n        :return: a string\n        \"\"\"\nreturn parse_obj_as(cls, pb_msg)\nif is_pydantic_v2:\n@classmethod\ndef __get_pydantic_core_schema__(\ncls, source: Type[Any], handler: 'GetCoreSchemaHandler'\n) -&gt; core_schema.CoreSchema:\nreturn core_schema.general_plain_validator_function(\ncls.validate,\n)\n@classmethod\ndef __get_pydantic_json_schema__(\ncls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n) -&gt; JsonSchemaValue:\nfield_schema: dict[str, Any] = {}\nfield_schema.update(type='string')\nreturn field_schema\n</code></pre>"},{"location":"API_reference/typing/id/#docarray.typing.id.ID.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>a string</p> Source code in <code>docarray/typing/id.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    read ndarray from a proto msg\n    :param pb_msg:\n    :return: a string\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/","title":"Url","text":""},{"location":"API_reference/typing/url/#url","title":"Url","text":""},{"location":"API_reference/typing/url/#docarray.typing.url","title":"<code>docarray.typing.url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl","title":"<code>AnyUrl</code>","text":"<p>             Bases: <code>AnyUrl</code>, <code>AbstractType</code></p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@_register_proto(proto_type_name='any_url')\nclass AnyUrl(BaseAnyUrl, AbstractType):\nhost_required = (\nFalse  # turn off host requirement to allow passing of local paths as URL\n)\n@classmethod\ndef mime_type(cls) -&gt; str:\n\"\"\"Returns the mime type associated with the class.\"\"\"\nraise NotImplementedError\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"Returns a list of allowed file extensions for the class\n        that are not covered by the mimetypes library.\"\"\"\nraise NotImplementedError\ndef _to_node_protobuf(self) -&gt; 'NodeProto':\n\"\"\"Convert Document into a NodeProto protobuf message. This function should\n        be called when the Document is nested into another Document that need to\n        be converted into a protobuf\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nreturn NodeProto(text=str(self), type=self._proto_type_name)\n@staticmethod\ndef _get_url_extension(url: str) -&gt; str:\n\"\"\"\n        Extracts and returns the file extension from a given URL.\n        If no file extension is present, the function returns an empty string.\n        :param url: The URL to extract the file extension from.\n        :return: The file extension without the period, if one exists,\n            otherwise an empty string.\n        \"\"\"\nparsed_url = urllib.parse.urlparse(url)\next = os.path.splitext(parsed_url.path)[1]\next = ext[1:] if ext.startswith('.') else ext\nreturn ext\n@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n        Check if the file extension of the URL is allowed for this class.\n        First, it guesses the mime type of the file. If it fails to detect the\n        mime type, it then checks the extra file extensions.\n        Note: This method assumes that any URL without an extension is valid.\n        :param value: The URL or file path.\n        :return: True if the extension is allowed, False otherwise\n        \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, Any],\nfield: 'ModelField',\nconfig: 'BaseConfig',\n) -&gt; T:\nimport os\nabs_path: Union[T, np.ndarray, Any]\nif (\nisinstance(value, str)\nand not value.startswith('http')\nand not os.path.isabs(value)\n):\ninput_is_relative_path = True\nabs_path = os.path.abspath(value)\nelse:\ninput_is_relative_path = False\nabs_path = value\nurl = super().validate(abs_path, field, config)  # basic url validation\nif not cls.is_extension_allowed(value):\nraise ValueError(\nf\"The file '{value}' is not in a valid format for class '{cls.__name__}'.\"\n)\nreturn cls(str(value if input_is_relative_path else url), scheme=None)\n@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n        A method used to validate parts of a URL.\n        Our URLs should be able to function both in local and remote settings.\n        Therefore, we allow missing `scheme`, making it possible to pass a file\n        path without prefix.\n        If `scheme` is missing, we assume it is a local file path.\n        \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n        Build a URL from its parts.\n        The only difference from the pydantic implementation is that we allow\n        missing `scheme`, making it possible to pass a file path without prefix.\n        \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n        Read url from a proto msg.\n        :param pb_msg:\n        :return: url\n        \"\"\"\nreturn parse_obj_as(cls, pb_msg)\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n        it into a bytes object.\n        :param timeout: timeout for urlopen. Only relevant if URI is not local\n        :return: bytes.\n        \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of allowed file extensions for the class that are not covered by the mimetypes library.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"Returns a list of allowed file extensions for the class\n    that are not covered by the mimetypes library.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.mime_type","title":"<code>mime_type()</code>  <code>classmethod</code>","text":"<p>Returns the mime type associated with the class.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef mime_type(cls) -&gt; str:\n\"\"\"Returns the mime type associated with the class.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AnyUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl","title":"<code>AudioUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to an audio file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>@_register_proto(proto_type_name='audio_url')\nclass AudioUrl(AnyUrl):\n\"\"\"\n    URL to an audio file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn AUDIO_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load(self: T) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n        Load the data from the url into an [`AudioNdArray`][docarray.typing.AudioNdArray]\n        and the frame rate.\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import AudioNdArray, AudioUrl\n        class MyDoc(BaseDoc):\n            audio_url: AudioUrl\n            audio_tensor: Optional[AudioNdArray] = None\n        doc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\n        doc.audio_tensor, _ = doc.audio_url.load()\n        assert isinstance(doc.audio_tensor, AudioNdArray)\n        ```\n        ---\n        :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing\n            the audio file content, and an integer representing the frame rate.\n        \"\"\"\nbytes_ = self.load_bytes()\nreturn bytes_.load()\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; AudioBytes:\n\"\"\"\n        Convert url to [`AudioBytes`][docarray.typing.AudioBytes]. This will either load or\n        download the file and save it into an [`AudioBytes`][docarray.typing.AudioBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`AudioBytes`][docarray.typing.AudioBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn AudioBytes(bytes_)\ndef display(self):\n\"\"\"\n        Play the audio sound from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Audio(data=self))\nelse:\ndisplay(Audio(filename=self))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.display","title":"<code>display()</code>","text":"<p>Play the audio sound from url in notebook.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def display(self):\n\"\"\"\n    Play the audio sound from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Audio(data=self))\nelse:\ndisplay(Audio(filename=self))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.load","title":"<code>load()</code>","text":"<p>Load the data from the url into an <code>AudioNdArray</code> and the frame rate.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioNdArray, AudioUrl\nclass MyDoc(BaseDoc):\naudio_url: AudioUrl\naudio_tensor: Optional[AudioNdArray] = None\ndoc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\ndoc.audio_tensor, _ = doc.audio_url.load()\nassert isinstance(doc.audio_tensor, AudioNdArray)\n</code></pre> <p>Returns:</p> Type Description <code>Tuple[AudioNdArray, int]</code> <p>tuple of an <code>AudioNdArray</code> representing the audio file content, and an integer representing the frame rate.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def load(self: T) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n    Load the data from the url into an [`AudioNdArray`][docarray.typing.AudioNdArray]\n    and the frame rate.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import AudioNdArray, AudioUrl\n    class MyDoc(BaseDoc):\n        audio_url: AudioUrl\n        audio_tensor: Optional[AudioNdArray] = None\n    doc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\n    doc.audio_tensor, _ = doc.audio_url.load()\n    assert isinstance(doc.audio_tensor, AudioNdArray)\n    ```\n    ---\n    :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing\n        the audio file content, and an integer representing the frame rate.\n    \"\"\"\nbytes_ = self.load_bytes()\nreturn bytes_.load()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>AudioBytes</code>. This will either load or download the file and save it into an <code>AudioBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>AudioBytes</code> <p><code>AudioBytes</code> object</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; AudioBytes:\n\"\"\"\n    Convert url to [`AudioBytes`][docarray.typing.AudioBytes]. This will either load or\n    download the file and save it into an [`AudioBytes`][docarray.typing.AudioBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`AudioBytes`][docarray.typing.AudioBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn AudioBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.AudioUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl","title":"<code>ImageUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to an image file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>@_register_proto(proto_type_name='image_url')\nclass ImageUrl(AnyUrl):\n\"\"\"\n    URL to an image file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn IMAGE_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load_pil(self, timeout: Optional[float] = None) -&gt; 'PILImage.Image':\n\"\"\"\n        Load the image from the bytes into a `PIL.Image.Image` instance\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl\n        img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        img_url = parse_obj_as(ImageUrl, img_url)\n        img = img_url.load_pil()\n        from PIL.Image import Image\n        assert isinstance(img, Image)\n        ```\n        ---\n        :return: a Pillow image\n        \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(self.load_bytes(timeout=timeout)).load_pil()\ndef load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\ntimeout: Optional[float] = None,\n) -&gt; ImageNdArray:\n\"\"\"\n        Load the data from the url into an [`ImageNdArray`][docarray.typing.ImageNdArray]\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl, ImageNdArray\n        class MyDoc(BaseDoc):\n            img_url: ImageUrl\n        doc = MyDoc(\n            img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n            \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        )\n        img_tensor = doc.img_url.load()\n        assert isinstance(img_tensor, ImageNdArray)\n        img_tensor = doc.img_url.load(height=224, width=224)\n        assert img_tensor.shape == (224, 224, 3)\n        layout = ('C', 'W', 'H')\n        img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n        assert img_tensor.shape == (3, 200, 100)\n        ```\n        ---\n        :param width: width of the image tensor.\n        :param height: height of the image tensor.\n        :param axis_layout: ordering of the different image axes.\n            'H' = height, 'W' = width, 'C' = color channel\n        :param timeout: timeout (sec) for urlopen network request.\n            Only relevant if URL is not local\n        :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n        \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nbuffer = ImageBytes(self.load_bytes(timeout=timeout))\nreturn buffer.load(width, height, axis_layout)\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; ImageBytes:\n\"\"\"\n        Convert url to [`ImageBytes`][docarray.typing.ImageBytes]. This will either load or\n        download the file and save it into an [`ImageBytes`][docarray.typing.ImageBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`ImageBytes`][docarray.typing.ImageBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn ImageBytes(bytes_)\ndef display(self) -&gt; None:\n\"\"\"\n        Display image data from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Image, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Image(url=self))\nelse:\ndisplay(Image(filename=self))\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.display","title":"<code>display()</code>","text":"<p>Display image data from url in notebook.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Display image data from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Image, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Image(url=self))\nelse:\ndisplay(Image(filename=self))\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.load","title":"<code>load(width=None, height=None, axis_layout=('H', 'W', 'C'), timeout=None)</code>","text":"<p>Load the data from the url into an <code>ImageNdArray</code></p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageUrl, ImageNdArray\nclass MyDoc(BaseDoc):\nimg_url: ImageUrl\ndoc = MyDoc(\nimg_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n)\nimg_tensor = doc.img_url.load()\nassert isinstance(img_tensor, ImageNdArray)\nimg_tensor = doc.img_url.load(height=224, width=224)\nassert img_tensor.shape == (224, 224, 3)\nlayout = ('C', 'W', 'H')\nimg_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\nassert img_tensor.shape == (3, 200, 100)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[int]</code> <p>width of the image tensor.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>height of the image tensor.</p> <code>None</code> <code>axis_layout</code> <code>Tuple[str, str, str]</code> <p>ordering of the different image axes. 'H' = height, 'W' = width, 'C' = color channel</p> <code>('H', 'W', 'C')</code> <code>timeout</code> <code>Optional[float]</code> <p>timeout (sec) for urlopen network request. Only relevant if URL is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageNdArray</code> <p><code>ImageNdArray</code> representing the image as RGB values</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\ntimeout: Optional[float] = None,\n) -&gt; ImageNdArray:\n\"\"\"\n    Load the data from the url into an [`ImageNdArray`][docarray.typing.ImageNdArray]\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl, ImageNdArray\n    class MyDoc(BaseDoc):\n        img_url: ImageUrl\n    doc = MyDoc(\n        img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    )\n    img_tensor = doc.img_url.load()\n    assert isinstance(img_tensor, ImageNdArray)\n    img_tensor = doc.img_url.load(height=224, width=224)\n    assert img_tensor.shape == (224, 224, 3)\n    layout = ('C', 'W', 'H')\n    img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n    assert img_tensor.shape == (3, 200, 100)\n    ```\n    ---\n    :param width: width of the image tensor.\n    :param height: height of the image tensor.\n    :param axis_layout: ordering of the different image axes.\n        'H' = height, 'W' = width, 'C' = color channel\n    :param timeout: timeout (sec) for urlopen network request.\n        Only relevant if URL is not local\n    :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n    \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nbuffer = ImageBytes(self.load_bytes(timeout=timeout))\nreturn buffer.load(width, height, axis_layout)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>ImageBytes</code>. This will either load or download the file and save it into an <code>ImageBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageBytes</code> <p><code>ImageBytes</code> object</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; ImageBytes:\n\"\"\"\n    Convert url to [`ImageBytes`][docarray.typing.ImageBytes]. This will either load or\n    download the file and save it into an [`ImageBytes`][docarray.typing.ImageBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`ImageBytes`][docarray.typing.ImageBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn ImageBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.load_pil","title":"<code>load_pil(timeout=None)</code>","text":"<p>Load the image from the bytes into a <code>PIL.Image.Image</code> instance</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageUrl\nimg_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\nimg_url = parse_obj_as(ImageUrl, img_url)\nimg = img_url.load_pil()\nfrom PIL.Image import Image\nassert isinstance(img, Image)\n</code></pre> <p>Returns:</p> Type Description <code>Image</code> <p>a Pillow image</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load_pil(self, timeout: Optional[float] = None) -&gt; 'PILImage.Image':\n\"\"\"\n    Load the image from the bytes into a `PIL.Image.Image` instance\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl\n    img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    img_url = parse_obj_as(ImageUrl, img_url)\n    img = img_url.load_pil()\n    from PIL.Image import Image\n    assert isinstance(img, Image)\n    ```\n    ---\n    :return: a Pillow image\n    \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(self.load_bytes(timeout=timeout)).load_pil()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.ImageUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl","title":"<code>Mesh3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@_register_proto(proto_type_name='mesh_url')\nclass Mesh3DUrl(Url3D):\n\"\"\"\n    URL to a file containing 3D mesh information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\ndef load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n        Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n        object containing vertices and faces information.\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import Mesh3DUrl, NdArray\n        class MyDoc(BaseDoc):\n            mesh_url: Mesh3DUrl\n        doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        tensors = doc.mesh_url.load()\n        assert isinstance(tensors.vertices, NdArray)\n        assert isinstance(tensors.faces, NdArray)\n        ```\n        :param skip_materials: Skip materials if True, else skip.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: VerticesAndFaces object containing vertices and faces information.\n        \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh from url.\n        This loads the Trimesh instance of the 3D mesh, and then displays it.\n        \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.display","title":"<code>display()</code>","text":"<p>Plot mesh from url. This loads the Trimesh instance of the 3D mesh, and then displays it.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh from url.\n    This loads the Trimesh instance of the 3D mesh, and then displays it.\n    \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.load","title":"<code>load(skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into a <code>VerticesAndFaces</code> object containing vertices and faces information.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import Mesh3DUrl, NdArray\nclass MyDoc(BaseDoc):\nmesh_url: Mesh3DUrl\ndoc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\ntensors = doc.mesh_url.load()\nassert isinstance(tensors.vertices, NdArray)\nassert isinstance(tensors.faces, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else skip.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>VerticesAndFaces</code> <p>VerticesAndFaces object containing vertices and faces information.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n    Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n    object containing vertices and faces information.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import Mesh3DUrl, NdArray\n    class MyDoc(BaseDoc):\n        mesh_url: Mesh3DUrl\n    doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    tensors = doc.mesh_url.load()\n    assert isinstance(tensors.vertices, NdArray)\n    assert isinstance(tensors.faces, NdArray)\n    ```\n    :param skip_materials: Skip materials if True, else skip.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: VerticesAndFaces object containing vertices and faces information.\n    \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.Mesh3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl","title":"<code>PointCloud3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@_register_proto(proto_type_name='point_cloud_url')\nclass PointCloud3DUrl(Url3D):\n\"\"\"\n    URL to a file containing point cloud information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\ndef load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n        Load the data from the url into an `NdArray` containing point cloud information.\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.typing import PointCloud3DUrl\n        class MyDoc(BaseDoc):\n            point_cloud_url: PointCloud3DUrl\n        doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # point_cloud = doc.point_cloud_url.load(samples=100)\n        # assert isinstance(point_cloud, np.ndarray)\n        # assert point_cloud.shape == (100, 3)\n        ```\n        ---\n        :param samples: number of points to sample from the mesh\n        :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n            If True, store point clouds from multiple geometries in 3D np.ndarray.\n        :param skip_materials: Skip materials if True, else load.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: np.ndarray representing the point cloud\n        \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\ndef display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n        Plot point cloud from url.\n        First, it loads the point cloud into a `PointsAndColors` object, and then\n        calls display on it. The following is therefore equivalent:\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.documents import PointCloud3D\n        pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # option 1\n        # pc.url.display()\n        # option 2 (equivalent)\n        # pc.url.load(samples=10000).display()\n        ```\n        ---\n        :param samples: number of points to sample from the mesh.\n        \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.display","title":"<code>display(samples=10000)</code>","text":"<p>Plot point cloud from url.</p> <p>First, it loads the point cloud into a <code>PointsAndColors</code> object, and then calls display on it. The following is therefore equivalent:</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.documents import PointCloud3D\npc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# option 1\n# pc.url.display()\n# option 2 (equivalent)\n# pc.url.load(samples=10000).display()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh.</p> <code>10000</code> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n    Plot point cloud from url.\n    First, it loads the point cloud into a `PointsAndColors` object, and then\n    calls display on it. The following is therefore equivalent:\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D\n    pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # option 1\n    # pc.url.display()\n    # option 2 (equivalent)\n    # pc.url.load(samples=10000).display()\n    ```\n    ---\n    :param samples: number of points to sample from the mesh.\n    \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.load","title":"<code>load(samples, multiple_geometries=False, skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into an <code>NdArray</code> containing point cloud information.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing import PointCloud3DUrl\nclass MyDoc(BaseDoc):\npoint_cloud_url: PointCloud3DUrl\ndoc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# point_cloud = doc.point_cloud_url.load(samples=100)\n# assert isinstance(point_cloud, np.ndarray)\n# assert point_cloud.shape == (100, 3)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh</p> required <code>multiple_geometries</code> <code>bool</code> <p>if False, store point cloud in 2D np.ndarray. If True, store point clouds from multiple geometries in 3D np.ndarray.</p> <code>False</code> <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else load.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>PointsAndColors</code> <p>np.ndarray representing the point cloud</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n    Load the data from the url into an `NdArray` containing point cloud information.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing import PointCloud3DUrl\n    class MyDoc(BaseDoc):\n        point_cloud_url: PointCloud3DUrl\n    doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # point_cloud = doc.point_cloud_url.load(samples=100)\n    # assert isinstance(point_cloud, np.ndarray)\n    # assert point_cloud.shape == (100, 3)\n    ```\n    ---\n    :param samples: number of points to sample from the mesh\n    :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n        If True, store point clouds from multiple geometries in 3D np.ndarray.\n    :param skip_materials: Skip materials if True, else load.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: np.ndarray representing the point cloud\n    \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.PointCloud3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl","title":"<code>TextUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to a text file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>@_register_proto(proto_type_name='text_url')\nclass TextUrl(AnyUrl):\n\"\"\"\n    URL to a text file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn TEXT_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn TEXT_EXTRA_EXTENSIONS\ndef load(self, charset: str = 'utf-8', timeout: Optional[float] = None) -&gt; str:\n\"\"\"\n        Load the text file into a string.\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import TextUrl\n        class MyDoc(BaseDoc):\n            remote_url: TextUrl\n        doc = MyDoc(\n            remote_url='https://de.wikipedia.org/wiki/Brixen',\n        )\n        remote_txt = doc.remote_url.load()\n        ```\n        ---\n        :param timeout: timeout (sec) for urlopen network request.\n            Only relevant if URL is not local\n        :param charset: decoding charset; may be any character set registered with IANA\n        :return: the text file content\n        \"\"\"\n_bytes = self.load_bytes(timeout=timeout)\nreturn _bytes.decode(charset)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn TEXT_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.load","title":"<code>load(charset='utf-8', timeout=None)</code>","text":"<p>Load the text file into a string.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TextUrl\nclass MyDoc(BaseDoc):\nremote_url: TextUrl\ndoc = MyDoc(\nremote_url='https://de.wikipedia.org/wiki/Brixen',\n)\nremote_txt = doc.remote_url.load()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout (sec) for urlopen network request. Only relevant if URL is not local</p> <code>None</code> <code>charset</code> <code>str</code> <p>decoding charset; may be any character set registered with IANA</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>the text file content</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>def load(self, charset: str = 'utf-8', timeout: Optional[float] = None) -&gt; str:\n\"\"\"\n    Load the text file into a string.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import TextUrl\n    class MyDoc(BaseDoc):\n        remote_url: TextUrl\n    doc = MyDoc(\n        remote_url='https://de.wikipedia.org/wiki/Brixen',\n    )\n    remote_txt = doc.remote_url.load()\n    ```\n    ---\n    :param timeout: timeout (sec) for urlopen network request.\n        Only relevant if URL is not local\n    :param charset: decoding charset; may be any character set registered with IANA\n    :return: the text file content\n    \"\"\"\n_bytes = self.load_bytes(timeout=timeout)\nreturn _bytes.decode(charset)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.TextUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl","title":"<code>VideoUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to a video file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>@_register_proto(proto_type_name='video_url')\nclass VideoUrl(AnyUrl):\n\"\"\"\n    URL to a video file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn VIDEO_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load(self: T, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n        Load the data from the url into a `NamedTuple` of\n        [`VideoNdArray`][docarray.typing.VideoNdArray],\n        [`AudioNdArray`][docarray.typing.AudioNdArray]\n        and [`NdArray`][docarray.typing.NdArray].\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\n        class MyDoc(BaseDoc):\n            video_url: VideoUrl\n            video: Optional[VideoNdArray] = None\n            audio: Optional[AudioNdArray] = None\n            key_frame_indices: Optional[NdArray] = None\n        doc = MyDoc(\n            video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        )\n        doc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\n        assert isinstance(doc.video, VideoNdArray)\n        assert isinstance(doc.audio, AudioNdArray)\n        assert isinstance(doc.key_frame_indices, NdArray)\n        ```\n        ---\n        You can load only the key frames (or video, audio respectively):\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray.typing import NdArray, VideoUrl\n        url = parse_obj_as(\n            VideoUrl,\n            'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n        )\n        key_frame_indices = url.load().key_frame_indices\n        assert isinstance(key_frame_indices, NdArray)\n        ```\n        ---\n        :param kwargs: supports all keyword arguments that are being supported by\n            av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n        :return: [`AudioNdArray`][docarray.typing.AudioNdArray] representing the audio content,\n            [`VideoNdArray`][docarray.typing.VideoNdArray] representing the images of the video,\n            [`NdArray`][docarray.typing.NdArray] of the key frame indices.\n        \"\"\"\nbuffer = self.load_bytes(**kwargs)\nreturn buffer.load()\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; VideoBytes:\n\"\"\"\n        Convert url to [`VideoBytes`][docarray.typing.VideoBytes]. This will either load or download\n        the file and save it into an [`VideoBytes`][docarray.typing.VideoBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`VideoBytes`][docarray.typing.VideoBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn VideoBytes(bytes_)\ndef display(self):\n\"\"\"\n        Play video from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import display\nremote_url = True if self.startswith('http') else False\nif remote_url:\nfrom IPython.display import Video\nb = self.load_bytes()\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nimport os\nfrom IPython.display import HTML\npath = os.path.relpath(self)\nsrc = f'''\n                    &lt;body&gt;\n                    &lt;video width=\"320\" height=\"240\" autoplay muted controls&gt;\n                    &lt;source src=\"{path}\"&gt;\n                    Your browser does not support the video tag.\n                    &lt;/video&gt;\n                    &lt;/body&gt;\n                    '''\ndisplay(HTML(src))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.display","title":"<code>display()</code>","text":"<p>Play video from url in notebook.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def display(self):\n\"\"\"\n    Play video from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import display\nremote_url = True if self.startswith('http') else False\nif remote_url:\nfrom IPython.display import Video\nb = self.load_bytes()\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nimport os\nfrom IPython.display import HTML\npath = os.path.relpath(self)\nsrc = f'''\n                &lt;body&gt;\n                &lt;video width=\"320\" height=\"240\" autoplay muted controls&gt;\n                &lt;source src=\"{path}\"&gt;\n                Your browser does not support the video tag.\n                &lt;/video&gt;\n                &lt;/body&gt;\n                '''\ndisplay(HTML(src))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.load","title":"<code>load(**kwargs)</code>","text":"<p>Load the data from the url into a <code>NamedTuple</code> of <code>VideoNdArray</code>, <code>AudioNdArray</code> and <code>NdArray</code>.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\nclass MyDoc(BaseDoc):\nvideo_url: VideoUrl\nvideo: Optional[VideoNdArray] = None\naudio: Optional[AudioNdArray] = None\nkey_frame_indices: Optional[NdArray] = None\ndoc = MyDoc(\nvideo_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ndoc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\nassert isinstance(doc.video, VideoNdArray)\nassert isinstance(doc.audio, AudioNdArray)\nassert isinstance(doc.key_frame_indices, NdArray)\n</code></pre> <p>You can load only the key frames (or video, audio respectively):</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray.typing import NdArray, VideoUrl\nurl = parse_obj_as(\nVideoUrl,\n'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n)\nkey_frame_indices = url.load().key_frame_indices\nassert isinstance(key_frame_indices, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>supports all keyword arguments that are being supported by av.open() as described here</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoLoadResult</code> <p><code>AudioNdArray</code> representing the audio content, <code>VideoNdArray</code> representing the images of the video, <code>NdArray</code> of the key frame indices.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def load(self: T, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n    Load the data from the url into a `NamedTuple` of\n    [`VideoNdArray`][docarray.typing.VideoNdArray],\n    [`AudioNdArray`][docarray.typing.AudioNdArray]\n    and [`NdArray`][docarray.typing.NdArray].\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\n    class MyDoc(BaseDoc):\n        video_url: VideoUrl\n        video: Optional[VideoNdArray] = None\n        audio: Optional[AudioNdArray] = None\n        key_frame_indices: Optional[NdArray] = None\n    doc = MyDoc(\n        video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    doc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\n    assert isinstance(doc.video, VideoNdArray)\n    assert isinstance(doc.audio, AudioNdArray)\n    assert isinstance(doc.key_frame_indices, NdArray)\n    ```\n    ---\n    You can load only the key frames (or video, audio respectively):\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray.typing import NdArray, VideoUrl\n    url = parse_obj_as(\n        VideoUrl,\n        'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n    )\n    key_frame_indices = url.load().key_frame_indices\n    assert isinstance(key_frame_indices, NdArray)\n    ```\n    ---\n    :param kwargs: supports all keyword arguments that are being supported by\n        av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n    :return: [`AudioNdArray`][docarray.typing.AudioNdArray] representing the audio content,\n        [`VideoNdArray`][docarray.typing.VideoNdArray] representing the images of the video,\n        [`NdArray`][docarray.typing.NdArray] of the key frame indices.\n    \"\"\"\nbuffer = self.load_bytes(**kwargs)\nreturn buffer.load()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>VideoBytes</code>. This will either load or download the file and save it into an <code>VideoBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p><code>VideoBytes</code> object</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; VideoBytes:\n\"\"\"\n    Convert url to [`VideoBytes`][docarray.typing.VideoBytes]. This will either load or download\n    the file and save it into an [`VideoBytes`][docarray.typing.VideoBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`VideoBytes`][docarray.typing.VideoBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn VideoBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.VideoUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url","title":"<code>any_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl","title":"<code>AnyUrl</code>","text":"<p>             Bases: <code>AnyUrl</code>, <code>AbstractType</code></p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@_register_proto(proto_type_name='any_url')\nclass AnyUrl(BaseAnyUrl, AbstractType):\nhost_required = (\nFalse  # turn off host requirement to allow passing of local paths as URL\n)\n@classmethod\ndef mime_type(cls) -&gt; str:\n\"\"\"Returns the mime type associated with the class.\"\"\"\nraise NotImplementedError\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"Returns a list of allowed file extensions for the class\n        that are not covered by the mimetypes library.\"\"\"\nraise NotImplementedError\ndef _to_node_protobuf(self) -&gt; 'NodeProto':\n\"\"\"Convert Document into a NodeProto protobuf message. This function should\n        be called when the Document is nested into another Document that need to\n        be converted into a protobuf\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nreturn NodeProto(text=str(self), type=self._proto_type_name)\n@staticmethod\ndef _get_url_extension(url: str) -&gt; str:\n\"\"\"\n        Extracts and returns the file extension from a given URL.\n        If no file extension is present, the function returns an empty string.\n        :param url: The URL to extract the file extension from.\n        :return: The file extension without the period, if one exists,\n            otherwise an empty string.\n        \"\"\"\nparsed_url = urllib.parse.urlparse(url)\next = os.path.splitext(parsed_url.path)[1]\next = ext[1:] if ext.startswith('.') else ext\nreturn ext\n@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n        Check if the file extension of the URL is allowed for this class.\n        First, it guesses the mime type of the file. If it fails to detect the\n        mime type, it then checks the extra file extensions.\n        Note: This method assumes that any URL without an extension is valid.\n        :param value: The URL or file path.\n        :return: True if the extension is allowed, False otherwise\n        \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n@classmethod\ndef validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, Any],\nfield: 'ModelField',\nconfig: 'BaseConfig',\n) -&gt; T:\nimport os\nabs_path: Union[T, np.ndarray, Any]\nif (\nisinstance(value, str)\nand not value.startswith('http')\nand not os.path.isabs(value)\n):\ninput_is_relative_path = True\nabs_path = os.path.abspath(value)\nelse:\ninput_is_relative_path = False\nabs_path = value\nurl = super().validate(abs_path, field, config)  # basic url validation\nif not cls.is_extension_allowed(value):\nraise ValueError(\nf\"The file '{value}' is not in a valid format for class '{cls.__name__}'.\"\n)\nreturn cls(str(value if input_is_relative_path else url), scheme=None)\n@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n        A method used to validate parts of a URL.\n        Our URLs should be able to function both in local and remote settings.\n        Therefore, we allow missing `scheme`, making it possible to pass a file\n        path without prefix.\n        If `scheme` is missing, we assume it is a local file path.\n        \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n        Build a URL from its parts.\n        The only difference from the pydantic implementation is that we allow\n        missing `scheme`, making it possible to pass a file path without prefix.\n        \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n        Read url from a proto msg.\n        :param pb_msg:\n        :return: url\n        \"\"\"\nreturn parse_obj_as(cls, pb_msg)\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n        it into a bytes object.\n        :param timeout: timeout for urlopen. Only relevant if URI is not local\n        :return: bytes.\n        \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of allowed file extensions for the class that are not covered by the mimetypes library.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"Returns a list of allowed file extensions for the class\n    that are not covered by the mimetypes library.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.mime_type","title":"<code>mime_type()</code>  <code>classmethod</code>","text":"<p>Returns the mime type associated with the class.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef mime_type(cls) -&gt; str:\n\"\"\"Returns the mime type associated with the class.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.any_url.AnyUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url","title":"<code>audio_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl","title":"<code>AudioUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to an audio file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>@_register_proto(proto_type_name='audio_url')\nclass AudioUrl(AnyUrl):\n\"\"\"\n    URL to an audio file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn AUDIO_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load(self: T) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n        Load the data from the url into an [`AudioNdArray`][docarray.typing.AudioNdArray]\n        and the frame rate.\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import AudioNdArray, AudioUrl\n        class MyDoc(BaseDoc):\n            audio_url: AudioUrl\n            audio_tensor: Optional[AudioNdArray] = None\n        doc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\n        doc.audio_tensor, _ = doc.audio_url.load()\n        assert isinstance(doc.audio_tensor, AudioNdArray)\n        ```\n        ---\n        :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing\n            the audio file content, and an integer representing the frame rate.\n        \"\"\"\nbytes_ = self.load_bytes()\nreturn bytes_.load()\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; AudioBytes:\n\"\"\"\n        Convert url to [`AudioBytes`][docarray.typing.AudioBytes]. This will either load or\n        download the file and save it into an [`AudioBytes`][docarray.typing.AudioBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`AudioBytes`][docarray.typing.AudioBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn AudioBytes(bytes_)\ndef display(self):\n\"\"\"\n        Play the audio sound from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Audio(data=self))\nelse:\ndisplay(Audio(filename=self))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.display","title":"<code>display()</code>","text":"<p>Play the audio sound from url in notebook.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def display(self):\n\"\"\"\n    Play the audio sound from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Audio(data=self))\nelse:\ndisplay(Audio(filename=self))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.load","title":"<code>load()</code>","text":"<p>Load the data from the url into an <code>AudioNdArray</code> and the frame rate.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioNdArray, AudioUrl\nclass MyDoc(BaseDoc):\naudio_url: AudioUrl\naudio_tensor: Optional[AudioNdArray] = None\ndoc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\ndoc.audio_tensor, _ = doc.audio_url.load()\nassert isinstance(doc.audio_tensor, AudioNdArray)\n</code></pre> <p>Returns:</p> Type Description <code>Tuple[AudioNdArray, int]</code> <p>tuple of an <code>AudioNdArray</code> representing the audio file content, and an integer representing the frame rate.</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def load(self: T) -&gt; Tuple[AudioNdArray, int]:\n\"\"\"\n    Load the data from the url into an [`AudioNdArray`][docarray.typing.AudioNdArray]\n    and the frame rate.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import AudioNdArray, AudioUrl\n    class MyDoc(BaseDoc):\n        audio_url: AudioUrl\n        audio_tensor: Optional[AudioNdArray] = None\n    doc = MyDoc(audio_url='https://www.kozco.com/tech/piano2.wav')\n    doc.audio_tensor, _ = doc.audio_url.load()\n    assert isinstance(doc.audio_tensor, AudioNdArray)\n    ```\n    ---\n    :return: tuple of an [`AudioNdArray`][docarray.typing.AudioNdArray] representing\n        the audio file content, and an integer representing the frame rate.\n    \"\"\"\nbytes_ = self.load_bytes()\nreturn bytes_.load()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>AudioBytes</code>. This will either load or download the file and save it into an <code>AudioBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>AudioBytes</code> <p><code>AudioBytes</code> object</p> Source code in <code>docarray/typing/url/audio_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; AudioBytes:\n\"\"\"\n    Convert url to [`AudioBytes`][docarray.typing.AudioBytes]. This will either load or\n    download the file and save it into an [`AudioBytes`][docarray.typing.AudioBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`AudioBytes`][docarray.typing.AudioBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn AudioBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.audio_url.AudioUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url","title":"<code>image_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl","title":"<code>ImageUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to an image file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>@_register_proto(proto_type_name='image_url')\nclass ImageUrl(AnyUrl):\n\"\"\"\n    URL to an image file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn IMAGE_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load_pil(self, timeout: Optional[float] = None) -&gt; 'PILImage.Image':\n\"\"\"\n        Load the image from the bytes into a `PIL.Image.Image` instance\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl\n        img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        img_url = parse_obj_as(ImageUrl, img_url)\n        img = img_url.load_pil()\n        from PIL.Image import Image\n        assert isinstance(img, Image)\n        ```\n        ---\n        :return: a Pillow image\n        \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(self.load_bytes(timeout=timeout)).load_pil()\ndef load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\ntimeout: Optional[float] = None,\n) -&gt; ImageNdArray:\n\"\"\"\n        Load the data from the url into an [`ImageNdArray`][docarray.typing.ImageNdArray]\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import ImageUrl, ImageNdArray\n        class MyDoc(BaseDoc):\n            img_url: ImageUrl\n        doc = MyDoc(\n            img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n            \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n        )\n        img_tensor = doc.img_url.load()\n        assert isinstance(img_tensor, ImageNdArray)\n        img_tensor = doc.img_url.load(height=224, width=224)\n        assert img_tensor.shape == (224, 224, 3)\n        layout = ('C', 'W', 'H')\n        img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n        assert img_tensor.shape == (3, 200, 100)\n        ```\n        ---\n        :param width: width of the image tensor.\n        :param height: height of the image tensor.\n        :param axis_layout: ordering of the different image axes.\n            'H' = height, 'W' = width, 'C' = color channel\n        :param timeout: timeout (sec) for urlopen network request.\n            Only relevant if URL is not local\n        :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n        \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nbuffer = ImageBytes(self.load_bytes(timeout=timeout))\nreturn buffer.load(width, height, axis_layout)\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; ImageBytes:\n\"\"\"\n        Convert url to [`ImageBytes`][docarray.typing.ImageBytes]. This will either load or\n        download the file and save it into an [`ImageBytes`][docarray.typing.ImageBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`ImageBytes`][docarray.typing.ImageBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn ImageBytes(bytes_)\ndef display(self) -&gt; None:\n\"\"\"\n        Display image data from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Image, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Image(url=self))\nelse:\ndisplay(Image(filename=self))\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.display","title":"<code>display()</code>","text":"<p>Display image data from url in notebook.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Display image data from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Image, display\nremote_url = True if self.startswith('http') else False\nif remote_url:\ndisplay(Image(url=self))\nelse:\ndisplay(Image(filename=self))\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.load","title":"<code>load(width=None, height=None, axis_layout=('H', 'W', 'C'), timeout=None)</code>","text":"<p>Load the data from the url into an <code>ImageNdArray</code></p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageUrl, ImageNdArray\nclass MyDoc(BaseDoc):\nimg_url: ImageUrl\ndoc = MyDoc(\nimg_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n)\nimg_tensor = doc.img_url.load()\nassert isinstance(img_tensor, ImageNdArray)\nimg_tensor = doc.img_url.load(height=224, width=224)\nassert img_tensor.shape == (224, 224, 3)\nlayout = ('C', 'W', 'H')\nimg_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\nassert img_tensor.shape == (3, 200, 100)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[int]</code> <p>width of the image tensor.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>height of the image tensor.</p> <code>None</code> <code>axis_layout</code> <code>Tuple[str, str, str]</code> <p>ordering of the different image axes. 'H' = height, 'W' = width, 'C' = color channel</p> <code>('H', 'W', 'C')</code> <code>timeout</code> <code>Optional[float]</code> <p>timeout (sec) for urlopen network request. Only relevant if URL is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageNdArray</code> <p><code>ImageNdArray</code> representing the image as RGB values</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load(\nself,\nwidth: Optional[int] = None,\nheight: Optional[int] = None,\naxis_layout: Tuple[str, str, str] = ('H', 'W', 'C'),\ntimeout: Optional[float] = None,\n) -&gt; ImageNdArray:\n\"\"\"\n    Load the data from the url into an [`ImageNdArray`][docarray.typing.ImageNdArray]\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl, ImageNdArray\n    class MyDoc(BaseDoc):\n        img_url: ImageUrl\n    doc = MyDoc(\n        img_url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    )\n    img_tensor = doc.img_url.load()\n    assert isinstance(img_tensor, ImageNdArray)\n    img_tensor = doc.img_url.load(height=224, width=224)\n    assert img_tensor.shape == (224, 224, 3)\n    layout = ('C', 'W', 'H')\n    img_tensor = doc.img_url.load(height=100, width=200, axis_layout=layout)\n    assert img_tensor.shape == (3, 200, 100)\n    ```\n    ---\n    :param width: width of the image tensor.\n    :param height: height of the image tensor.\n    :param axis_layout: ordering of the different image axes.\n        'H' = height, 'W' = width, 'C' = color channel\n    :param timeout: timeout (sec) for urlopen network request.\n        Only relevant if URL is not local\n    :return: [`ImageNdArray`][docarray.typing.ImageNdArray] representing the image as RGB values\n    \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nbuffer = ImageBytes(self.load_bytes(timeout=timeout))\nreturn buffer.load(width, height, axis_layout)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>ImageBytes</code>. This will either load or download the file and save it into an <code>ImageBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageBytes</code> <p><code>ImageBytes</code> object</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; ImageBytes:\n\"\"\"\n    Convert url to [`ImageBytes`][docarray.typing.ImageBytes]. This will either load or\n    download the file and save it into an [`ImageBytes`][docarray.typing.ImageBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`ImageBytes`][docarray.typing.ImageBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn ImageBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.load_pil","title":"<code>load_pil(timeout=None)</code>","text":"<p>Load the image from the bytes into a <code>PIL.Image.Image</code> instance</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageUrl\nimg_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\nimg_url = parse_obj_as(ImageUrl, img_url)\nimg = img_url.load_pil()\nfrom PIL.Image import Image\nassert isinstance(img, Image)\n</code></pre> <p>Returns:</p> Type Description <code>Image</code> <p>a Pillow image</p> Source code in <code>docarray/typing/url/image_url.py</code> <pre><code>def load_pil(self, timeout: Optional[float] = None) -&gt; 'PILImage.Image':\n\"\"\"\n    Load the image from the bytes into a `PIL.Image.Image` instance\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray import BaseDoc\n    from docarray.typing import ImageUrl\n    img_url = \"https://upload.wikimedia.org/wikipedia/commons/8/80/Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\"\n    img_url = parse_obj_as(ImageUrl, img_url)\n    img = img_url.load_pil()\n    from PIL.Image import Image\n    assert isinstance(img, Image)\n    ```\n    ---\n    :return: a Pillow image\n    \"\"\"\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(self.load_bytes(timeout=timeout)).load_pil()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.image_url.ImageUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url","title":"<code>text_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl","title":"<code>TextUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to a text file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>@_register_proto(proto_type_name='text_url')\nclass TextUrl(AnyUrl):\n\"\"\"\n    URL to a text file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn TEXT_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn TEXT_EXTRA_EXTENSIONS\ndef load(self, charset: str = 'utf-8', timeout: Optional[float] = None) -&gt; str:\n\"\"\"\n        Load the text file into a string.\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import TextUrl\n        class MyDoc(BaseDoc):\n            remote_url: TextUrl\n        doc = MyDoc(\n            remote_url='https://de.wikipedia.org/wiki/Brixen',\n        )\n        remote_txt = doc.remote_url.load()\n        ```\n        ---\n        :param timeout: timeout (sec) for urlopen network request.\n            Only relevant if URL is not local\n        :param charset: decoding charset; may be any character set registered with IANA\n        :return: the text file content\n        \"\"\"\n_bytes = self.load_bytes(timeout=timeout)\nreturn _bytes.decode(charset)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn TEXT_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.load","title":"<code>load(charset='utf-8', timeout=None)</code>","text":"<p>Load the text file into a string.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TextUrl\nclass MyDoc(BaseDoc):\nremote_url: TextUrl\ndoc = MyDoc(\nremote_url='https://de.wikipedia.org/wiki/Brixen',\n)\nremote_txt = doc.remote_url.load()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout (sec) for urlopen network request. Only relevant if URL is not local</p> <code>None</code> <code>charset</code> <code>str</code> <p>decoding charset; may be any character set registered with IANA</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>the text file content</p> Source code in <code>docarray/typing/url/text_url.py</code> <pre><code>def load(self, charset: str = 'utf-8', timeout: Optional[float] = None) -&gt; str:\n\"\"\"\n    Load the text file into a string.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import TextUrl\n    class MyDoc(BaseDoc):\n        remote_url: TextUrl\n    doc = MyDoc(\n        remote_url='https://de.wikipedia.org/wiki/Brixen',\n    )\n    remote_txt = doc.remote_url.load()\n    ```\n    ---\n    :param timeout: timeout (sec) for urlopen network request.\n        Only relevant if URL is not local\n    :param charset: decoding charset; may be any character set registered with IANA\n    :return: the text file content\n    \"\"\"\n_bytes = self.load_bytes(timeout=timeout)\nreturn _bytes.decode(charset)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.text_url.TextUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d","title":"<code>url_3d</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl","title":"<code>Mesh3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@_register_proto(proto_type_name='mesh_url')\nclass Mesh3DUrl(Url3D):\n\"\"\"\n    URL to a file containing 3D mesh information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\ndef load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n        Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n        object containing vertices and faces information.\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import Mesh3DUrl, NdArray\n        class MyDoc(BaseDoc):\n            mesh_url: Mesh3DUrl\n        doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        tensors = doc.mesh_url.load()\n        assert isinstance(tensors.vertices, NdArray)\n        assert isinstance(tensors.faces, NdArray)\n        ```\n        :param skip_materials: Skip materials if True, else skip.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: VerticesAndFaces object containing vertices and faces information.\n        \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh from url.\n        This loads the Trimesh instance of the 3D mesh, and then displays it.\n        \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.display","title":"<code>display()</code>","text":"<p>Plot mesh from url. This loads the Trimesh instance of the 3D mesh, and then displays it.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh from url.\n    This loads the Trimesh instance of the 3D mesh, and then displays it.\n    \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.load","title":"<code>load(skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into a <code>VerticesAndFaces</code> object containing vertices and faces information.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import Mesh3DUrl, NdArray\nclass MyDoc(BaseDoc):\nmesh_url: Mesh3DUrl\ndoc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\ntensors = doc.mesh_url.load()\nassert isinstance(tensors.vertices, NdArray)\nassert isinstance(tensors.faces, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else skip.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>VerticesAndFaces</code> <p>VerticesAndFaces object containing vertices and faces information.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n    Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n    object containing vertices and faces information.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import Mesh3DUrl, NdArray\n    class MyDoc(BaseDoc):\n        mesh_url: Mesh3DUrl\n    doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    tensors = doc.mesh_url.load()\n    assert isinstance(tensors.vertices, NdArray)\n    assert isinstance(tensors.faces, NdArray)\n    ```\n    :param skip_materials: Skip materials if True, else skip.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: VerticesAndFaces object containing vertices and faces information.\n    \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.Mesh3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl","title":"<code>PointCloud3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@_register_proto(proto_type_name='point_cloud_url')\nclass PointCloud3DUrl(Url3D):\n\"\"\"\n    URL to a file containing point cloud information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\ndef load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n        Load the data from the url into an `NdArray` containing point cloud information.\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.typing import PointCloud3DUrl\n        class MyDoc(BaseDoc):\n            point_cloud_url: PointCloud3DUrl\n        doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # point_cloud = doc.point_cloud_url.load(samples=100)\n        # assert isinstance(point_cloud, np.ndarray)\n        # assert point_cloud.shape == (100, 3)\n        ```\n        ---\n        :param samples: number of points to sample from the mesh\n        :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n            If True, store point clouds from multiple geometries in 3D np.ndarray.\n        :param skip_materials: Skip materials if True, else load.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: np.ndarray representing the point cloud\n        \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\ndef display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n        Plot point cloud from url.\n        First, it loads the point cloud into a `PointsAndColors` object, and then\n        calls display on it. The following is therefore equivalent:\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.documents import PointCloud3D\n        pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # option 1\n        # pc.url.display()\n        # option 2 (equivalent)\n        # pc.url.load(samples=10000).display()\n        ```\n        ---\n        :param samples: number of points to sample from the mesh.\n        \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.display","title":"<code>display(samples=10000)</code>","text":"<p>Plot point cloud from url.</p> <p>First, it loads the point cloud into a <code>PointsAndColors</code> object, and then calls display on it. The following is therefore equivalent:</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.documents import PointCloud3D\npc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# option 1\n# pc.url.display()\n# option 2 (equivalent)\n# pc.url.load(samples=10000).display()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh.</p> <code>10000</code> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n    Plot point cloud from url.\n    First, it loads the point cloud into a `PointsAndColors` object, and then\n    calls display on it. The following is therefore equivalent:\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D\n    pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # option 1\n    # pc.url.display()\n    # option 2 (equivalent)\n    # pc.url.load(samples=10000).display()\n    ```\n    ---\n    :param samples: number of points to sample from the mesh.\n    \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.load","title":"<code>load(samples, multiple_geometries=False, skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into an <code>NdArray</code> containing point cloud information.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing import PointCloud3DUrl\nclass MyDoc(BaseDoc):\npoint_cloud_url: PointCloud3DUrl\ndoc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# point_cloud = doc.point_cloud_url.load(samples=100)\n# assert isinstance(point_cloud, np.ndarray)\n# assert point_cloud.shape == (100, 3)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh</p> required <code>multiple_geometries</code> <code>bool</code> <p>if False, store point cloud in 2D np.ndarray. If True, store point clouds from multiple geometries in 3D np.ndarray.</p> <code>False</code> <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else load.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>PointsAndColors</code> <p>np.ndarray representing the point cloud</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n    Load the data from the url into an `NdArray` containing point cloud information.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing import PointCloud3DUrl\n    class MyDoc(BaseDoc):\n        point_cloud_url: PointCloud3DUrl\n    doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # point_cloud = doc.point_cloud_url.load(samples=100)\n    # assert isinstance(point_cloud, np.ndarray)\n    # assert point_cloud.shape == (100, 3)\n    ```\n    ---\n    :param samples: number of points to sample from the mesh\n    :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n        If True, store point clouds from multiple geometries in 3D np.ndarray.\n    :param skip_materials: Skip materials if True, else load.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: np.ndarray representing the point cloud\n    \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.PointCloud3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url","title":"<code>mesh_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl","title":"<code>Mesh3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing 3D mesh information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@_register_proto(proto_type_name='mesh_url')\nclass Mesh3DUrl(Url3D):\n\"\"\"\n    URL to a file containing 3D mesh information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\ndef load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n        Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n        object containing vertices and faces information.\n        ---\n        ```python\n        from docarray import BaseDoc\n        from docarray.typing import Mesh3DUrl, NdArray\n        class MyDoc(BaseDoc):\n            mesh_url: Mesh3DUrl\n        doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        tensors = doc.mesh_url.load()\n        assert isinstance(tensors.vertices, NdArray)\n        assert isinstance(tensors.faces, NdArray)\n        ```\n        :param skip_materials: Skip materials if True, else skip.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: VerticesAndFaces object containing vertices and faces information.\n        \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\ndef display(self) -&gt; None:\n\"\"\"\n        Plot mesh from url.\n        This loads the Trimesh instance of the 3D mesh, and then displays it.\n        \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.display","title":"<code>display()</code>","text":"<p>Plot mesh from url. This loads the Trimesh instance of the 3D mesh, and then displays it.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Plot mesh from url.\n    This loads the Trimesh instance of the 3D mesh, and then displays it.\n    \"\"\"\nfrom IPython.display import display\nmesh = self._load_trimesh_instance(skip_materials=False)\ndisplay(mesh.show())\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn MESH_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.load","title":"<code>load(skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into a <code>VerticesAndFaces</code> object containing vertices and faces information.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import Mesh3DUrl, NdArray\nclass MyDoc(BaseDoc):\nmesh_url: Mesh3DUrl\ndoc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\ntensors = doc.mesh_url.load()\nassert isinstance(tensors.vertices, NdArray)\nassert isinstance(tensors.faces, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else skip.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>VerticesAndFaces</code> <p>VerticesAndFaces object containing vertices and faces information.</p> Source code in <code>docarray/typing/url/url_3d/mesh_url.py</code> <pre><code>def load(\nself: T,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'VerticesAndFaces':\n\"\"\"\n    Load the data from the url into a [`VerticesAndFaces`][docarray.documents.VerticesAndFaces]\n    object containing vertices and faces information.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import Mesh3DUrl, NdArray\n    class MyDoc(BaseDoc):\n        mesh_url: Mesh3DUrl\n    doc = MyDoc(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    tensors = doc.mesh_url.load()\n    assert isinstance(tensors.vertices, NdArray)\n    assert isinstance(tensors.faces, NdArray)\n    ```\n    :param skip_materials: Skip materials if True, else skip.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: VerticesAndFaces object containing vertices and faces information.\n    \"\"\"\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nif not trimesh_args:\ntrimesh_args = {}\nmesh = self._load_trimesh_instance(\nforce='mesh', skip_materials=skip_materials, **trimesh_args\n)\nvertices = parse_obj_as(NdArray, mesh.vertices.view(np.ndarray))\nfaces = parse_obj_as(NdArray, mesh.faces.view(np.ndarray))\nreturn VerticesAndFaces(vertices=vertices, faces=faces)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.mesh_url.Mesh3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url","title":"<code>point_cloud_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl","title":"<code>PointCloud3DUrl</code>","text":"<p>             Bases: <code>Url3D</code></p> <p>URL to a file containing point cloud information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@_register_proto(proto_type_name='point_cloud_url')\nclass PointCloud3DUrl(Url3D):\n\"\"\"\n    URL to a file containing point cloud information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\ndef load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n        Load the data from the url into an `NdArray` containing point cloud information.\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.typing import PointCloud3DUrl\n        class MyDoc(BaseDoc):\n            point_cloud_url: PointCloud3DUrl\n        doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # point_cloud = doc.point_cloud_url.load(samples=100)\n        # assert isinstance(point_cloud, np.ndarray)\n        # assert point_cloud.shape == (100, 3)\n        ```\n        ---\n        :param samples: number of points to sample from the mesh\n        :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n            If True, store point clouds from multiple geometries in 3D np.ndarray.\n        :param skip_materials: Skip materials if True, else load.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: np.ndarray representing the point cloud\n        \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\ndef display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n        Plot point cloud from url.\n        First, it loads the point cloud into a `PointsAndColors` object, and then\n        calls display on it. The following is therefore equivalent:\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.documents import PointCloud3D\n        pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n        # option 1\n        # pc.url.display()\n        # option 2 (equivalent)\n        # pc.url.load(samples=10000).display()\n        ```\n        ---\n        :param samples: number of points to sample from the mesh.\n        \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.display","title":"<code>display(samples=10000)</code>","text":"<p>Plot point cloud from url.</p> <p>First, it loads the point cloud into a <code>PointsAndColors</code> object, and then calls display on it. The following is therefore equivalent:</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.documents import PointCloud3D\npc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# option 1\n# pc.url.display()\n# option 2 (equivalent)\n# pc.url.load(samples=10000).display()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh.</p> <code>10000</code> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def display(\nself,\nsamples: int = 10000,\n) -&gt; None:\n\"\"\"\n    Plot point cloud from url.\n    First, it loads the point cloud into a `PointsAndColors` object, and then\n    calls display on it. The following is therefore equivalent:\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.documents import PointCloud3D\n    pc = PointCloud3D(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # option 1\n    # pc.url.display()\n    # option 2 (equivalent)\n    # pc.url.load(samples=10000).display()\n    ```\n    ---\n    :param samples: number of points to sample from the mesh.\n    \"\"\"\nself.load(samples=samples, skip_materials=False).display()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn POINT_CLOUD_EXTRA_EXTENSIONS\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.load","title":"<code>load(samples, multiple_geometries=False, skip_materials=True, trimesh_args=None)</code>","text":"<p>Load the data from the url into an <code>NdArray</code> containing point cloud information.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing import PointCloud3DUrl\nclass MyDoc(BaseDoc):\npoint_cloud_url: PointCloud3DUrl\ndoc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n# point_cloud = doc.point_cloud_url.load(samples=100)\n# assert isinstance(point_cloud, np.ndarray)\n# assert point_cloud.shape == (100, 3)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>number of points to sample from the mesh</p> required <code>multiple_geometries</code> <code>bool</code> <p>if False, store point cloud in 2D np.ndarray. If True, store point clouds from multiple geometries in 3D np.ndarray.</p> <code>False</code> <code>skip_materials</code> <code>bool</code> <p>Skip materials if True, else load.</p> <code>True</code> <code>trimesh_args</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary of additional arguments for <code>trimesh.load()</code> or <code>trimesh.load_remote()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>PointsAndColors</code> <p>np.ndarray representing the point cloud</p> Source code in <code>docarray/typing/url/url_3d/point_cloud_url.py</code> <pre><code>def load(\nself: T,\nsamples: int,\nmultiple_geometries: bool = False,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; 'PointsAndColors':\n\"\"\"\n    Load the data from the url into an `NdArray` containing point cloud information.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing import PointCloud3DUrl\n    class MyDoc(BaseDoc):\n        point_cloud_url: PointCloud3DUrl\n    doc = MyDoc(point_cloud_url=\"thttps://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n    # point_cloud = doc.point_cloud_url.load(samples=100)\n    # assert isinstance(point_cloud, np.ndarray)\n    # assert point_cloud.shape == (100, 3)\n    ```\n    ---\n    :param samples: number of points to sample from the mesh\n    :param multiple_geometries: if False, store point cloud in 2D np.ndarray.\n        If True, store point clouds from multiple geometries in 3D np.ndarray.\n    :param skip_materials: Skip materials if True, else load.\n    :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n        or `trimesh.load_remote()`.\n    :return: np.ndarray representing the point cloud\n    \"\"\"\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nif not trimesh_args:\ntrimesh_args = {}\nif multiple_geometries:\n# try to coerce everything into a scene\nscene = self._load_trimesh_instance(\nforce='scene', skip_materials=skip_materials, **trimesh_args\n)\npoint_cloud = np.stack(\n[np.array(geo.sample(samples)) for geo in scene.geometry.values()],\naxis=0,\n)\nelse:\n# combine a scene into a single mesh\nmesh = self._load_trimesh_instance(force='mesh', **trimesh_args)\npoint_cloud = np.array(mesh.sample(samples))\npoints = parse_obj_as(NdArray, point_cloud)\nreturn PointsAndColors(points=points, colors=None)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.point_cloud_url.PointCloud3DUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d","title":"<code>url_3d</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D","title":"<code>Url3D</code>","text":"<p>             Bases: <code>AnyUrl</code>, <code>ABC</code></p> <p>URL to a file containing 3D mesh or point cloud information. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/url_3d/url_3d.py</code> <pre><code>@_register_proto(proto_type_name='url3d')\nclass Url3D(AnyUrl, ABC):\n\"\"\"\n    URL to a file containing 3D mesh or point cloud information.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn OBJ_MIMETYPE\ndef _load_trimesh_instance(\nself: T,\nforce: Optional[str] = None,\nskip_materials: bool = True,\ntrimesh_args: Optional[Dict[str, Any]] = None,\n) -&gt; Union['trimesh.Trimesh', 'trimesh.Scene']:\n\"\"\"\n        Load the data from the url into a trimesh.Mesh or trimesh.Scene object.\n        :param force: str or None. For 'mesh' try to coerce scenes into a single mesh.\n            For 'scene' try to coerce everything into a scene.\n        :param skip_materials: Skip materials if True, else skip.\n        :param trimesh_args: dictionary of additional arguments for `trimesh.load()`\n            or `trimesh.load_remote()`.\n        :return: trimesh.Mesh or trimesh.Scene object\n        \"\"\"\nimport urllib.parse\nif TYPE_CHECKING:\nimport trimesh\nelse:\ntrimesh = import_library('trimesh', raise_error=True)\nif not trimesh_args:\ntrimesh_args = {}\nscheme = urllib.parse.urlparse(self).scheme\nloader = trimesh.load_remote if scheme in ['http', 'https'] else trimesh.load\nmesh = loader(self, force=force, skip_materials=skip_materials, **trimesh_args)\nreturn mesh\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of allowed file extensions for the class that are not covered by the mimetypes library.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"Returns a list of allowed file extensions for the class\n    that are not covered by the mimetypes library.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to bytes. This will either load or download the file and save it into a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if URI is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>bytes.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; bytes:\n\"\"\"Convert url to bytes. This will either load or download the file and save\n    it into a bytes object.\n    :param timeout: timeout for urlopen. Only relevant if URI is not local\n    :return: bytes.\n    \"\"\"\nif urllib.parse.urlparse(self).scheme in {'http', 'https', 'data'}:\nreq = urllib.request.Request(\nself, headers={'User-Agent': 'Mozilla/5.0'}\n)\nurlopen_kwargs = {'timeout': timeout} if timeout is not None else {}\nwith urllib.request.urlopen(req, **urlopen_kwargs) as fp:  # type: ignore\nreturn fp.read()\nelif os.path.exists(self):\nwith open(self, 'rb') as fp:\nreturn fp.read()\nelse:\nraise FileNotFoundError(f'`{self}` is not a URL or a valid local path')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.url_3d.url_3d.Url3D.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url","title":"<code>video_url</code>","text":""},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl","title":"<code>VideoUrl</code>","text":"<p>             Bases: <code>AnyUrl</code></p> <p>URL to a video file. Can be remote (web) URL, or a local file path.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>@_register_proto(proto_type_name='video_url')\nclass VideoUrl(AnyUrl):\n\"\"\"\n    URL to a video file.\n    Can be remote (web) URL, or a local file path.\n    \"\"\"\n@classmethod\ndef mime_type(cls) -&gt; str:\nreturn VIDEO_MIMETYPE\n@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n        Returns a list of additional file extensions that are valid for this class\n        but cannot be identified by the mimetypes library.\n        \"\"\"\nreturn []\ndef load(self: T, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n        Load the data from the url into a `NamedTuple` of\n        [`VideoNdArray`][docarray.typing.VideoNdArray],\n        [`AudioNdArray`][docarray.typing.AudioNdArray]\n        and [`NdArray`][docarray.typing.NdArray].\n        ---\n        ```python\n        from typing import Optional\n        from docarray import BaseDoc\n        from docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\n        class MyDoc(BaseDoc):\n            video_url: VideoUrl\n            video: Optional[VideoNdArray] = None\n            audio: Optional[AudioNdArray] = None\n            key_frame_indices: Optional[NdArray] = None\n        doc = MyDoc(\n            video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n        )\n        doc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\n        assert isinstance(doc.video, VideoNdArray)\n        assert isinstance(doc.audio, AudioNdArray)\n        assert isinstance(doc.key_frame_indices, NdArray)\n        ```\n        ---\n        You can load only the key frames (or video, audio respectively):\n        ---\n        ```python\n        from pydantic import parse_obj_as\n        from docarray.typing import NdArray, VideoUrl\n        url = parse_obj_as(\n            VideoUrl,\n            'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n        )\n        key_frame_indices = url.load().key_frame_indices\n        assert isinstance(key_frame_indices, NdArray)\n        ```\n        ---\n        :param kwargs: supports all keyword arguments that are being supported by\n            av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n        :return: [`AudioNdArray`][docarray.typing.AudioNdArray] representing the audio content,\n            [`VideoNdArray`][docarray.typing.VideoNdArray] representing the images of the video,\n            [`NdArray`][docarray.typing.NdArray] of the key frame indices.\n        \"\"\"\nbuffer = self.load_bytes(**kwargs)\nreturn buffer.load()\ndef load_bytes(self, timeout: Optional[float] = None) -&gt; VideoBytes:\n\"\"\"\n        Convert url to [`VideoBytes`][docarray.typing.VideoBytes]. This will either load or download\n        the file and save it into an [`VideoBytes`][docarray.typing.VideoBytes] object.\n        :param timeout: timeout for urlopen. Only relevant if url is not local\n        :return: [`VideoBytes`][docarray.typing.VideoBytes] object\n        \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn VideoBytes(bytes_)\ndef display(self):\n\"\"\"\n        Play video from url in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import display\nremote_url = True if self.startswith('http') else False\nif remote_url:\nfrom IPython.display import Video\nb = self.load_bytes()\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nimport os\nfrom IPython.display import HTML\npath = os.path.relpath(self)\nsrc = f'''\n                    &lt;body&gt;\n                    &lt;video width=\"320\" height=\"240\" autoplay muted controls&gt;\n                    &lt;source src=\"{path}\"&gt;\n                    Your browser does not support the video tag.\n                    &lt;/video&gt;\n                    &lt;/body&gt;\n                    '''\ndisplay(HTML(src))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.build","title":"<code>build(*, scheme, user=None, password=None, host, port=None, path=None, query=None, fragment=None, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Build a URL from its parts. The only difference from the pydantic implementation is that we allow missing <code>scheme</code>, making it possible to pass a file path without prefix.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef build(\ncls,\n*,\nscheme: str,\nuser: Optional[str] = None,\npassword: Optional[str] = None,\nhost: str,\nport: Optional[str] = None,\npath: Optional[str] = None,\nquery: Optional[str] = None,\nfragment: Optional[str] = None,\n**_kwargs: str,\n) -&gt; str:\n\"\"\"\n    Build a URL from its parts.\n    The only difference from the pydantic implementation is that we allow\n    missing `scheme`, making it possible to pass a file path without prefix.\n    \"\"\"\n# allow missing scheme, unlike pydantic\nscheme_ = scheme if scheme is not None else ''\nurl = super().build(\nscheme=scheme_,\nuser=user,\npassword=password,\nhost=host,\nport=port,\npath=path,\nquery=query,\nfragment=fragment,\n**_kwargs,\n)\nif scheme is None and url.startswith('://'):\n# remove the `://` prefix, since scheme is missing\nurl = url[3:]\nreturn url\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.display","title":"<code>display()</code>","text":"<p>Play video from url in notebook.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def display(self):\n\"\"\"\n    Play video from url in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import display\nremote_url = True if self.startswith('http') else False\nif remote_url:\nfrom IPython.display import Video\nb = self.load_bytes()\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nimport os\nfrom IPython.display import HTML\npath = os.path.relpath(self)\nsrc = f'''\n                &lt;body&gt;\n                &lt;video width=\"320\" height=\"240\" autoplay muted controls&gt;\n                &lt;source src=\"{path}\"&gt;\n                Your browser does not support the video tag.\n                &lt;/video&gt;\n                &lt;/body&gt;\n                '''\ndisplay(HTML(src))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.extra_extensions","title":"<code>extra_extensions()</code>  <code>classmethod</code>","text":"<p>Returns a list of additional file extensions that are valid for this class but cannot be identified by the mimetypes library.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>@classmethod\ndef extra_extensions(cls) -&gt; List[str]:\n\"\"\"\n    Returns a list of additional file extensions that are valid for this class\n    but cannot be identified by the mimetypes library.\n    \"\"\"\nreturn []\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read url from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>str</code> required <p>Returns:</p> Type Description <code>T</code> <p>url</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'str') -&gt; T:\n\"\"\"\n    Read url from a proto msg.\n    :param pb_msg:\n    :return: url\n    \"\"\"\nreturn parse_obj_as(cls, pb_msg)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.is_extension_allowed","title":"<code>is_extension_allowed(value)</code>  <code>classmethod</code>","text":"<p>Check if the file extension of the URL is allowed for this class. First, it guesses the mime type of the file. If it fails to detect the mime type, it then checks the extra file extensions. Note: This method assumes that any URL without an extension is valid.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The URL or file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the extension is allowed, False otherwise</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef is_extension_allowed(cls, value: Any) -&gt; bool:\n\"\"\"\n    Check if the file extension of the URL is allowed for this class.\n    First, it guesses the mime type of the file. If it fails to detect the\n    mime type, it then checks the extra file extensions.\n    Note: This method assumes that any URL without an extension is valid.\n    :param value: The URL or file path.\n    :return: True if the extension is allowed, False otherwise\n    \"\"\"\nif cls is AnyUrl:\nreturn True\nurl_parts = value.split('?')\nextension = cls._get_url_extension(value)\nif not extension:\nreturn True\nmimetype, _ = mimetypes.guess_type(url_parts[0])\nif mimetype and mimetype.startswith(cls.mime_type()):\nreturn True\nreturn extension in cls.extra_extensions()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.load","title":"<code>load(**kwargs)</code>","text":"<p>Load the data from the url into a <code>NamedTuple</code> of <code>VideoNdArray</code>, <code>AudioNdArray</code> and <code>NdArray</code>.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\nclass MyDoc(BaseDoc):\nvideo_url: VideoUrl\nvideo: Optional[VideoNdArray] = None\naudio: Optional[AudioNdArray] = None\nkey_frame_indices: Optional[NdArray] = None\ndoc = MyDoc(\nvideo_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ndoc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\nassert isinstance(doc.video, VideoNdArray)\nassert isinstance(doc.audio, AudioNdArray)\nassert isinstance(doc.key_frame_indices, NdArray)\n</code></pre> <p>You can load only the key frames (or video, audio respectively):</p> <pre><code>from pydantic import parse_obj_as\nfrom docarray.typing import NdArray, VideoUrl\nurl = parse_obj_as(\nVideoUrl,\n'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n)\nkey_frame_indices = url.load().key_frame_indices\nassert isinstance(key_frame_indices, NdArray)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>supports all keyword arguments that are being supported by av.open() as described here</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoLoadResult</code> <p><code>AudioNdArray</code> representing the audio content, <code>VideoNdArray</code> representing the images of the video, <code>NdArray</code> of the key frame indices.</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def load(self: T, **kwargs) -&gt; VideoLoadResult:\n\"\"\"\n    Load the data from the url into a `NamedTuple` of\n    [`VideoNdArray`][docarray.typing.VideoNdArray],\n    [`AudioNdArray`][docarray.typing.AudioNdArray]\n    and [`NdArray`][docarray.typing.NdArray].\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import VideoUrl, VideoNdArray, AudioNdArray, NdArray\n    class MyDoc(BaseDoc):\n        video_url: VideoUrl\n        video: Optional[VideoNdArray] = None\n        audio: Optional[AudioNdArray] = None\n        key_frame_indices: Optional[NdArray] = None\n    doc = MyDoc(\n        video_url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n    )\n    doc.video, doc.audio, doc.key_frame_indices = doc.video_url.load()\n    assert isinstance(doc.video, VideoNdArray)\n    assert isinstance(doc.audio, AudioNdArray)\n    assert isinstance(doc.key_frame_indices, NdArray)\n    ```\n    ---\n    You can load only the key frames (or video, audio respectively):\n    ---\n    ```python\n    from pydantic import parse_obj_as\n    from docarray.typing import NdArray, VideoUrl\n    url = parse_obj_as(\n        VideoUrl,\n        'https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n    )\n    key_frame_indices = url.load().key_frame_indices\n    assert isinstance(key_frame_indices, NdArray)\n    ```\n    ---\n    :param kwargs: supports all keyword arguments that are being supported by\n        av.open() as described [here](https://pyav.org/docs/stable/api/_globals.html?highlight=open#av.open)\n    :return: [`AudioNdArray`][docarray.typing.AudioNdArray] representing the audio content,\n        [`VideoNdArray`][docarray.typing.VideoNdArray] representing the images of the video,\n        [`NdArray`][docarray.typing.NdArray] of the key frame indices.\n    \"\"\"\nbuffer = self.load_bytes(**kwargs)\nreturn buffer.load()\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.load_bytes","title":"<code>load_bytes(timeout=None)</code>","text":"<p>Convert url to <code>VideoBytes</code>. This will either load or download the file and save it into an <code>VideoBytes</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>timeout for urlopen. Only relevant if url is not local</p> <code>None</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p><code>VideoBytes</code> object</p> Source code in <code>docarray/typing/url/video_url.py</code> <pre><code>def load_bytes(self, timeout: Optional[float] = None) -&gt; VideoBytes:\n\"\"\"\n    Convert url to [`VideoBytes`][docarray.typing.VideoBytes]. This will either load or download\n    the file and save it into an [`VideoBytes`][docarray.typing.VideoBytes] object.\n    :param timeout: timeout for urlopen. Only relevant if url is not local\n    :return: [`VideoBytes`][docarray.typing.VideoBytes] object\n    \"\"\"\nbytes_ = super().load_bytes(timeout=timeout)\nreturn VideoBytes(bytes_)\n</code></pre>"},{"location":"API_reference/typing/url/#docarray.typing.url.video_url.VideoUrl.validate_parts","title":"<code>validate_parts(parts, validate_port=True)</code>  <code>classmethod</code>","text":"<p>A method used to validate parts of a URL. Our URLs should be able to function both in local and remote settings. Therefore, we allow missing <code>scheme</code>, making it possible to pass a file path without prefix. If <code>scheme</code> is missing, we assume it is a local file path.</p> Source code in <code>docarray/typing/url/any_url.py</code> <pre><code>@classmethod\ndef validate_parts(cls, parts: 'Parts', validate_port: bool = True) -&gt; 'Parts':\n\"\"\"\n    A method used to validate parts of a URL.\n    Our URLs should be able to function both in local and remote settings.\n    Therefore, we allow missing `scheme`, making it possible to pass a file\n    path without prefix.\n    If `scheme` is missing, we assume it is a local file path.\n    \"\"\"\nscheme = parts['scheme']\nif scheme is None:\n# allow missing scheme, unlike pydantic\npass\nelif cls.allowed_schemes and scheme.lower() not in cls.allowed_schemes:\nraise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\nif validate_port:\ncls._validate_port(parts['port'])\nuser = parts['user']\nif cls.user_required and user is None:\nraise errors.UrlUserInfoError()\nreturn parts\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/","title":"AudioTensor","text":""},{"location":"API_reference/typing/tensor/audio/#audiotensor","title":"AudioTensor","text":""},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_ndarray","title":"<code>docarray.typing.tensor.audio.audio_ndarray</code>","text":""},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_ndarray.AudioNdArray","title":"<code>AudioNdArray</code>","text":"<p>             Bases: <code>AbstractAudioTensor</code>, <code>NdArray</code></p> <p>Subclass of <code>NdArray</code>, to represent an audio tensor. Adds audio-specific features to the tensor.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioNdArray, AudioUrl\nimport numpy as np\nclass MyAudioDoc(BaseDoc):\ntitle: str\naudio_tensor: Optional[AudioNdArray] = None\nurl: Optional[AudioUrl] = None\nbytes_: Optional[AudioBytes] = None\n# from tensor\ndoc_1 = MyAudioDoc(\ntitle='my_first_audio_doc',\naudio_tensor=np.random.rand(1000, 2),\n)\n# doc_1.audio_tensor.save(file_path='/tmp/file_1.wav')\ndoc_1.bytes_ = doc_1.audio_tensor.to_bytes()\n# from url\ndoc_2 = MyAudioDoc(\ntitle='my_second_audio_doc',\nurl='https://www.kozco.com/tech/piano2.wav',\n)\ndoc_2.audio_tensor, _ = doc_2.url.load()\n# doc_2.audio_tensor.save(file_path='/tmp/file_2.wav')\ndoc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/audio/audio_ndarray.py</code> <pre><code>@_register_proto(proto_type_name='audio_ndarray')\nclass AudioNdArray(AbstractAudioTensor, NdArray):\n\"\"\"\n    Subclass of [`NdArray`][docarray.typing.NdArray], to represent an audio tensor.\n    Adds audio-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import AudioBytes, AudioNdArray, AudioUrl\n    import numpy as np\n    class MyAudioDoc(BaseDoc):\n        title: str\n        audio_tensor: Optional[AudioNdArray] = None\n        url: Optional[AudioUrl] = None\n        bytes_: Optional[AudioBytes] = None\n    # from tensor\n    doc_1 = MyAudioDoc(\n        title='my_first_audio_doc',\n        audio_tensor=np.random.rand(1000, 2),\n    )\n    # doc_1.audio_tensor.save(file_path='/tmp/file_1.wav')\n    doc_1.bytes_ = doc_1.audio_tensor.to_bytes()\n    # from url\n    doc_2 = MyAudioDoc(\n        title='my_second_audio_doc',\n        url='https://www.kozco.com/tech/piano2.wav',\n    )\n    doc_2.audio_tensor, _ = doc_2.url.load()\n    # doc_2.audio_tensor.save(file_path='/tmp/file_2.wav')\n    doc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor","title":"<code>docarray.typing.tensor.audio.abstract_audio_tensor</code>","text":""},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor","title":"<code>AbstractAudioTensor</code>","text":"<p>             Bases: <code>AbstractTensor</code>, <code>ABC</code></p> Source code in <code>docarray/typing/tensor/audio/abstract_audio_tensor.py</code> <pre><code>class AbstractAudioTensor(AbstractTensor, ABC):\ndef to_bytes(self) -&gt; 'AudioBytes':\n\"\"\"\n        Convert audio tensor to [`AudioBytes`][docarray.typing.AudioBytes].\n        \"\"\"\nfrom docarray.typing.bytes.audio_bytes import AudioBytes\ntensor = self.get_comp_backend().to_numpy(self)\ntensor = (tensor * MAX_INT_16).astype('&lt;h')\nreturn AudioBytes(tensor.tobytes())\ndef save(\nself: 'T',\nfile_path: Union[str, BinaryIO],\nformat: str = 'wav',\nframe_rate: int = 44100,\nsample_width: int = 2,\npydub_args: Dict[str, Any] = {},\n) -&gt; None:\n\"\"\"\n        Save audio tensor to an audio file. Mono/stereo is preserved.\n        :param file_path: path to an audio file. If file is a string, open the file by\n            that name, otherwise treat it as a file-like object.\n        :param format: format for the audio file ('mp3', 'wav', 'raw', 'ogg' or other ffmpeg/avconv supported files)\n        :param frame_rate: sampling frequency\n        :param sample_width: sample width in bytes\n        :param pydub_args: dictionary of additional arguments for pydub.AudioSegment.export function\n        \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\ncomp_backend = self.get_comp_backend()\nchannels = 2 if comp_backend.n_dim(array=self) &gt; 1 else 1  # type: ignore\nsegment = AudioSegment(\nself.to_bytes(),\nframe_rate=frame_rate,\nsample_width=sample_width,\nchannels=channels,\n)\nsegment.export(file_path, format=format, **pydub_args)\ndef display(self, rate=44100):\n\"\"\"\n        Play audio data from tensor in notebook.\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\naudio_np = self.get_comp_backend().to_numpy(self)\ndisplay(Audio(audio_np, rate=rate))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.display","title":"<code>display(rate=44100)</code>","text":"<p>Play audio data from tensor in notebook.</p> Source code in <code>docarray/typing/tensor/audio/abstract_audio_tensor.py</code> <pre><code>def display(self, rate=44100):\n\"\"\"\n    Play audio data from tensor in notebook.\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Audio, display\naudio_np = self.get_comp_backend().to_numpy(self)\ndisplay(Audio(audio_np, rate=rate))\nelse:\nwarnings.warn('Display of audio is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>The computational backend compatible with this tensor type.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef get_comp_backend() -&gt; AbstractComputationalBackend:\n\"\"\"The computational backend compatible with this tensor type.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.save","title":"<code>save(file_path, format='wav', frame_rate=44100, sample_width=2, pydub_args={})</code>","text":"<p>Save audio tensor to an audio file. Mono/stereo is preserved.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, BinaryIO]</code> <p>path to an audio file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required <code>format</code> <code>str</code> <p>format for the audio file ('mp3', 'wav', 'raw', 'ogg' or other ffmpeg/avconv supported files)</p> <code>'wav'</code> <code>frame_rate</code> <code>int</code> <p>sampling frequency</p> <code>44100</code> <code>sample_width</code> <code>int</code> <p>sample width in bytes</p> <code>2</code> <code>pydub_args</code> <code>Dict[str, Any]</code> <p>dictionary of additional arguments for pydub.AudioSegment.export function</p> <code>{}</code> Source code in <code>docarray/typing/tensor/audio/abstract_audio_tensor.py</code> <pre><code>def save(\nself: 'T',\nfile_path: Union[str, BinaryIO],\nformat: str = 'wav',\nframe_rate: int = 44100,\nsample_width: int = 2,\npydub_args: Dict[str, Any] = {},\n) -&gt; None:\n\"\"\"\n    Save audio tensor to an audio file. Mono/stereo is preserved.\n    :param file_path: path to an audio file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    :param format: format for the audio file ('mp3', 'wav', 'raw', 'ogg' or other ffmpeg/avconv supported files)\n    :param frame_rate: sampling frequency\n    :param sample_width: sample width in bytes\n    :param pydub_args: dictionary of additional arguments for pydub.AudioSegment.export function\n    \"\"\"\npydub = import_library('pydub', raise_error=True)  # noqa: F841\nfrom pydub import AudioSegment\ncomp_backend = self.get_comp_backend()\nchannels = 2 if comp_backend.n_dim(array=self) &gt; 1 else 1  # type: ignore\nsegment = AudioSegment(\nself.to_bytes(),\nframe_rate=frame_rate,\nsample_width=sample_width,\nchannels=channels,\n)\nsegment.export(file_path, format=format, **pydub_args)\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.to_bytes","title":"<code>to_bytes()</code>","text":"<p>Convert audio tensor to <code>AudioBytes</code>.</p> Source code in <code>docarray/typing/tensor/audio/abstract_audio_tensor.py</code> <pre><code>def to_bytes(self) -&gt; 'AudioBytes':\n\"\"\"\n    Convert audio tensor to [`AudioBytes`][docarray.typing.AudioBytes].\n    \"\"\"\nfrom docarray.typing.bytes.audio_bytes import AudioBytes\ntensor = self.get_comp_backend().to_numpy(self)\ntensor = (tensor * MAX_INT_16).astype('&lt;h')\nreturn AudioBytes(tensor.tobytes())\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.to_protobuf","title":"<code>to_protobuf()</code>  <code>abstractmethod</code>","text":"<p>Convert DocList into a Protobuf message</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.abstract_audio_tensor.AbstractAudioTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_tensorflow_tensor","title":"<code>docarray.typing.tensor.audio.audio_tensorflow_tensor</code>","text":""},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_tensorflow_tensor.AudioTensorFlowTensor","title":"<code>AudioTensorFlowTensor</code>","text":"<p>             Bases: <code>AbstractAudioTensor</code>, <code>TensorFlowTensor</code></p> <p>Subclass of <code>TensorFlowTensor</code>, to represent an audio tensor. Adds audio-specific features to the tensor.</p> <pre><code>from typing import Optional\nimport tensorflow as tf\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioTensorFlowTensor, AudioUrl\nclass MyAudioDoc(BaseDoc):\ntitle: str\naudio_tensor: Optional[AudioTensorFlowTensor]\nurl: Optional[AudioUrl]\nbytes_: Optional[AudioBytes]\ndoc_1 = MyAudioDoc(\ntitle='my_first_audio_doc',\naudio_tensor=tf.random.normal((1000, 2)),\n)\n# doc_1.audio_tensor.save(file_path='file_1.wav')\ndoc_1.bytes_ = doc_1.audio_tensor.to_bytes()\ndoc_2 = MyAudioDoc(\ntitle='my_second_audio_doc',\nurl='https://www.kozco.com/tech/piano2.wav',\n)\ndoc_2.audio_tensor, _ = doc_2.url.load()\ndoc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/audio/audio_tensorflow_tensor.py</code> <pre><code>@_register_proto(proto_type_name='audio_tensorflow_tensor')\nclass AudioTensorFlowTensor(\nAbstractAudioTensor, TensorFlowTensor, metaclass=metaTensorFlow\n):\n\"\"\"\n    Subclass of [`TensorFlowTensor`][docarray.typing.TensorFlowTensor],\n    to represent an audio tensor. Adds audio-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    import tensorflow as tf\n    from docarray import BaseDoc\n    from docarray.typing import AudioBytes, AudioTensorFlowTensor, AudioUrl\n    class MyAudioDoc(BaseDoc):\n        title: str\n        audio_tensor: Optional[AudioTensorFlowTensor]\n        url: Optional[AudioUrl]\n        bytes_: Optional[AudioBytes]\n    doc_1 = MyAudioDoc(\n        title='my_first_audio_doc',\n        audio_tensor=tf.random.normal((1000, 2)),\n    )\n    # doc_1.audio_tensor.save(file_path='file_1.wav')\n    doc_1.bytes_ = doc_1.audio_tensor.to_bytes()\n    doc_2 = MyAudioDoc(\n        title='my_second_audio_doc',\n        url='https://www.kozco.com/tech/piano2.wav',\n    )\n    doc_2.audio_tensor, _ = doc_2.url.load()\n    doc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_torch_tensor","title":"<code>docarray.typing.tensor.audio.audio_torch_tensor</code>","text":""},{"location":"API_reference/typing/tensor/audio/#docarray.typing.tensor.audio.audio_torch_tensor.AudioTorchTensor","title":"<code>AudioTorchTensor</code>","text":"<p>             Bases: <code>AbstractAudioTensor</code>, <code>TorchTensor</code></p> <p>Subclass of <code>TorchTensor</code>, to represent an audio tensor. Adds audio-specific features to the tensor.</p> <pre><code>from typing import Optional\nimport torch\nfrom docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioTorchTensor, AudioUrl\nclass MyAudioDoc(BaseDoc):\ntitle: str\naudio_tensor: Optional[AudioTorchTensor] = None\nurl: Optional[AudioUrl] = None\nbytes_: Optional[AudioBytes] = None\ndoc_1 = MyAudioDoc(\ntitle='my_first_audio_doc',\naudio_tensor=torch.zeros(1000, 2),\n)\n# doc_1.audio_tensor.save(file_path='/tmp/file_1.wav')\ndoc_1.bytes_ = doc_1.audio_tensor.to_bytes()\ndoc_2 = MyAudioDoc(\ntitle='my_second_audio_doc',\nurl='https://www.kozco.com/tech/piano2.wav',\n)\ndoc_2.audio_tensor, _ = doc_2.url.load()\n# doc_2.audio_tensor.save(file_path='/tmp/file_2.wav')\ndoc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/audio/audio_torch_tensor.py</code> <pre><code>@_register_proto(proto_type_name='audio_torch_tensor')\nclass AudioTorchTensor(AbstractAudioTensor, TorchTensor, metaclass=metaTorchAndNode):\n\"\"\"\n    Subclass of [`TorchTensor`][docarray.typing.TorchTensor], to represent an audio tensor.\n    Adds audio-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    import torch\n    from docarray import BaseDoc\n    from docarray.typing import AudioBytes, AudioTorchTensor, AudioUrl\n    class MyAudioDoc(BaseDoc):\n        title: str\n        audio_tensor: Optional[AudioTorchTensor] = None\n        url: Optional[AudioUrl] = None\n        bytes_: Optional[AudioBytes] = None\n    doc_1 = MyAudioDoc(\n        title='my_first_audio_doc',\n        audio_tensor=torch.zeros(1000, 2),\n    )\n    # doc_1.audio_tensor.save(file_path='/tmp/file_1.wav')\n    doc_1.bytes_ = doc_1.audio_tensor.to_bytes()\n    doc_2 = MyAudioDoc(\n        title='my_second_audio_doc',\n        url='https://www.kozco.com/tech/piano2.wav',\n    )\n    doc_2.audio_tensor, _ = doc_2.url.load()\n    # doc_2.audio_tensor.save(file_path='/tmp/file_2.wav')\n    doc_2.bytes_ = doc_1.audio_tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/","title":"Embedding","text":""},{"location":"API_reference/typing/tensor/embedding/#embedding","title":"Embedding","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding","title":"<code>docarray.typing.tensor.embedding.embedding</code>","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding","title":"<code>AnyEmbedding</code>","text":"<p>             Bases: <code>AnyTensor</code>, <code>EmbeddingMixin</code></p> <p>Represents an embedding tensor object that can be used with TensorFlow, PyTorch, and NumPy type.</p> <p>'''python from docarray import BaseDoc from docarray.typing import AnyEmbedding</p> <p>class MyEmbeddingDoc(BaseDoc):     embedding: AnyEmbedding</p>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding--example-usage-with-tensorflow","title":"Example usage with TensorFlow:","text":"<p>import tensorflow as tf</p> <p>doc = MyEmbeddingDoc(embedding=tf.zeros(1000, 2)) type(doc.embedding)  # TensorFlowEmbedding</p>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding--example-usage-with-pytorch","title":"Example usage with PyTorch:","text":"<p>import torch</p> <p>doc = MyEmbeddingDoc(embedding=torch.zeros(1000, 2)) type(doc.embedding)  # TorchEmbedding</p>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding--example-usage-with-numpy","title":"Example usage with NumPy:","text":"<p>import numpy as np</p> <p>doc = MyEmbeddingDoc(embedding=np.zeros((1000, 2))) type(doc.embedding)  # NdArrayEmbedding '''</p> <p>Raises:     TypeError: If the type of the value is not one of [torch.Tensor, tensorflow.Tensor, numpy.ndarray]</p> Source code in <code>docarray/typing/tensor/embedding/embedding.py</code> <pre><code>class AnyEmbedding(AnyTensor, EmbeddingMixin):\n\"\"\"\n    Represents an embedding tensor object that can be used with TensorFlow, PyTorch, and NumPy type.\n    ---\n    '''python\n    from docarray import BaseDoc\n    from docarray.typing import AnyEmbedding\n    class MyEmbeddingDoc(BaseDoc):\n        embedding: AnyEmbedding\n    # Example usage with TensorFlow:\n    import tensorflow as tf\n    doc = MyEmbeddingDoc(embedding=tf.zeros(1000, 2))\n    type(doc.embedding)  # TensorFlowEmbedding\n    # Example usage with PyTorch:\n    import torch\n    doc = MyEmbeddingDoc(embedding=torch.zeros(1000, 2))\n    type(doc.embedding)  # TorchEmbedding\n    # Example usage with NumPy:\n    import numpy as np\n    doc = MyEmbeddingDoc(embedding=np.zeros((1000, 2)))\n    type(doc.embedding)  # NdArrayEmbedding\n    '''\n    ---\n    Raises:\n        TypeError: If the type of the value is not one of [torch.Tensor, tensorflow.Tensor, numpy.ndarray]\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, Any],\n):\nif torch_available:\nif isinstance(value, TorchTensor):\nreturn cast(TorchEmbedding, value)\nelif isinstance(value, torch.Tensor):\nreturn TorchEmbedding._docarray_from_native(value)  # noqa\nif tf_available:\nif isinstance(value, TensorFlowTensor):\nreturn cast(TensorFlowEmbedding, value)\nelif isinstance(value, tf.Tensor):\nreturn TensorFlowEmbedding._docarray_from_native(value)  # noqa\nif jax_available:\nif isinstance(value, JaxArray):\nreturn cast(JaxArrayEmbedding, value)\nelif isinstance(value, jnp.ndarray):\nreturn JaxArrayEmbedding._docarray_from_native(value)  # noqa\ntry:\nreturn NdArrayEmbedding._docarray_validate(value)\nexcept Exception:  # noqa\npass\nraise TypeError(\nf\"Expected one of [torch.Tensor, tensorflow.Tensor, numpy.ndarray] \"\nf\"compatible type, got {type(value)}\"\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding.AnyEmbedding.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.embedding_mixin","title":"<code>docarray.typing.tensor.embedding.embedding_mixin</code>","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.ndarray","title":"<code>docarray.typing.tensor.embedding.ndarray</code>","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.tensorflow","title":"<code>docarray.typing.tensor.embedding.tensorflow</code>","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch","title":"<code>docarray.typing.tensor.embedding.torch</code>","text":""},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding","title":"<code>TorchEmbedding</code>","text":"<p>             Bases: <code>TorchTensor</code>, <code>EmbeddingMixin</code></p> Source code in <code>docarray/typing/tensor/embedding/torch.py</code> <pre><code>@_register_proto(proto_type_name='torch_embedding')\nclass TorchEmbedding(TorchTensor, EmbeddingMixin, metaclass=metaTorchAndEmbedding):\nalternative_type = TorchTensor\ndef new_empty(self, *args, **kwargs):\n\"\"\"\n        This method enables the deepcopy of `TorchEmbedding` by returning another instance of this subclass.\n        If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.\n        \"\"\"\nreturn self.__class__(TorchTensor.new_empty(self, *args, **kwargs))\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def __deepcopy__(self, memo):\n\"\"\"\n    Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.\n    \"\"\"\n# Create a new tensor with the same data and properties\nnew_tensor = self.clone()\n# Set the class to the custom TorchTensor class\nnew_tensor.__class__ = self.__class__\nreturn new_tensor\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.from_ndarray","title":"<code>from_ndarray(value)</code>  <code>classmethod</code>","text":"<p>Create a <code>TorchTensor</code> from a numpy array</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>the numpy array</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TorchTensor` from a numpy array\n    :param value: the numpy array\n    :return: a `TorchTensor`\n    \"\"\"\nreturn cls._docarray_from_native(torch.from_numpy(value))\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg\n    :param pb_msg:\n    :return: a `TorchTensor`\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a TorchTensor')\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'TorchCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.torch_backend import TorchCompBackend\nreturn TorchCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.new_empty","title":"<code>new_empty(*args, **kwargs)</code>","text":"<p>This method enables the deepcopy of <code>TorchEmbedding</code> by returning another instance of this subclass. If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.</p> Source code in <code>docarray/typing/tensor/embedding/torch.py</code> <pre><code>def new_empty(self, *args, **kwargs):\n\"\"\"\n    This method enables the deepcopy of `TorchEmbedding` by returning another instance of this subclass.\n    If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.\n    \"\"\"\nreturn self.__class__(TorchTensor.new_empty(self, *args, **kwargs))\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into a <code>NdArrayProto</code> protobuf message</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into a `NdArrayProto` protobuf message\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.detach().cpu().numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/embedding/#docarray.typing.tensor.embedding.torch.TorchEmbedding.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original <code>torch.Tensor</code> without any memory copy.</p> <p>The original view rest intact and is still a Document <code>TorchTensor</code> but the return object is a pure <code>torch.Tensor</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import TorchTensor\nimport torch\nfrom pydantic import parse_obj_as\nt = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n# here t is a docarray TorchTensor\nt2 = t.unwrap()\n# here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n# But both share the same underlying memory\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>a <code>torch.Tensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def unwrap(self) -&gt; torch.Tensor:\n\"\"\"\n    Return the original `torch.Tensor` without any memory copy.\n    The original view rest intact and is still a Document `TorchTensor`\n    but the return object is a pure `torch.Tensor` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import TorchTensor\n    import torch\n    from pydantic import parse_obj_as\n    t = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n    # here t is a docarray TorchTensor\n    t2 = t.unwrap()\n    # here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n    # But both share the same underlying memory\n    ```\n    ---\n    :return: a `torch.Tensor`\n    \"\"\"\nvalue = copy(self)  # as unintuitive as it sounds, this\n# does not do any relevant memory copying, just shallow\n# reference to the torch data\nvalue.__class__ = torch.Tensor  # type: ignore\nreturn value\n</code></pre>"},{"location":"API_reference/typing/tensor/image/","title":"ImageTensor","text":""},{"location":"API_reference/typing/tensor/image/#imagetensor","title":"ImageTensor","text":""},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_ndarray","title":"<code>docarray.typing.tensor.image.image_ndarray</code>","text":""},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_ndarray.ImageNdArray","title":"<code>ImageNdArray</code>","text":"<p>             Bases: <code>AbstractImageTensor</code>, <code>NdArray</code></p> <p>Subclass of <code>NdArray</code>, to represent an image tensor. Adds image-specific features to the tensor. For instance the ability convert the tensor back to image bytes which are optimized to send over the wire.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageBytes, ImageNdArray, ImageUrl\nclass MyImageDoc(BaseDoc):\ntitle: str\ntensor: Optional[ImageNdArray] = None\nurl: Optional[ImageUrl] = None\nbytes: Optional[ImageBytes] = None\n# from url\ndoc = MyImageDoc(\ntitle='my_second_audio_doc',\nurl=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n)\ndoc.tensor = doc.url.load()\ndoc.bytes = doc.tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/image/image_ndarray.py</code> <pre><code>@_register_proto(proto_type_name='image_ndarray')\nclass ImageNdArray(AbstractImageTensor, NdArray):\n\"\"\"\n    Subclass of [`NdArray`][docarray.typing.NdArray], to represent an image tensor.\n    Adds image-specific features to the tensor.\n    For instance the ability convert the tensor back to image bytes which are\n    optimized to send over the wire.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import ImageBytes, ImageNdArray, ImageUrl\n    class MyImageDoc(BaseDoc):\n        title: str\n        tensor: Optional[ImageNdArray] = None\n        url: Optional[ImageUrl] = None\n        bytes: Optional[ImageBytes] = None\n    # from url\n    doc = MyImageDoc(\n        title='my_second_audio_doc',\n        url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n    )\n    doc.tensor = doc.url.load()\n    doc.bytes = doc.tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor","title":"<code>docarray.typing.tensor.image.abstract_image_tensor</code>","text":""},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor","title":"<code>AbstractImageTensor</code>","text":"<p>             Bases: <code>AbstractTensor</code>, <code>ABC</code></p> Source code in <code>docarray/typing/tensor/image/abstract_image_tensor.py</code> <pre><code>class AbstractImageTensor(AbstractTensor, ABC):\ndef to_bytes(self, format: str = 'PNG') -&gt; 'ImageBytes':\n\"\"\"\n        Convert image tensor to [`ImageBytes`][docarray.typing.ImageBytes].\n        :param format: the image format use to store the image, can be 'PNG' , 'JPG' ...\n        :return: an ImageBytes object\n        \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nif format == 'jpg':\nformat = 'jpeg'  # unify it to ISO standard\ntensor = self.get_comp_backend().to_numpy(self)\nmode = 'RGB' if tensor.ndim == 3 else 'L'\npil_image = PILImage.fromarray(tensor, mode=mode)\nwith io.BytesIO() as buffer:\npil_image.save(buffer, format=format)\nimg_byte_arr = buffer.getvalue()\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(img_byte_arr)\ndef save(self, file_path: str) -&gt; None:\n\"\"\"\n        Save image tensor to an image file.\n        :param file_path: path to an image file. If file is a string, open the file by\n            that name, otherwise treat it as a file-like object.\n        \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\ncomp_backend = self.get_comp_backend()\nnp_img = comp_backend.to_numpy(self).astype(np.uint8)\npil_img = PILImage.fromarray(np_img)\npil_img.save(file_path)\ndef display(self) -&gt; None:\n\"\"\"\n        Display image data from tensor in notebook.\n        \"\"\"\nif is_notebook():\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nnp_array = self.get_comp_backend().to_numpy(self)\nimg = PILImage.fromarray(np_array)\nfrom IPython.display import display\ndisplay(img)\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.display","title":"<code>display()</code>","text":"<p>Display image data from tensor in notebook.</p> Source code in <code>docarray/typing/tensor/image/abstract_image_tensor.py</code> <pre><code>def display(self) -&gt; None:\n\"\"\"\n    Display image data from tensor in notebook.\n    \"\"\"\nif is_notebook():\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nnp_array = self.get_comp_backend().to_numpy(self)\nimg = PILImage.fromarray(np_array)\nfrom IPython.display import display\ndisplay(img)\nelse:\nwarnings.warn('Display of image is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>The computational backend compatible with this tensor type.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef get_comp_backend() -&gt; AbstractComputationalBackend:\n\"\"\"The computational backend compatible with this tensor type.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.save","title":"<code>save(file_path)</code>","text":"<p>Save image tensor to an image file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to an image file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required Source code in <code>docarray/typing/tensor/image/abstract_image_tensor.py</code> <pre><code>def save(self, file_path: str) -&gt; None:\n\"\"\"\n    Save image tensor to an image file.\n    :param file_path: path to an image file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\ncomp_backend = self.get_comp_backend()\nnp_img = comp_backend.to_numpy(self).astype(np.uint8)\npil_img = PILImage.fromarray(np_img)\npil_img.save(file_path)\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.to_bytes","title":"<code>to_bytes(format='PNG')</code>","text":"<p>Convert image tensor to <code>ImageBytes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>the image format use to store the image, can be 'PNG' , 'JPG' ...</p> <code>'PNG'</code> <p>Returns:</p> Type Description <code>ImageBytes</code> <p>an ImageBytes object</p> Source code in <code>docarray/typing/tensor/image/abstract_image_tensor.py</code> <pre><code>def to_bytes(self, format: str = 'PNG') -&gt; 'ImageBytes':\n\"\"\"\n    Convert image tensor to [`ImageBytes`][docarray.typing.ImageBytes].\n    :param format: the image format use to store the image, can be 'PNG' , 'JPG' ...\n    :return: an ImageBytes object\n    \"\"\"\nPIL = import_library('PIL', raise_error=True)  # noqa: F841\nfrom PIL import Image as PILImage\nif format == 'jpg':\nformat = 'jpeg'  # unify it to ISO standard\ntensor = self.get_comp_backend().to_numpy(self)\nmode = 'RGB' if tensor.ndim == 3 else 'L'\npil_image = PILImage.fromarray(tensor, mode=mode)\nwith io.BytesIO() as buffer:\npil_image.save(buffer, format=format)\nimg_byte_arr = buffer.getvalue()\nfrom docarray.typing.bytes.image_bytes import ImageBytes\nreturn ImageBytes(img_byte_arr)\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.to_protobuf","title":"<code>to_protobuf()</code>  <code>abstractmethod</code>","text":"<p>Convert DocList into a Protobuf message</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.abstract_image_tensor.AbstractImageTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_tensorflow_tensor","title":"<code>docarray.typing.tensor.image.image_tensorflow_tensor</code>","text":""},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_tensorflow_tensor.ImageTensorFlowTensor","title":"<code>ImageTensorFlowTensor</code>","text":"<p>             Bases: <code>TensorFlowTensor</code>, <code>AbstractImageTensor</code></p> <p>Subclass of <code>TensorFlowTensor</code>, to represent an image tensor. Adds image-specific features to the tensor. For instance the ability convert the tensor back to <code>ImageBytes</code> which are optimized to send over the wire.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageBytes, ImageTensorFlowTensor, ImageUrl\nclass MyImageDoc(BaseDoc):\ntitle: str\ntensor: Optional[ImageTensorFlowTensor]\nurl: Optional[ImageUrl]\nbytes: Optional[ImageBytes]\ndoc = MyImageDoc(\ntitle='my_second_image_doc',\nurl=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n)\ndoc.tensor = doc.url.load()\ndoc.bytes = doc.tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/image/image_tensorflow_tensor.py</code> <pre><code>@_register_proto(proto_type_name='image_tensorflow_tensor')\nclass ImageTensorFlowTensor(\nTensorFlowTensor, AbstractImageTensor, metaclass=metaTensorFlow\n):\n\"\"\"\n    Subclass of [`TensorFlowTensor`][docarray.typing.TensorFlowTensor],\n    to represent an image tensor. Adds image-specific features to the tensor.\n    For instance the ability convert the tensor back to\n    [`ImageBytes`][docarray.typing.ImageBytes] which are\n    optimized to send over the wire.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import ImageBytes, ImageTensorFlowTensor, ImageUrl\n    class MyImageDoc(BaseDoc):\n        title: str\n        tensor: Optional[ImageTensorFlowTensor]\n        url: Optional[ImageUrl]\n        bytes: Optional[ImageBytes]\n    doc = MyImageDoc(\n        title='my_second_image_doc',\n        url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n    )\n    doc.tensor = doc.url.load()\n    doc.bytes = doc.tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_torch_tensor","title":"<code>docarray.typing.tensor.image.image_torch_tensor</code>","text":""},{"location":"API_reference/typing/tensor/image/#docarray.typing.tensor.image.image_torch_tensor.ImageTorchTensor","title":"<code>ImageTorchTensor</code>","text":"<p>             Bases: <code>AbstractImageTensor</code>, <code>TorchTensor</code></p> <p>Subclass of <code>TorchTensor</code>, to represent an image tensor. Adds image-specific features to the tensor. For instance the ability convert the tensor back to <code>ImageBytes</code> which are optimized to send over the wire.</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageBytes, ImageTorchTensor, ImageUrl\nclass MyImageDoc(BaseDoc):\ntitle: str\ntensor: Optional[ImageTorchTensor] = None\nurl: Optional[ImageUrl] = None\nbytes: Optional[ImageBytes] = None\ndoc = MyImageDoc(\ntitle='my_second_image_doc',\nurl=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n\"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n)\ndoc.tensor = doc.url.load()\ndoc.bytes = doc.tensor.to_bytes()\n</code></pre> Source code in <code>docarray/typing/tensor/image/image_torch_tensor.py</code> <pre><code>@_register_proto(proto_type_name='image_torch_tensor')\nclass ImageTorchTensor(AbstractImageTensor, TorchTensor, metaclass=metaTorchAndNode):\n\"\"\"\n    Subclass of [`TorchTensor`][docarray.typing.TorchTensor], to represent an image tensor.\n    Adds image-specific features to the tensor.\n    For instance the ability convert the tensor back to\n    [`ImageBytes`][docarray.typing.ImageBytes] which are\n    optimized to send over the wire.\n    ---\n    ```python\n    from typing import Optional\n    from docarray import BaseDoc\n    from docarray.typing import ImageBytes, ImageTorchTensor, ImageUrl\n    class MyImageDoc(BaseDoc):\n        title: str\n        tensor: Optional[ImageTorchTensor] = None\n        url: Optional[ImageUrl] = None\n        bytes: Optional[ImageBytes] = None\n    doc = MyImageDoc(\n        title='my_second_image_doc',\n        url=\"https://upload.wikimedia.org/wikipedia/commons/8/80/\"\n        \"Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg\",\n    )\n    doc.tensor = doc.url.load()\n    doc.bytes = doc.tensor.to_bytes()\n    ```\n    ---\n    \"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/","title":"Tensor","text":""},{"location":"API_reference/typing/tensor/tensor/#tensor","title":"Tensor","text":""},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor","title":"<code>docarray.typing.tensor.abstract_tensor</code>","text":""},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor","title":"<code>AbstractTensor</code>","text":"<p>             Bases: <code>Generic[TTensor, T]</code>, <code>AbstractType</code>, <code>ABC</code>, <code>Sized</code></p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>class AbstractTensor(Generic[TTensor, T], AbstractType, ABC, Sized):\n__parametrized_meta__: type = _ParametrizedMeta\n__unparametrizedcls__: Optional[Type['AbstractTensor']] = None\n__docarray_target_shape__: Optional[Tuple[int, ...]] = None\n_proto_type_name: str\ndef _to_node_protobuf(self: T) -&gt; 'NodeProto':\n\"\"\"Convert itself into a NodeProto protobuf message. This function should\n        be called when the Document is nested into another Document that need to be\n        converted into a protobuf\n        :return: the nested item protobuf message\n        \"\"\"\nfrom docarray.proto import NodeProto\nnd_proto = self.to_protobuf()\nreturn NodeProto(ndarray=nd_proto, type=self._proto_type_name)\n@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n        enable syntax of the form AnyTensor[shape].\n        It is called when a tensor is assigned to a field of this type.\n        i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n        The intended behaviour is as follows:\n        - If the shape of `t` is equal to `shape`, return `t`.\n        - If the shape of `t` is not equal to `shape`,\n            but can be reshaped to `shape`, return `t` reshaped to `shape`.\n        - If the shape of `t` is not equal to `shape`\n            and cannot be reshaped to `shape`, raise a ValueError.\n        :param t: The tensor to validate.\n        :param shape: The shape to validate against.\n        :return: The validated tensor.\n        \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n        It is called at \"class creation time\",\n        i.e. when a class is created with syntax of the form AnyTensor[shape].\n        The default implementation tries to cast any `item` to a tuple of ints.\n        A subclass can override this method to implement custom validation logic.\n        The output of this is eventually passed to\n        [`AbstractTensor.__docarray_validate_shape__`]\n        [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n        as its `shape` argument.\n        Raises `ValueError` if the input `item` does not pass validation.\n        :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n        :return: The validated item == the target shape of this tensor.\n        \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\nif is_pydantic_v2:\n@classmethod\ndef __get_pydantic_json_schema__(\ncls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n) -&gt; Dict[str, Any]:\njson_schema = {}\njson_schema.update(type='array', items={'type': 'number'})\nif cls.__docarray_target_shape__ is not None:\nshape_info = (\n'['\n+ ', '.join([str(s) for s in cls.__docarray_target_shape__])\n+ ']'\n)\nif (\nreduce(mul, cls.__docarray_target_shape__, 1)\n&lt;= DISPLAY_TENSOR_OPENAPI_MAX_ITEMS\n):\n# custom example only for 'small' shapes, otherwise it is too big to display\nexample_payload = orjson_dumps(\nnp.zeros(cls.__docarray_target_shape__)\n).decode()\njson_schema.update(example=example_payload)\nelse:\nshape_info = 'not specified'\njson_schema['tensor/array shape'] = shape_info\nreturn json_schema\nelse:\n@classmethod\ndef __modify_schema__(cls, field_schema: Dict[str, Any]) -&gt; None:\nfield_schema.update(type='array', items={'type': 'number'})\nif cls.__docarray_target_shape__ is not None:\nshape_info = (\n'['\n+ ', '.join([str(s) for s in cls.__docarray_target_shape__])\n+ ']'\n)\nif (\nreduce(mul, cls.__docarray_target_shape__, 1)\n&lt;= DISPLAY_TENSOR_OPENAPI_MAX_ITEMS\n):\n# custom example only for 'small' shapes, otherwise it is too big to display\nexample_payload = orjson_dumps(\nnp.zeros(cls.__docarray_target_shape__)\n).decode()\nfield_schema.update(example=example_payload)\nelse:\nshape_info = 'not specified'\nfield_schema['tensor/array shape'] = shape_info\n@classmethod\ndef _docarray_create_parametrized_type(cls: Type[T], shape: Tuple[int]):\nshape_str = ', '.join([str(s) for s in shape])\nclass _ParametrizedTensor(\ncls,  # type: ignore\nmetaclass=cls.__parametrized_meta__,  # type: ignore\n):\n__unparametrizedcls__ = cls\n__docarray_target_shape__ = shape\n@classmethod\ndef _docarray_validate(\n_cls,\nvalue: Any,\n):\nt = super()._docarray_validate(value)\nreturn _cls.__docarray_validate_shape__(\nt, _cls.__docarray_target_shape__\n)\n_ParametrizedTensor.__name__ = f'{cls.__name__}[{shape_str}]'\n_ParametrizedTensor.__qualname__ = f'{cls.__qualname__}[{shape_str}]'\nreturn _ParametrizedTensor\ndef __class_getitem__(cls, item: Any):\ntarget_shape = cls.__docarray_validate_getitem__(item)\nreturn cls._docarray_create_parametrized_type(target_shape)\n@classmethod\ndef _docarray_stack(cls: Type[T], seq: Union[List[T], Tuple[T]]) -&gt; T:\n\"\"\"Stack a sequence of tensors into a single tensor.\"\"\"\ncomp_backend = cls.get_comp_backend()\n# at runtime, 'T' is always the correct input type for .stack()\n# but mypy doesn't know that, so we ignore it here\nreturn cls._docarray_from_native(comp_backend.stack(seq))  # type: ignore\n@classmethod\n@abc.abstractmethod\ndef _docarray_from_native(cls: Type[T], value: Any) -&gt; T:\n\"\"\"\n        Create a DocList tensor from a tensor that is native to the given framework,\n        e.g. from numpy.ndarray or torch.Tensor.\n        \"\"\"\n...\n@staticmethod\n@abc.abstractmethod\ndef get_comp_backend() -&gt; AbstractComputationalBackend:\n\"\"\"The computational backend compatible with this tensor type.\"\"\"\n...\n@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n@abc.abstractmethod\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\ndef unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n@abc.abstractmethod\ndef _docarray_to_json_compatible(self):\n\"\"\"\n        Convert tensor into a json compatible object\n        :return: a representation of the tensor compatible with orjson\n        \"\"\"\nreturn self\n@classmethod\n@abc.abstractmethod\ndef _docarray_from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `tensor from a numpy array\n        PS: this function is different from `from_ndarray` because it is private under the docarray namesapce.\n        This allows us to avoid breaking change if one day we introduce a Tensor backend with a `from_ndarray` method.\n        \"\"\"\n...\n@abc.abstractmethod\ndef _docarray_to_ndarray(self) -&gt; np.ndarray:\n\"\"\"cast itself to a numpy array\"\"\"\n...\nif is_pydantic_v2:\n@classmethod\ndef __get_pydantic_core_schema__(\ncls, _source_type: Any, handler: GetCoreSchemaHandler\n) -&gt; core_schema.CoreSchema:\nreturn core_schema.general_plain_validator_function(\ncls.validate,\nserialization=core_schema.plain_serializer_function_ser_schema(\nfunction=orjson_dumps,\nreturn_schema=handler.generate_schema(bytes),\nwhen_used=\"json-unless-none\",\n),\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>The computational backend compatible with this tensor type.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef get_comp_backend() -&gt; AbstractComputationalBackend:\n\"\"\"The computational backend compatible with this tensor type.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.to_protobuf","title":"<code>to_protobuf()</code>  <code>abstractmethod</code>","text":"<p>Convert DocList into a Protobuf message</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.abstract_tensor.AbstractTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray","title":"<code>docarray.typing.tensor.ndarray</code>","text":""},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray","title":"<code>NdArray</code>","text":"<p>             Bases: <code>ndarray</code>, <code>AbstractTensor</code>, <code>Generic[ShapeT]</code></p> <p>Subclass of <code>np.ndarray</code>, intended for use in a Document. This enables (de)serialization from/to protobuf and json, data validation, and coercion from compatible types like <code>torch.Tensor</code>.</p> <p>This type can also be used in a parametrized way, specifying the shape of the array.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nimport numpy as np\nclass MyDoc(BaseDoc):\narr: NdArray\nimage_arr: NdArray[3, 224, 224]\nsquare_crop: NdArray[3, 'x', 'x']\nrandom_image: NdArray[3, ...]  # first dimension is fixed, can have arbitrary shape\n# create a document with tensors\ndoc = MyDoc(\narr=np.zeros((128,)),\nimage_arr=np.zeros((3, 224, 224)),\nsquare_crop=np.zeros((3, 64, 64)),\nrandom_image=np.zeros((3, 128, 256)),\n)\nassert doc.image_arr.shape == (3, 224, 224)\n# automatic shape conversion\ndoc = MyDoc(\narr=np.zeros((128,)),\nimage_arr=np.zeros((224, 224, 3)),  # will reshape to (3, 224, 224)\nsquare_crop=np.zeros((3, 128, 128)),\nrandom_image=np.zeros((3, 64, 128)),\n)\nassert doc.image_arr.shape == (3, 224, 224)\n# !! The following will raise an error due to shape mismatch !!\nfrom pydantic import ValidationError\ntry:\ndoc = MyDoc(\narr=np.zeros((128,)),\nimage_arr=np.zeros((224, 224)),  # this will fail validation\nsquare_crop=np.zeros((3, 128, 64)),  # this will also fail validation\nrandom_image=np.zeros((4, 64, 128)),  # this will also fail validation\n)\nexcept ValidationError as e:\npass\n</code></pre> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>@_register_proto(proto_type_name='ndarray')\nclass NdArray(np.ndarray, AbstractTensor, Generic[ShapeT]):\n\"\"\"\n    Subclass of `np.ndarray`, intended for use in a Document.\n    This enables (de)serialization from/to protobuf and json, data validation,\n    and coercion from compatible types like `torch.Tensor`.\n    This type can also be used in a parametrized way, specifying the shape of the array.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import NdArray\n    import numpy as np\n    class MyDoc(BaseDoc):\n        arr: NdArray\n        image_arr: NdArray[3, 224, 224]\n        square_crop: NdArray[3, 'x', 'x']\n        random_image: NdArray[3, ...]  # first dimension is fixed, can have arbitrary shape\n    # create a document with tensors\n    doc = MyDoc(\n        arr=np.zeros((128,)),\n        image_arr=np.zeros((3, 224, 224)),\n        square_crop=np.zeros((3, 64, 64)),\n        random_image=np.zeros((3, 128, 256)),\n    )\n    assert doc.image_arr.shape == (3, 224, 224)\n    # automatic shape conversion\n    doc = MyDoc(\n        arr=np.zeros((128,)),\n        image_arr=np.zeros((224, 224, 3)),  # will reshape to (3, 224, 224)\n        square_crop=np.zeros((3, 128, 128)),\n        random_image=np.zeros((3, 64, 128)),\n    )\n    assert doc.image_arr.shape == (3, 224, 224)\n    # !! The following will raise an error due to shape mismatch !!\n    from pydantic import ValidationError\n    try:\n        doc = MyDoc(\n            arr=np.zeros((128,)),\n            image_arr=np.zeros((224, 224)),  # this will fail validation\n            square_crop=np.zeros((3, 128, 64)),  # this will also fail validation\n            random_image=np.zeros((4, 64, 128)),  # this will also fail validation\n        )\n    except ValidationError as e:\n        pass\n    ```\n    ---\n    \"\"\"\n__parametrized_meta__ = metaNumpy\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, str, List[Any], Tuple[Any], Any],\n) -&gt; T:\nif isinstance(value, str):\nvalue = orjson.loads(value)\nif isinstance(value, np.ndarray):\nreturn cls._docarray_from_native(value)\nelif isinstance(value, NdArray):\nreturn cast(T, value)\nelif isinstance(value, AbstractTensor):\nreturn cls._docarray_from_native(value._docarray_to_ndarray())\nelif torch_available and isinstance(value, torch.Tensor):\nreturn cls._docarray_from_native(value.detach().cpu().numpy())\nelif tf_available and isinstance(value, tf.Tensor):\nreturn cls._docarray_from_native(value.numpy())\nelif jax_available and isinstance(value, jnp.ndarray):\nreturn cls._docarray_from_native(value.__array__())\nelif isinstance(value, list) or isinstance(value, tuple):\ntry:\narr_from_list: np.ndarray = np.asarray(value)\nreturn cls._docarray_from_native(arr_from_list)\nexcept Exception:\npass  # handled below\ntry:\narr: np.ndarray = np.ndarray(value)\nreturn cls._docarray_from_native(arr)\nexcept Exception:\npass  # handled below\nraise ValueError(f'Expected a numpy.ndarray compatible type, got {type(value)}')\n@classmethod\ndef _docarray_from_native(cls: Type[T], value: np.ndarray) -&gt; T:\nif cls.__unparametrizedcls__:  # This is not None if the tensor is parametrized\nreturn cast(T, value.view(cls.__unparametrizedcls__))\nreturn value.view(cls)\ndef _docarray_to_json_compatible(self) -&gt; np.ndarray:\n\"\"\"\n        Convert `NdArray` into a json compatible object\n        :return: a representation of the tensor compatible with orjson\n        \"\"\"\nreturn self.unwrap()\ndef unwrap(self) -&gt; np.ndarray:\n\"\"\"\n        Return the original ndarray without any memory copy.\n        The original view rest intact and is still a Document `NdArray`\n        but the return object is a pure `np.ndarray` but both object share\n        the same memory layout.\n        ---\n        ```python\n        from docarray.typing import NdArray\n        import numpy as np\n        from pydantic import parse_obj_as\n        t1 = parse_obj_as(NdArray, np.zeros((3, 224, 224)))\n        t2 = t1.unwrap()\n        # here t2 is a pure np.ndarray but t1 is still a Docarray NdArray\n        # But both share the same underlying memory\n        ```\n        ---\n        :return: a `numpy.ndarray`\n        \"\"\"\nreturn self.view(np.ndarray)\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n        Read ndarray from a proto msg\n        :param pb_msg:\n        :return: a numpy array\n        \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls._docarray_from_native(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls._docarray_from_native(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a NdArray')\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n        Transform self into a NdArrayProto protobuf message\n        \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nnd_proto.dense.buffer = self.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(self.shape))\nnd_proto.dense.dtype = self.dtype.str\nreturn nd_proto\n@staticmethod\ndef get_comp_backend() -&gt; 'NumpyCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.numpy_backend import NumpyCompBackend\nreturn NumpyCompBackend()\ndef __class_getitem__(cls, item: Any, *args, **kwargs):\n# see here for mypy bug: https://github.com/python/mypy/issues/14123\nreturn AbstractTensor.__class_getitem__.__func__(cls, item)  # type: ignore\n@classmethod\ndef _docarray_from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `tensor from a numpy array\n        PS: this function is different from `from_ndarray` because it is private under the docarray namesapce.\n        This allows us to avoid breaking change if one day we introduce a Tensor backend with a `from_ndarray` method.\n        \"\"\"\nreturn cls._docarray_from_native(value)\ndef _docarray_to_ndarray(self) -&gt; np.ndarray:\n\"\"\"Create a `tensor from a numpy array\n        PS: this function is different from `from_ndarray` because it is private under the docarray namesapce.\n        This allows us to avoid breaking change if one day we introduce a Tensor backend with a `from_ndarray` method.\n        \"\"\"\nreturn self.unwrap()\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a numpy array</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg\n    :param pb_msg:\n    :return: a numpy array\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls._docarray_from_native(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls._docarray_from_native(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a NdArray')\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'NumpyCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.numpy_backend import NumpyCompBackend\nreturn NumpyCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into a NdArrayProto protobuf message</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into a NdArrayProto protobuf message\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nnd_proto.dense.buffer = self.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(self.shape))\nnd_proto.dense.dtype = self.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.ndarray.NdArray.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original ndarray without any memory copy.</p> <p>The original view rest intact and is still a Document <code>NdArray</code> but the return object is a pure <code>np.ndarray</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import NdArray\nimport numpy as np\nfrom pydantic import parse_obj_as\nt1 = parse_obj_as(NdArray, np.zeros((3, 224, 224)))\nt2 = t1.unwrap()\n# here t2 is a pure np.ndarray but t1 is still a Docarray NdArray\n# But both share the same underlying memory\n</code></pre> <p>Returns:</p> Type Description <code>ndarray</code> <p>a <code>numpy.ndarray</code></p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>def unwrap(self) -&gt; np.ndarray:\n\"\"\"\n    Return the original ndarray without any memory copy.\n    The original view rest intact and is still a Document `NdArray`\n    but the return object is a pure `np.ndarray` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import NdArray\n    import numpy as np\n    from pydantic import parse_obj_as\n    t1 = parse_obj_as(NdArray, np.zeros((3, 224, 224)))\n    t2 = t1.unwrap()\n    # here t2 is a pure np.ndarray but t1 is still a Docarray NdArray\n    # But both share the same underlying memory\n    ```\n    ---\n    :return: a `numpy.ndarray`\n    \"\"\"\nreturn self.view(np.ndarray)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor","title":"<code>docarray.typing.tensor.tensorflow_tensor</code>","text":""},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor","title":"<code>TensorFlowTensor</code>","text":"<p>             Bases: <code>AbstractTensor</code>, <code>Generic[ShapeT]</code></p> <p>TensorFlowTensor class with a <code>.tensor</code> attribute of type <code>tf.Tensor</code>, intended for use in a Document.</p> <p>This enables (de)serialization from/to protobuf and json, data validation, and coercion from compatible types like numpy.ndarray.</p> <p>This type can also be used in a parametrized way, specifying the shape of the tensor.</p> <p>In comparison to <code>TorchTensor</code> and <code>NdArray</code>, <code>TensorFlowTensor</code> is not a subclass of <code>tf.Tensor</code> (or <code>torch.Tensor</code>, <code>np.ndarray</code> respectively). Instead, the <code>tf.Tensor</code> is stored in <code>TensorFlowTensor.tensor</code>. Therefore, to do operations on the actual tensor data you have to always access the <code>TensorFlowTensor.tensor</code> attribute.</p> <pre><code>import tensorflow as tf\nfrom docarray.typing import TensorFlowTensor\nt = TensorFlowTensor(tensor=tf.zeros((224, 224)))\n# tensorflow functions\nbroadcasted = tf.broadcast_to(t.tensor, (3, 224, 224))\nbroadcasted = tf.broadcast_to(t.unwrap(), (3, 224, 224))\n# this will fail:\n# broadcasted = tf.broadcast_to(t, (3, 224, 224))\n# tensorflow.Tensor methods:\narr = t.tensor.numpy()\narr = t.unwrap().numpy()\n# this will fail:\n# arr = t.numpy()\n</code></pre> <p>The [<code>TensorFlowBackend</code>] however, operates on our <code>TensorFlowTensor</code> instances. Here, you do not have to access the <code>.tensor</code> attribute, but can instead just hand over your <code>TensorFlowTensor</code> instance.</p> <pre><code>import tensorflow as tf\nfrom docarray.typing import TensorFlowTensor\nzeros = TensorFlowTensor(tensor=tf.zeros((3, 224, 224)))\ncomp_be = zeros.get_comp_backend()\nreshaped = comp_be.reshape(zeros, (224, 224, 3))\nassert comp_be.shape(reshaped) == (224, 224, 3)\n</code></pre> <p>You can use <code>TensorFlowTensor</code> in a Document as follows:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TensorFlowTensor\nimport tensorflow as tf\nclass MyDoc(BaseDoc):\ntensor: TensorFlowTensor\nimage_tensor: TensorFlowTensor[3, 224, 224]\nsquare_crop: TensorFlowTensor[3, 'x', 'x']\nrandom_image: TensorFlowTensor[\n3, ...\n]  # first dimension is fixed, can have arbitrary shape\n# create a document with tensors\ndoc = MyDoc(\ntensor=tf.zeros((128,)),\nimage_tensor=tf.zeros((3, 224, 224)),\nsquare_crop=tf.zeros((3, 64, 64)),\nrandom_image=tf.zeros((3, 128, 256)),\n)\n# automatic shape conversion\ndoc = MyDoc(\ntensor=tf.zeros((128,)),\nimage_tensor=tf.zeros((224, 224, 3)),  # will reshape to (3, 224, 224)\nsquare_crop=tf.zeros((3, 128, 128)),\nrandom_image=tf.zeros((3, 64, 128)),\n)\n# !! The following will raise an error due to shape mismatch !!\nfrom pydantic import ValidationError\ntry:\ndoc = MyDoc(\ntensor=tf.zeros((128,)),\nimage_tensor=tf.zeros((224, 224)),  # this will fail validation\nsquare_crop=tf.zeros((3, 128, 64)),  # this will also fail validation\nrandom_image=tf.zeros(4, 64, 128),  # this will also fail validation\n)\nexcept ValidationError as e:\npass\n</code></pre> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@_register_proto(proto_type_name='tensorflow_tensor')\nclass TensorFlowTensor(AbstractTensor, Generic[ShapeT], metaclass=metaTensorFlow):\n\"\"\"\n    TensorFlowTensor class with a `.tensor` attribute of type `tf.Tensor`,\n    intended for use in a Document.\n    This enables (de)serialization from/to protobuf and json, data validation,\n    and coercion from compatible types like numpy.ndarray.\n    This type can also be used in a parametrized way, specifying the shape of the\n    tensor.\n    In comparison to [`TorchTensor`][docarray.typing.TorchTensor] and\n    [`NdArray`][docarray.typing.tensor.ndarray.NdArray],\n    [`TensorFlowTensor`][docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor]\n    is not a subclass of `tf.Tensor` (or `torch.Tensor`, `np.ndarray` respectively).\n    Instead, the `tf.Tensor` is stored in\n    [`TensorFlowTensor.tensor`][docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor].\n    Therefore, to do operations on the actual tensor data you have to always access the\n    [`TensorFlowTensor.tensor`][docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor]\n    attribute.\n    ---\n    ```python\n    import tensorflow as tf\n    from docarray.typing import TensorFlowTensor\n    t = TensorFlowTensor(tensor=tf.zeros((224, 224)))\n    # tensorflow functions\n    broadcasted = tf.broadcast_to(t.tensor, (3, 224, 224))\n    broadcasted = tf.broadcast_to(t.unwrap(), (3, 224, 224))\n    # this will fail:\n    # broadcasted = tf.broadcast_to(t, (3, 224, 224))\n    # tensorflow.Tensor methods:\n    arr = t.tensor.numpy()\n    arr = t.unwrap().numpy()\n    # this will fail:\n    # arr = t.numpy()\n    ```\n    ---\n    The [`TensorFlowBackend`] however, operates on our\n    [`TensorFlowTensor`][docarray.typing.TensorFlowTensor] instances.\n    Here, you do not have to access the `.tensor` attribute,\n    but can instead just hand over your\n    [`TensorFlowTensor`][docarray.typing.TensorFlowTensor] instance.\n    ---\n    ```python\n    import tensorflow as tf\n    from docarray.typing import TensorFlowTensor\n    zeros = TensorFlowTensor(tensor=tf.zeros((3, 224, 224)))\n    comp_be = zeros.get_comp_backend()\n    reshaped = comp_be.reshape(zeros, (224, 224, 3))\n    assert comp_be.shape(reshaped) == (224, 224, 3)\n    ```\n    ---\n    You can use [`TensorFlowTensor`][docarray.typing.TensorFlowTensor] in a Document as follows:\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import TensorFlowTensor\n    import tensorflow as tf\n    class MyDoc(BaseDoc):\n        tensor: TensorFlowTensor\n        image_tensor: TensorFlowTensor[3, 224, 224]\n        square_crop: TensorFlowTensor[3, 'x', 'x']\n        random_image: TensorFlowTensor[\n            3, ...\n        ]  # first dimension is fixed, can have arbitrary shape\n    # create a document with tensors\n    doc = MyDoc(\n        tensor=tf.zeros((128,)),\n        image_tensor=tf.zeros((3, 224, 224)),\n        square_crop=tf.zeros((3, 64, 64)),\n        random_image=tf.zeros((3, 128, 256)),\n    )\n    # automatic shape conversion\n    doc = MyDoc(\n        tensor=tf.zeros((128,)),\n        image_tensor=tf.zeros((224, 224, 3)),  # will reshape to (3, 224, 224)\n        square_crop=tf.zeros((3, 128, 128)),\n        random_image=tf.zeros((3, 64, 128)),\n    )\n    # !! The following will raise an error due to shape mismatch !!\n    from pydantic import ValidationError\n    try:\n        doc = MyDoc(\n            tensor=tf.zeros((128,)),\n            image_tensor=tf.zeros((224, 224)),  # this will fail validation\n            square_crop=tf.zeros((3, 128, 64)),  # this will also fail validation\n            random_image=tf.zeros(4, 64, 128),  # this will also fail validation\n        )\n    except ValidationError as e:\n        pass\n    ```\n    ---\n    \"\"\"\n__parametrized_meta__ = metaTensorFlow\ndef __init__(self, tensor: tf.Tensor):\nsuper().__init__()\nself.tensor = tensor\ndef __getitem__(self, item):\nfrom docarray.computation.tensorflow_backend import TensorFlowCompBackend\ntensor = self.unwrap()\nif tensor is not None:\ntensor = tensor[item]\nreturn TensorFlowCompBackend._cast_output(t=tensor)\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor's `tf.Tensor`\"\"\"\nt = self.unwrap()\nvalue = tf.cast(value, dtype=t.dtype)\nvar = tf.Variable(t)\nvar[index].assign(value)\nself.tensor = tf.constant(var)\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor's `tf.Tensor`.\"\"\"\nfor i in range(len(self)):\nyield self[i]\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, str, Any],\n) -&gt; T:\nif isinstance(value, TensorFlowTensor):\nreturn cast(T, value)\nelif isinstance(value, tf.Tensor):\nreturn cls._docarray_from_native(value)\nelif isinstance(value, np.ndarray):\nreturn cls._docarray_from_ndarray(value)\nelif isinstance(value, AbstractTensor):\nreturn cls._docarray_from_ndarray(value._docarray_to_ndarray())\nelif torch_available and isinstance(value, torch.Tensor):\nreturn cls._docarray_from_native(value.detach().cpu().numpy())\nelif jax_available and isinstance(value, jnp.ndarray):\nreturn cls._docarray_from_native(value.__array__())\nelif isinstance(value, str):\nvalue = orjson.loads(value)\ntry:\narr: tf.Tensor = tf.constant(value)\nreturn cls(tensor=arr)\nexcept Exception:\npass  # handled below\nraise ValueError(\nf'Expected a tensorflow.Tensor compatible type, got {type(value)}'\n)\n@classmethod\ndef _docarray_from_native(cls: Type[T], value: Union[tf.Tensor, T]) -&gt; T:\n\"\"\"\n        Create a `TensorFlowTensor` from a `tf.Tensor` or `TensorFlowTensor`\n        instance.\n        :param value: instance of `tf.Tensor` or `TensorFlowTensor`\n        :return: a `TensorFlowTensor`\n        \"\"\"\nif isinstance(value, TensorFlowTensor):\nif cls.__unparametrizedcls__:  # None if the tensor is parametrized\nvalue.__class__ = cls.__unparametrizedcls__  # type: ignore\nelse:\nvalue.__class__ = cls\nreturn cast(T, value)\nelse:\nif cls.__unparametrizedcls__:  # None if the tensor is parametrized\ncls_param_ = cls.__unparametrizedcls__\ncls_param = cast(Type[T], cls_param_)\nelse:\ncls_param = cls\nreturn cls_param(tensor=value)\n@staticmethod\ndef get_comp_backend() -&gt; 'TensorFlowCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.tensorflow_backend import TensorFlowCompBackend\nreturn TensorFlowCompBackend()\ndef _docarray_to_json_compatible(self) -&gt; np.ndarray:\n\"\"\"\n        Convert `TensorFlowTensor` into a json compatible object\n        :return: a representation of the tensor compatible with orjson\n        \"\"\"\nreturn self.unwrap().numpy()\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n        Transform self into an NdArrayProto protobuf message.\n        \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.tensor.numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n        Read ndarray from a proto msg.\n        :param pb_msg:\n        :return: a `TensorFlowTensor`\n        \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(\nf'Proto message {pb_msg} cannot be cast to a TensorFlowTensor.'\n)\n@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TensorFlowTensor` from a numpy array.\n        :param value: the numpy array\n        :return: a `TensorFlowTensor`\n        \"\"\"\nreturn cls._docarray_from_native(tf.convert_to_tensor(value))\ndef unwrap(self) -&gt; tf.Tensor:\n\"\"\"\n        Return the original `tf.Tensor` without any memory copy.\n        The original view rest intact and is still a Document `TensorFlowTensor`\n        but the return object is a pure `tf.Tensor` but both object share\n        the same memory layout.\n        ---\n        ```python\n        from docarray.typing import TensorFlowTensor\n        import tensorflow as tf\n        t1 = TensorFlowTensor.validate(tf.zeros((3, 224, 224)), None, None)\n        # here t1 is a docarray TensorFlowTensor\n        t2 = t1.unwrap()\n        # here t2 is a pure tf.Tensor but t1 is still a Docarray TensorFlowTensor\n        ```\n        ---\n        :return: a `tf.Tensor`\n        \"\"\"\nreturn self.tensor\ndef __len__(self) -&gt; int:\nreturn len(self.tensor)\n@classmethod\ndef _docarray_from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `tensor from a numpy array\n        PS: this function is different from `from_ndarray` because it is private under the docarray namesapce.\n        This allows us to avoid breaking change if one day we introduce a Tensor backend with a `from_ndarray` method.\n        \"\"\"\nreturn cls.from_ndarray(value)\ndef _docarray_to_ndarray(self) -&gt; np.ndarray:\n\"\"\"cast itself to a numpy array\"\"\"\nreturn self.tensor.numpy()\n@property\ndef shape(self):\nreturn tf.shape(self.tensor)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the elements of this tensor's <code>tf.Tensor</code>.</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over the elements of this tensor's `tf.Tensor`.\"\"\"\nfor i in range(len(self)):\nyield self[i]\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.__setitem__","title":"<code>__setitem__(index, value)</code>","text":"<p>Set a slice of this tensor's <code>tf.Tensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor's `tf.Tensor`\"\"\"\nt = self.unwrap()\nvalue = tf.cast(value, dtype=t.dtype)\nvar = tf.Variable(t)\nvar[index].assign(value)\nself.tensor = tf.constant(var)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.from_ndarray","title":"<code>from_ndarray(value)</code>  <code>classmethod</code>","text":"<p>Create a <code>TensorFlowTensor</code> from a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>the numpy array</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TensorFlowTensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TensorFlowTensor` from a numpy array.\n    :param value: the numpy array\n    :return: a `TensorFlowTensor`\n    \"\"\"\nreturn cls._docarray_from_native(tf.convert_to_tensor(value))\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TensorFlowTensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg.\n    :param pb_msg:\n    :return: a `TensorFlowTensor`\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(\nf'Proto message {pb_msg} cannot be cast to a TensorFlowTensor.'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'TensorFlowCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.tensorflow_backend import TensorFlowCompBackend\nreturn TensorFlowCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into an NdArrayProto protobuf message.</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into an NdArrayProto protobuf message.\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.tensor.numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.tensorflow_tensor.TensorFlowTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original <code>tf.Tensor</code> without any memory copy.</p> <p>The original view rest intact and is still a Document <code>TensorFlowTensor</code> but the return object is a pure <code>tf.Tensor</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import TensorFlowTensor\nimport tensorflow as tf\nt1 = TensorFlowTensor.validate(tf.zeros((3, 224, 224)), None, None)\n# here t1 is a docarray TensorFlowTensor\nt2 = t1.unwrap()\n# here t2 is a pure tf.Tensor but t1 is still a Docarray TensorFlowTensor\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>a <code>tf.Tensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def unwrap(self) -&gt; tf.Tensor:\n\"\"\"\n    Return the original `tf.Tensor` without any memory copy.\n    The original view rest intact and is still a Document `TensorFlowTensor`\n    but the return object is a pure `tf.Tensor` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import TensorFlowTensor\n    import tensorflow as tf\n    t1 = TensorFlowTensor.validate(tf.zeros((3, 224, 224)), None, None)\n    # here t1 is a docarray TensorFlowTensor\n    t2 = t1.unwrap()\n    # here t2 is a pure tf.Tensor but t1 is still a Docarray TensorFlowTensor\n    ```\n    ---\n    :return: a `tf.Tensor`\n    \"\"\"\nreturn self.tensor\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor","title":"<code>docarray.typing.tensor.torch_tensor</code>","text":""},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor","title":"<code>TorchTensor</code>","text":"<p>             Bases: <code>Tensor</code>, <code>AbstractTensor</code>, <code>Generic[ShapeT]</code></p> <p>Subclass of <code>torch.Tensor</code>, intended for use in a Document. This enables (de)serialization from/to protobuf and json, data validation, and coercion from compatible types like numpy.ndarray.</p> <p>This type can also be used in a parametrized way, specifying the shape of the tensor.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor\nimport torch\nclass MyDoc(BaseDoc):\ntensor: TorchTensor\nimage_tensor: TorchTensor[3, 224, 224]\nsquare_crop: TorchTensor[3, 'x', 'x']\nrandom_image: TorchTensor[\n3, ...\n]  # first dimension is fixed, can have arbitrary shape\n# create a document with tensors\ndoc = MyDoc(\ntensor=torch.zeros(128),\nimage_tensor=torch.zeros(3, 224, 224),\nsquare_crop=torch.zeros(3, 64, 64),\nrandom_image=torch.zeros(3, 128, 256),\n)\n# automatic shape conversion\ndoc = MyDoc(\ntensor=torch.zeros(128),\nimage_tensor=torch.zeros(224, 224, 3),  # will reshape to (3, 224, 224)\nsquare_crop=torch.zeros(3, 128, 128),\nrandom_image=torch.zeros(3, 64, 128),\n)\n# !! The following will raise an error due to shape mismatch !!\nfrom pydantic import ValidationError\ntry:\ndoc = MyDoc(\ntensor=torch.zeros(128),\nimage_tensor=torch.zeros(224, 224),  # this will fail validation\nsquare_crop=torch.zeros(3, 128, 64),  # this will also fail validation\nrandom_image=torch.zeros(4, 64, 128),  # this will also fail validation\n)\nexcept ValidationError as e:\npass\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor--compatibility-with-torchcompile","title":"Compatibility with <code>torch.compile()</code>","text":"<p>PyTorch 2 introduced compilation support in the form of <code>torch.compile()</code>.</p> <p>Currently, <code>torch.compile()</code> does not properly support subclasses of <code>torch.Tensor</code> such as <code>TorchTensor</code>. The PyTorch team is currently working on a fix for this issue.</p> <p>In the meantime, you can use the following workaround:</p>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor--workaround-convert-torchtensor-to-torchtensor-before-calling-torchcompile","title":"Workaround: Convert <code>TorchTensor</code> to <code>torch.Tensor</code> before calling <code>torch.compile()</code>","text":"<p>Converting any <code>TorchTensor</code>s tor <code>torch.Tensor</code> before calling <code>torch.compile()</code> side-steps the issue:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor\nimport torch\nclass MyDoc(BaseDoc):\ntensor: TorchTensor\ndoc = MyDoc(tensor=torch.zeros(128))\ndef foo(tensor: torch.Tensor):\nreturn tensor @ tensor.t()\nfoo_compiled = torch.compile(foo)\n# unwrap the tensor before passing it to torch.compile()\nfoo_compiled(doc.tensor.unwrap())\n</code></pre> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@_register_proto(proto_type_name='torch_tensor')\nclass TorchTensor(\ntorch.Tensor,\nAbstractTensor,\nGeneric[ShapeT],\nmetaclass=metaTorchAndNode,\n):\n# Subclassing torch.Tensor following the advice from here:\n# https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor\n\"\"\"\n    Subclass of `torch.Tensor`, intended for use in a Document.\n    This enables (de)serialization from/to protobuf and json, data validation,\n    and coercion from compatible types like numpy.ndarray.\n    This type can also be used in a parametrized way,\n    specifying the shape of the tensor.\n    ---\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import TorchTensor\n    import torch\n    class MyDoc(BaseDoc):\n        tensor: TorchTensor\n        image_tensor: TorchTensor[3, 224, 224]\n        square_crop: TorchTensor[3, 'x', 'x']\n        random_image: TorchTensor[\n            3, ...\n        ]  # first dimension is fixed, can have arbitrary shape\n    # create a document with tensors\n    doc = MyDoc(\n        tensor=torch.zeros(128),\n        image_tensor=torch.zeros(3, 224, 224),\n        square_crop=torch.zeros(3, 64, 64),\n        random_image=torch.zeros(3, 128, 256),\n    )\n    # automatic shape conversion\n    doc = MyDoc(\n        tensor=torch.zeros(128),\n        image_tensor=torch.zeros(224, 224, 3),  # will reshape to (3, 224, 224)\n        square_crop=torch.zeros(3, 128, 128),\n        random_image=torch.zeros(3, 64, 128),\n    )\n    # !! The following will raise an error due to shape mismatch !!\n    from pydantic import ValidationError\n    try:\n        doc = MyDoc(\n            tensor=torch.zeros(128),\n            image_tensor=torch.zeros(224, 224),  # this will fail validation\n            square_crop=torch.zeros(3, 128, 64),  # this will also fail validation\n            random_image=torch.zeros(4, 64, 128),  # this will also fail validation\n        )\n    except ValidationError as e:\n        pass\n    ```\n    ---\n    ## Compatibility with `torch.compile()`\n    PyTorch 2 [introduced compilation support](https://pytorch.org/blog/pytorch-2.0-release/) in the form of `torch.compile()`.\n    Currently, **`torch.compile()` does not properly support subclasses of `torch.Tensor` such as `TorchTensor`**.\n    The PyTorch team is currently working on a [fix for this issue](https://github.com/pytorch/pytorch/pull/105167#issuecomment-1678050808).\n    In the meantime, you can use the following workaround:\n    ### Workaround: Convert `TorchTensor` to `torch.Tensor` before calling `torch.compile()`\n    Converting any `TorchTensor`s tor `torch.Tensor` before calling `torch.compile()` side-steps the issue:\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import TorchTensor\n    import torch\n    class MyDoc(BaseDoc):\n        tensor: TorchTensor\n    doc = MyDoc(tensor=torch.zeros(128))\n    def foo(tensor: torch.Tensor):\n        return tensor @ tensor.t()\n    foo_compiled = torch.compile(foo)\n    # unwrap the tensor before passing it to torch.compile()\n    foo_compiled(doc.tensor.unwrap())\n    ```\n    \"\"\"\n__parametrized_meta__ = metaTorchAndNode\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, str, Any],\n) -&gt; T:\nif isinstance(value, TorchTensor):\nreturn cast(T, value)\nelif isinstance(value, torch.Tensor):\nreturn cls._docarray_from_native(value)\nelif isinstance(value, AbstractTensor):\nreturn cls._docarray_from_ndarray(value._docarray_to_ndarray())\nelif tf_available and isinstance(value, tf.Tensor):\nreturn cls._docarray_from_ndarray(value.numpy())\nelif isinstance(value, np.ndarray):\nreturn cls._docarray_from_ndarray(value)\nelif jax_available and isinstance(value, jnp.ndarray):\nreturn cls._docarray_from_ndarray(value.__array__())\nelif isinstance(value, str):\nvalue = orjson.loads(value)\ntry:\narr: torch.Tensor = torch.tensor(value)\nreturn cls._docarray_from_native(arr)\nexcept Exception:\npass  # handled below\nraise ValueError(f'Expected a torch.Tensor compatible type, got {type(value)}')\ndef _docarray_to_json_compatible(self) -&gt; np.ndarray:\n\"\"\"\n        Convert `TorchTensor` into a json compatible object\n        :return: a representation of the tensor compatible with orjson\n        \"\"\"\nreturn self.detach().numpy()  # might need to check device later\ndef unwrap(self) -&gt; torch.Tensor:\n\"\"\"\n        Return the original `torch.Tensor` without any memory copy.\n        The original view rest intact and is still a Document `TorchTensor`\n        but the return object is a pure `torch.Tensor` but both object share\n        the same memory layout.\n        ---\n        ```python\n        from docarray.typing import TorchTensor\n        import torch\n        from pydantic import parse_obj_as\n        t = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n        # here t is a docarray TorchTensor\n        t2 = t.unwrap()\n        # here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n        # But both share the same underlying memory\n        ```\n        ---\n        :return: a `torch.Tensor`\n        \"\"\"\nvalue = copy(self)  # as unintuitive as it sounds, this\n# does not do any relevant memory copying, just shallow\n# reference to the torch data\nvalue.__class__ = torch.Tensor  # type: ignore\nreturn value\n@classmethod\ndef _docarray_from_native(cls: Type[T], value: torch.Tensor) -&gt; T:\n\"\"\"Create a `TorchTensor` from a native `torch.Tensor`\n        :param value: the native `torch.Tensor`\n        :return: a `TorchTensor`\n        \"\"\"\nif cls.__unparametrizedcls__:  # This is not None if the tensor is parametrized\nvalue.__class__ = cls.__unparametrizedcls__  # type: ignore\nelse:\nvalue.__class__ = cls\nreturn cast(T, value)\n@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TorchTensor` from a numpy array\n        :param value: the numpy array\n        :return: a `TorchTensor`\n        \"\"\"\nreturn cls._docarray_from_native(torch.from_numpy(value))\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n        Read ndarray from a proto msg\n        :param pb_msg:\n        :return: a `TorchTensor`\n        \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a TorchTensor')\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n        Transform self into a `NdArrayProto` protobuf message\n        \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.detach().cpu().numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n@staticmethod\ndef get_comp_backend() -&gt; 'TorchCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.torch_backend import TorchCompBackend\nreturn TorchCompBackend()\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n# this tells torch to treat all of our custom tensors just like\n# torch.Tensor's. Otherwise, torch will complain that it doesn't\n# know how to handle our custom tensor type.\ndocarray_torch_tensors = TorchTensor.__subclasses__()\ntypes_ = tuple(\ntorch.Tensor if t in docarray_torch_tensors else t for t in types\n)\nreturn super().__torch_function__(func, types_, args, kwargs)\ndef __deepcopy__(self, memo):\n\"\"\"\n        Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.\n        \"\"\"\n# Create a new tensor with the same data and properties\nnew_tensor = self.clone()\n# Set the class to the custom TorchTensor class\nnew_tensor.__class__ = self.__class__\nreturn new_tensor\n@classmethod\ndef _docarray_from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `tensor from a numpy array\n        PS: this function is different from `from_ndarray` because it is private under the docarray namesapce.\n        This allows us to avoid breaking change if one day we introduce a Tensor backend with a `from_ndarray` method.\n        \"\"\"\nreturn cls.from_ndarray(value)\ndef _docarray_to_ndarray(self) -&gt; np.ndarray:\n\"\"\"cast itself to a numpy array\"\"\"\nreturn self.detach().cpu().numpy()\ndef new_empty(self, *args, **kwargs):\n\"\"\"\n        This method enables the deepcopy of `TorchTensor` by returning another instance of this subclass.\n        If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.\n        \"\"\"\nreturn self.__class__(*args, **kwargs)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def __deepcopy__(self, memo):\n\"\"\"\n    Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.\n    \"\"\"\n# Create a new tensor with the same data and properties\nnew_tensor = self.clone()\n# Set the class to the custom TorchTensor class\nnew_tensor.__class__ = self.__class__\nreturn new_tensor\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.from_ndarray","title":"<code>from_ndarray(value)</code>  <code>classmethod</code>","text":"<p>Create a <code>TorchTensor</code> from a numpy array</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>the numpy array</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TorchTensor` from a numpy array\n    :param value: the numpy array\n    :return: a `TorchTensor`\n    \"\"\"\nreturn cls._docarray_from_native(torch.from_numpy(value))\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg\n    :param pb_msg:\n    :return: a `TorchTensor`\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a TorchTensor')\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'TorchCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.torch_backend import TorchCompBackend\nreturn TorchCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.new_empty","title":"<code>new_empty(*args, **kwargs)</code>","text":"<p>This method enables the deepcopy of <code>TorchTensor</code> by returning another instance of this subclass. If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def new_empty(self, *args, **kwargs):\n\"\"\"\n    This method enables the deepcopy of `TorchTensor` by returning another instance of this subclass.\n    If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.\n    \"\"\"\nreturn self.__class__(*args, **kwargs)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into a <code>NdArrayProto</code> protobuf message</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into a `NdArrayProto` protobuf message\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.detach().cpu().numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.torch_tensor.TorchTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original <code>torch.Tensor</code> without any memory copy.</p> <p>The original view rest intact and is still a Document <code>TorchTensor</code> but the return object is a pure <code>torch.Tensor</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import TorchTensor\nimport torch\nfrom pydantic import parse_obj_as\nt = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n# here t is a docarray TorchTensor\nt2 = t.unwrap()\n# here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n# But both share the same underlying memory\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>a <code>torch.Tensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def unwrap(self) -&gt; torch.Tensor:\n\"\"\"\n    Return the original `torch.Tensor` without any memory copy.\n    The original view rest intact and is still a Document `TorchTensor`\n    but the return object is a pure `torch.Tensor` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import TorchTensor\n    import torch\n    from pydantic import parse_obj_as\n    t = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n    # here t is a docarray TorchTensor\n    t2 = t.unwrap()\n    # here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n    # But both share the same underlying memory\n    ```\n    ---\n    :return: a `torch.Tensor`\n    \"\"\"\nvalue = copy(self)  # as unintuitive as it sounds, this\n# does not do any relevant memory copying, just shallow\n# reference to the torch data\nvalue.__class__ = torch.Tensor  # type: ignore\nreturn value\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.AnyTensor","title":"<code>docarray.typing.tensor.AnyTensor</code>","text":"<p>             Bases: <code>AbstractTensor</code>, <code>Generic[ShapeT]</code></p> <p>Represents a tensor object that can be used with TensorFlow, PyTorch, and NumPy type. !!! note:     when doing type checking (mypy or pycharm type checker), this class will actually be replace by a Union of the three     tensor types. You can reason about this class as if it was a Union.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AnyTensor\nclass MyTensorDoc(BaseDoc):\ntensor: AnyTensor\n# Example usage with TensorFlow:\n# import tensorflow as tf\n# doc = MyTensorDoc(tensor=tf.zeros(1000, 2))\n# Example usage with PyTorch:\nimport torch\ndoc = MyTensorDoc(tensor=torch.zeros(1000, 2))\n# Example usage with NumPy:\nimport numpy as np\ndoc = MyTensorDoc(tensor=np.zeros((1000, 2)))\n</code></pre> Source code in <code>docarray/typing/tensor/tensor.py</code> <pre><code>class AnyTensor(AbstractTensor, Generic[ShapeT]):\n\"\"\"\n    Represents a tensor object that can be used with TensorFlow, PyTorch, and NumPy type.\n    !!! note:\n        when doing type checking (mypy or pycharm type checker), this class will actually be replace by a Union of the three\n        tensor types. You can reason about this class as if it was a Union.\n    ```python\n    from docarray import BaseDoc\n    from docarray.typing import AnyTensor\n    class MyTensorDoc(BaseDoc):\n        tensor: AnyTensor\n    # Example usage with TensorFlow:\n    # import tensorflow as tf\n    # doc = MyTensorDoc(tensor=tf.zeros(1000, 2))\n    # Example usage with PyTorch:\n    import torch\n    doc = MyTensorDoc(tensor=torch.zeros(1000, 2))\n    # Example usage with NumPy:\n    import numpy as np\n    doc = MyTensorDoc(tensor=np.zeros((1000, 2)))\n    ```\n    \"\"\"\ndef __getitem__(self: T, item):\npass\ndef __setitem__(self, index, value):\npass\ndef __iter__(self):\npass\ndef __len__(self):\npass\n@classmethod\ndef _docarray_from_native(cls: Type[T], value: Any):\nraise RuntimeError(f'This method should not be called on {cls}.')\n@staticmethod\ndef get_comp_backend():\nraise RuntimeError('This method should not be called on AnyTensor.')\ndef to_protobuf(self):\nraise RuntimeError(f'This method should not be called on {self.__class__}.')\ndef _docarray_to_json_compatible(self):\nraise RuntimeError(f'This method should not be called on {self.__class__}.')\n@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: T):\nraise RuntimeError(f'This method should not be called on {cls}.')\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, Any],\n):\n# Check for TorchTensor first, then TensorFlowTensor, then NdArray\nif torch_available:\nif isinstance(value, TorchTensor):\nreturn value\nelif isinstance(value, torch.Tensor):\nreturn TorchTensor._docarray_from_native(value)  # noqa\nif tf_available:\nif isinstance(value, TensorFlowTensor):\nreturn value\nelif isinstance(value, tf.Tensor):\nreturn TensorFlowTensor._docarray_from_native(value)  # noqa\nif jax_available:\nif isinstance(value, JaxArray):\nreturn value\nelif isinstance(value, jnp.ndarray):\nreturn JaxArray._docarray_from_native(value)  # noqa\ntry:\nreturn NdArray._docarray_validate(value)\nexcept Exception as e:  # noqa\nprint(e)\npass\nraise TypeError(\nf\"Expected one of [torch.Tensor, tensorflow.Tensor, numpy.ndarray] \"\nf\"compatible type, got {type(value)}\"\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.AnyTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.AnyTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/tensor/#docarray.typing.tensor.AnyTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/video/","title":"VideoTensor","text":""},{"location":"API_reference/typing/tensor/video/#videotensor","title":"VideoTensor","text":""},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray","title":"<code>docarray.typing.tensor.video.video_ndarray</code>","text":""},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray","title":"<code>VideoNdArray</code>","text":"<p>             Bases: <code>NdArray</code>, <code>VideoTensorMixin</code></p> <p>Subclass of <code>NdArray</code>, to represent a video tensor. Adds video-specific features to the tensor.</p> <pre><code>from typing import Optional\nimport numpy as np\nfrom pydantic import parse_obj_as\nfrom docarray import BaseDoc\nfrom docarray.typing import VideoNdArray, VideoUrl\nclass MyVideoDoc(BaseDoc):\ntitle: str\nurl: Optional[VideoUrl] = None\nvideo_tensor: Optional[VideoNdArray] = None\ndoc_1 = MyVideoDoc(\ntitle='my_first_video_doc',\nvideo_tensor=np.random.random((100, 224, 224, 3)),\n)\ndoc_2 = MyVideoDoc(\ntitle='my_second_video_doc',\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n)\ndoc_2.video_tensor = parse_obj_as(VideoNdArray, doc_2.url.load().video)\n# doc_2.video_tensor.save(file_path='/tmp/file_2.mp4')\n</code></pre> Source code in <code>docarray/typing/tensor/video/video_ndarray.py</code> <pre><code>@_register_proto(proto_type_name='video_ndarray')\nclass VideoNdArray(NdArray, VideoTensorMixin):\n\"\"\"\n    Subclass of [`NdArray`][docarray.typing.NdArray], to represent a video tensor.\n    Adds video-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    import numpy as np\n    from pydantic import parse_obj_as\n    from docarray import BaseDoc\n    from docarray.typing import VideoNdArray, VideoUrl\n    class MyVideoDoc(BaseDoc):\n        title: str\n        url: Optional[VideoUrl] = None\n        video_tensor: Optional[VideoNdArray] = None\n    doc_1 = MyVideoDoc(\n        title='my_first_video_doc',\n        video_tensor=np.random.random((100, 224, 224, 3)),\n    )\n    doc_2 = MyVideoDoc(\n        title='my_second_video_doc',\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n    )\n    doc_2.video_tensor = parse_obj_as(VideoNdArray, doc_2.url.load().video)\n    # doc_2.video_tensor.save(file_path='/tmp/file_2.mp4')\n    ```\n    ---\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, List[Any], Tuple[Any], Any],\n) -&gt; T:\ntensor = super()._docarray_validate(value=value)\nreturn cls.validate_shape(value=tensor)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.display","title":"<code>display(audio=None)</code>","text":"<p>Display video data from tensor in notebook.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Optional[AudioTensor]</code> <p>sound to play with video tensor</p> <code>None</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def display(self, audio: Optional[AudioTensor] = None) -&gt; None:\n\"\"\"\n    Display video data from tensor in notebook.\n    :param audio: sound to play with video tensor\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Video, display\nb = self.to_bytes(audio_tensor=audio)\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a numpy array</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg\n    :param pb_msg:\n    :return: a numpy array\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls._docarray_from_native(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls._docarray_from_native(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a NdArray')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'NumpyCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.numpy_backend import NumpyCompBackend\nreturn NumpyCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.save","title":"<code>save(file_path, audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Save video tensor to a .mp4 file.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing.tensor.audio.audio_tensor import AudioTensor\nfrom docarray.typing.tensor.video.video_tensor import VideoTensor\nclass MyDoc(BaseDoc):\nvideo_tensor: VideoTensor\naudio_tensor: AudioTensor\ndoc = MyDoc(\nvideo_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\naudio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n)\ndoc.video_tensor.save(\nfile_path=\"/tmp/mp_.mp4\",\naudio_tensor=doc.audio_tensor,\naudio_format=\"flt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, BytesIO]</code> <p>path to a .mp4 file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def save(\nself: 'T',\nfile_path: Union[str, BytesIO],\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; None:\n\"\"\"\n    Save video tensor to a .mp4 file.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing.tensor.audio.audio_tensor import AudioTensor\n    from docarray.typing.tensor.video.video_tensor import VideoTensor\n    class MyDoc(BaseDoc):\n        video_tensor: VideoTensor\n        audio_tensor: AudioTensor\n    doc = MyDoc(\n        video_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\n        audio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n    )\n    doc.video_tensor.save(\n        file_path=\"/tmp/mp_.mp4\",\n        audio_tensor=doc.audio_tensor,\n        audio_format=\"flt\",\n    )\n    ```\n    ---\n    :param file_path: path to a .mp4 file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av', raise_error=True)\nnp_tensor = self.get_comp_backend().to_numpy(array=self)\nvideo_tensor = np_tensor.astype('uint8')\nif isinstance(file_path, str):\nformat = file_path.split('.')[-1]\nelse:\nformat = 'mp4'\nwith av.open(file_path, mode='w', format=format) as container:\nif video_tensor.ndim == 3:\nvideo_tensor = np.expand_dims(video_tensor, axis=0)\nstream_video = container.add_stream(video_codec, rate=video_frame_rate)\nstream_video.height = video_tensor.shape[-3]\nstream_video.width = video_tensor.shape[-2]\nif audio_tensor is not None:\nstream_audio = container.add_stream(audio_codec)\naudio_np = audio_tensor.get_comp_backend().to_numpy(array=audio_tensor)\naudio_layout = 'stereo' if audio_np.shape[-2] == 2 else 'mono'\nfor i, audio in enumerate(audio_np):\nframe = av.AudioFrame.from_ndarray(\narray=audio, format=audio_format, layout=audio_layout\n)\nframe.rate = audio_frame_rate\nframe.pts = audio.shape[-1] * i\nfor packet in stream_audio.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_audio.encode(None):\ncontainer.mux(packet)\nfor vid in video_tensor:\nframe = av.VideoFrame.from_ndarray(vid, format='rgb24')\nfor packet in stream_video.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_video.encode(None):\ncontainer.mux(packet)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.to_bytes","title":"<code>to_bytes(audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Convert video tensor to <code>VideoBytes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p>a VideoBytes object</p> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def to_bytes(\nself: 'T',\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; 'VideoBytes':\n\"\"\"\n    Convert video tensor to [`VideoBytes`][docarray.typing.VideoBytes].\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    :return: a VideoBytes object\n    \"\"\"\nfrom docarray.typing.bytes.video_bytes import VideoBytes\nbytes = BytesIO()\nself.save(\nfile_path=bytes,\naudio_tensor=audio_tensor,\nvideo_frame_rate=video_frame_rate,\nvideo_codec=video_codec,\naudio_frame_rate=audio_frame_rate,\naudio_codec=audio_codec,\naudio_format=audio_format,\n)\nreturn VideoBytes(bytes.getvalue())\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into a NdArrayProto protobuf message</p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into a NdArrayProto protobuf message\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nnd_proto.dense.buffer = self.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(self.shape))\nnd_proto.dense.dtype = self.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_ndarray.VideoNdArray.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original ndarray without any memory copy.</p> <p>The original view rest intact and is still a Document <code>NdArray</code> but the return object is a pure <code>np.ndarray</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import NdArray\nimport numpy as np\nfrom pydantic import parse_obj_as\nt1 = parse_obj_as(NdArray, np.zeros((3, 224, 224)))\nt2 = t1.unwrap()\n# here t2 is a pure np.ndarray but t1 is still a Docarray NdArray\n# But both share the same underlying memory\n</code></pre> <p>Returns:</p> Type Description <code>ndarray</code> <p>a <code>numpy.ndarray</code></p> Source code in <code>docarray/typing/tensor/ndarray.py</code> <pre><code>def unwrap(self) -&gt; np.ndarray:\n\"\"\"\n    Return the original ndarray without any memory copy.\n    The original view rest intact and is still a Document `NdArray`\n    but the return object is a pure `np.ndarray` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import NdArray\n    import numpy as np\n    from pydantic import parse_obj_as\n    t1 = parse_obj_as(NdArray, np.zeros((3, 224, 224)))\n    t2 = t1.unwrap()\n    # here t2 is a pure np.ndarray but t1 is still a Docarray NdArray\n    # But both share the same underlying memory\n    ```\n    ---\n    :return: a `numpy.ndarray`\n    \"\"\"\nreturn self.view(np.ndarray)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin","title":"<code>docarray.typing.tensor.video.video_tensor_mixin</code>","text":""},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin","title":"<code>VideoTensorMixin</code>","text":"<p>             Bases: <code>AbstractTensor</code>, <code>ABC</code></p> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>class VideoTensorMixin(AbstractTensor, abc.ABC):\n@classmethod\ndef validate_shape(cls: Type['T'], value: 'T') -&gt; 'T':\ncomp_be = cls.get_comp_backend()\nshape = comp_be.shape(value)  # type: ignore\nif comp_be.n_dim(value) not in [3, 4] or shape[-1] != 3:  # type: ignore\nraise ValueError(\nf'Expects tensor with 3 or 4 dimensions and the last dimension equal '\nf'to 3, but received {shape}.'\n)\nelse:\nreturn value\ndef save(\nself: 'T',\nfile_path: Union[str, BytesIO],\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; None:\n\"\"\"\n        Save video tensor to a .mp4 file.\n        ---\n        ```python\n        import numpy as np\n        from docarray import BaseDoc\n        from docarray.typing.tensor.audio.audio_tensor import AudioTensor\n        from docarray.typing.tensor.video.video_tensor import VideoTensor\n        class MyDoc(BaseDoc):\n            video_tensor: VideoTensor\n            audio_tensor: AudioTensor\n        doc = MyDoc(\n            video_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\n            audio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n        )\n        doc.video_tensor.save(\n            file_path=\"/tmp/mp_.mp4\",\n            audio_tensor=doc.audio_tensor,\n            audio_format=\"flt\",\n        )\n        ```\n        ---\n        :param file_path: path to a .mp4 file. If file is a string, open the file by\n            that name, otherwise treat it as a file-like object.\n        :param audio_tensor: AudioTensor containing the video's soundtrack.\n        :param video_frame_rate: video frames per second.\n        :param video_codec: the name of a video decoder/encoder.\n        :param audio_frame_rate: audio frames per second.\n        :param audio_codec: the name of an audio decoder/encoder.\n        :param audio_format: the name of one of the audio formats supported by PyAV,\n            such as 'flt', 'fltp', 's16' or 's16p'.\n        \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av', raise_error=True)\nnp_tensor = self.get_comp_backend().to_numpy(array=self)\nvideo_tensor = np_tensor.astype('uint8')\nif isinstance(file_path, str):\nformat = file_path.split('.')[-1]\nelse:\nformat = 'mp4'\nwith av.open(file_path, mode='w', format=format) as container:\nif video_tensor.ndim == 3:\nvideo_tensor = np.expand_dims(video_tensor, axis=0)\nstream_video = container.add_stream(video_codec, rate=video_frame_rate)\nstream_video.height = video_tensor.shape[-3]\nstream_video.width = video_tensor.shape[-2]\nif audio_tensor is not None:\nstream_audio = container.add_stream(audio_codec)\naudio_np = audio_tensor.get_comp_backend().to_numpy(array=audio_tensor)\naudio_layout = 'stereo' if audio_np.shape[-2] == 2 else 'mono'\nfor i, audio in enumerate(audio_np):\nframe = av.AudioFrame.from_ndarray(\narray=audio, format=audio_format, layout=audio_layout\n)\nframe.rate = audio_frame_rate\nframe.pts = audio.shape[-1] * i\nfor packet in stream_audio.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_audio.encode(None):\ncontainer.mux(packet)\nfor vid in video_tensor:\nframe = av.VideoFrame.from_ndarray(vid, format='rgb24')\nfor packet in stream_video.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_video.encode(None):\ncontainer.mux(packet)\ndef to_bytes(\nself: 'T',\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; 'VideoBytes':\n\"\"\"\n        Convert video tensor to [`VideoBytes`][docarray.typing.VideoBytes].\n        :param audio_tensor: AudioTensor containing the video's soundtrack.\n        :param video_frame_rate: video frames per second.\n        :param video_codec: the name of a video decoder/encoder.\n        :param audio_frame_rate: audio frames per second.\n        :param audio_codec: the name of an audio decoder/encoder.\n        :param audio_format: the name of one of the audio formats supported by PyAV,\n            such as 'flt', 'fltp', 's16' or 's16p'.\n        :return: a VideoBytes object\n        \"\"\"\nfrom docarray.typing.bytes.video_bytes import VideoBytes\nbytes = BytesIO()\nself.save(\nfile_path=bytes,\naudio_tensor=audio_tensor,\nvideo_frame_rate=video_frame_rate,\nvideo_codec=video_codec,\naudio_frame_rate=audio_frame_rate,\naudio_codec=audio_codec,\naudio_format=audio_format,\n)\nreturn VideoBytes(bytes.getvalue())\ndef display(self, audio: Optional[AudioTensor] = None) -&gt; None:\n\"\"\"\n        Display video data from tensor in notebook.\n        :param audio: sound to play with video tensor\n        \"\"\"\nif is_notebook():\nfrom IPython.display import Video, display\nb = self.to_bytes(audio_tensor=audio)\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.display","title":"<code>display(audio=None)</code>","text":"<p>Display video data from tensor in notebook.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Optional[AudioTensor]</code> <p>sound to play with video tensor</p> <code>None</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def display(self, audio: Optional[AudioTensor] = None) -&gt; None:\n\"\"\"\n    Display video data from tensor in notebook.\n    :param audio: sound to play with video tensor\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Video, display\nb = self.to_bytes(audio_tensor=audio)\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>The computational backend compatible with this tensor type.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef get_comp_backend() -&gt; AbstractComputationalBackend:\n\"\"\"The computational backend compatible with this tensor type.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.save","title":"<code>save(file_path, audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Save video tensor to a .mp4 file.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing.tensor.audio.audio_tensor import AudioTensor\nfrom docarray.typing.tensor.video.video_tensor import VideoTensor\nclass MyDoc(BaseDoc):\nvideo_tensor: VideoTensor\naudio_tensor: AudioTensor\ndoc = MyDoc(\nvideo_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\naudio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n)\ndoc.video_tensor.save(\nfile_path=\"/tmp/mp_.mp4\",\naudio_tensor=doc.audio_tensor,\naudio_format=\"flt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, BytesIO]</code> <p>path to a .mp4 file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def save(\nself: 'T',\nfile_path: Union[str, BytesIO],\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; None:\n\"\"\"\n    Save video tensor to a .mp4 file.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing.tensor.audio.audio_tensor import AudioTensor\n    from docarray.typing.tensor.video.video_tensor import VideoTensor\n    class MyDoc(BaseDoc):\n        video_tensor: VideoTensor\n        audio_tensor: AudioTensor\n    doc = MyDoc(\n        video_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\n        audio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n    )\n    doc.video_tensor.save(\n        file_path=\"/tmp/mp_.mp4\",\n        audio_tensor=doc.audio_tensor,\n        audio_format=\"flt\",\n    )\n    ```\n    ---\n    :param file_path: path to a .mp4 file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av', raise_error=True)\nnp_tensor = self.get_comp_backend().to_numpy(array=self)\nvideo_tensor = np_tensor.astype('uint8')\nif isinstance(file_path, str):\nformat = file_path.split('.')[-1]\nelse:\nformat = 'mp4'\nwith av.open(file_path, mode='w', format=format) as container:\nif video_tensor.ndim == 3:\nvideo_tensor = np.expand_dims(video_tensor, axis=0)\nstream_video = container.add_stream(video_codec, rate=video_frame_rate)\nstream_video.height = video_tensor.shape[-3]\nstream_video.width = video_tensor.shape[-2]\nif audio_tensor is not None:\nstream_audio = container.add_stream(audio_codec)\naudio_np = audio_tensor.get_comp_backend().to_numpy(array=audio_tensor)\naudio_layout = 'stereo' if audio_np.shape[-2] == 2 else 'mono'\nfor i, audio in enumerate(audio_np):\nframe = av.AudioFrame.from_ndarray(\narray=audio, format=audio_format, layout=audio_layout\n)\nframe.rate = audio_frame_rate\nframe.pts = audio.shape[-1] * i\nfor packet in stream_audio.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_audio.encode(None):\ncontainer.mux(packet)\nfor vid in video_tensor:\nframe = av.VideoFrame.from_ndarray(vid, format='rgb24')\nfor packet in stream_video.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_video.encode(None):\ncontainer.mux(packet)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.to_bytes","title":"<code>to_bytes(audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Convert video tensor to <code>VideoBytes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p>a VideoBytes object</p> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def to_bytes(\nself: 'T',\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; 'VideoBytes':\n\"\"\"\n    Convert video tensor to [`VideoBytes`][docarray.typing.VideoBytes].\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    :return: a VideoBytes object\n    \"\"\"\nfrom docarray.typing.bytes.video_bytes import VideoBytes\nbytes = BytesIO()\nself.save(\nfile_path=bytes,\naudio_tensor=audio_tensor,\nvideo_frame_rate=video_frame_rate,\nvideo_codec=video_codec,\naudio_frame_rate=audio_frame_rate,\naudio_codec=audio_codec,\naudio_format=audio_format,\n)\nreturn VideoBytes(bytes.getvalue())\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.to_protobuf","title":"<code>to_protobuf()</code>  <code>abstractmethod</code>","text":"<p>Convert DocList into a Protobuf message</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"Convert DocList into a Protobuf message\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensor_mixin.VideoTensorMixin.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the native tensor object that this DocList tensor wraps.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>def unwrap(self):\n\"\"\"Return the native tensor object that this DocList tensor wraps.\"\"\"\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor","title":"<code>docarray.typing.tensor.video.video_tensorflow_tensor</code>","text":""},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor","title":"<code>VideoTensorFlowTensor</code>","text":"<p>             Bases: <code>TensorFlowTensor</code>, <code>VideoTensorMixin</code></p> <p>Subclass of <code>TensorFlowTensor</code>, to represent a video tensor. Adds video-specific features to the tensor.</p> <pre><code>from typing import Optional\nimport tensorflow as tf\nfrom docarray import BaseDoc\nfrom docarray.typing import VideoTensorFlowTensor, VideoUrl\nclass MyVideoDoc(BaseDoc):\ntitle: str\nurl: Optional[VideoUrl]\nvideo_tensor: Optional[VideoTensorFlowTensor]\ndoc_1 = MyVideoDoc(\ntitle='my_first_video_doc',\nvideo_tensor=tf.random.normal((100, 224, 224, 3)),\n)\n# doc_1.video_tensor.save(file_path='file_1.mp4')\ndoc_2 = MyVideoDoc(\ntitle='my_second_video_doc',\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n)\ndoc_2.video_tensor = doc_2.url.load().video\n# doc_2.video_tensor.save(file_path='file_2.wav')\n</code></pre> Source code in <code>docarray/typing/tensor/video/video_tensorflow_tensor.py</code> <pre><code>@_register_proto(proto_type_name='video_tensorflow_tensor')\nclass VideoTensorFlowTensor(\nTensorFlowTensor, VideoTensorMixin, metaclass=metaTensorFlow\n):\n\"\"\"\n    Subclass of [`TensorFlowTensor`][docarray.typing.TensorFlowTensor],\n    to represent a video tensor. Adds video-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    import tensorflow as tf\n    from docarray import BaseDoc\n    from docarray.typing import VideoTensorFlowTensor, VideoUrl\n    class MyVideoDoc(BaseDoc):\n        title: str\n        url: Optional[VideoUrl]\n        video_tensor: Optional[VideoTensorFlowTensor]\n    doc_1 = MyVideoDoc(\n        title='my_first_video_doc',\n        video_tensor=tf.random.normal((100, 224, 224, 3)),\n    )\n    # doc_1.video_tensor.save(file_path='file_1.mp4')\n    doc_2 = MyVideoDoc(\n        title='my_second_video_doc',\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n    )\n    doc_2.video_tensor = doc_2.url.load().video\n    # doc_2.video_tensor.save(file_path='file_2.wav')\n    ```\n    ---\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, List[Any], Tuple[Any], Any],\n) -&gt; T:\ntensor = super()._docarray_validate(value=value)\nreturn cls.validate_shape(value=tensor)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the elements of this tensor's <code>tf.Tensor</code>.</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over the elements of this tensor's `tf.Tensor`.\"\"\"\nfor i in range(len(self)):\nyield self[i]\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.__setitem__","title":"<code>__setitem__(index, value)</code>","text":"<p>Set a slice of this tensor's <code>tf.Tensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor's `tf.Tensor`\"\"\"\nt = self.unwrap()\nvalue = tf.cast(value, dtype=t.dtype)\nvar = tf.Variable(t)\nvar[index].assign(value)\nself.tensor = tf.constant(var)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.display","title":"<code>display(audio=None)</code>","text":"<p>Display video data from tensor in notebook.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Optional[AudioTensor]</code> <p>sound to play with video tensor</p> <code>None</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def display(self, audio: Optional[AudioTensor] = None) -&gt; None:\n\"\"\"\n    Display video data from tensor in notebook.\n    :param audio: sound to play with video tensor\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Video, display\nb = self.to_bytes(audio_tensor=audio)\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.from_ndarray","title":"<code>from_ndarray(value)</code>  <code>classmethod</code>","text":"<p>Create a <code>TensorFlowTensor</code> from a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>the numpy array</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TensorFlowTensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TensorFlowTensor` from a numpy array.\n    :param value: the numpy array\n    :return: a `TensorFlowTensor`\n    \"\"\"\nreturn cls._docarray_from_native(tf.convert_to_tensor(value))\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg.</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TensorFlowTensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg.\n    :param pb_msg:\n    :return: a `TensorFlowTensor`\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(\nf'Proto message {pb_msg} cannot be cast to a TensorFlowTensor.'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'TensorFlowCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.tensorflow_backend import TensorFlowCompBackend\nreturn TensorFlowCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.save","title":"<code>save(file_path, audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Save video tensor to a .mp4 file.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing.tensor.audio.audio_tensor import AudioTensor\nfrom docarray.typing.tensor.video.video_tensor import VideoTensor\nclass MyDoc(BaseDoc):\nvideo_tensor: VideoTensor\naudio_tensor: AudioTensor\ndoc = MyDoc(\nvideo_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\naudio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n)\ndoc.video_tensor.save(\nfile_path=\"/tmp/mp_.mp4\",\naudio_tensor=doc.audio_tensor,\naudio_format=\"flt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, BytesIO]</code> <p>path to a .mp4 file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def save(\nself: 'T',\nfile_path: Union[str, BytesIO],\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; None:\n\"\"\"\n    Save video tensor to a .mp4 file.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing.tensor.audio.audio_tensor import AudioTensor\n    from docarray.typing.tensor.video.video_tensor import VideoTensor\n    class MyDoc(BaseDoc):\n        video_tensor: VideoTensor\n        audio_tensor: AudioTensor\n    doc = MyDoc(\n        video_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\n        audio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n    )\n    doc.video_tensor.save(\n        file_path=\"/tmp/mp_.mp4\",\n        audio_tensor=doc.audio_tensor,\n        audio_format=\"flt\",\n    )\n    ```\n    ---\n    :param file_path: path to a .mp4 file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av', raise_error=True)\nnp_tensor = self.get_comp_backend().to_numpy(array=self)\nvideo_tensor = np_tensor.astype('uint8')\nif isinstance(file_path, str):\nformat = file_path.split('.')[-1]\nelse:\nformat = 'mp4'\nwith av.open(file_path, mode='w', format=format) as container:\nif video_tensor.ndim == 3:\nvideo_tensor = np.expand_dims(video_tensor, axis=0)\nstream_video = container.add_stream(video_codec, rate=video_frame_rate)\nstream_video.height = video_tensor.shape[-3]\nstream_video.width = video_tensor.shape[-2]\nif audio_tensor is not None:\nstream_audio = container.add_stream(audio_codec)\naudio_np = audio_tensor.get_comp_backend().to_numpy(array=audio_tensor)\naudio_layout = 'stereo' if audio_np.shape[-2] == 2 else 'mono'\nfor i, audio in enumerate(audio_np):\nframe = av.AudioFrame.from_ndarray(\narray=audio, format=audio_format, layout=audio_layout\n)\nframe.rate = audio_frame_rate\nframe.pts = audio.shape[-1] * i\nfor packet in stream_audio.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_audio.encode(None):\ncontainer.mux(packet)\nfor vid in video_tensor:\nframe = av.VideoFrame.from_ndarray(vid, format='rgb24')\nfor packet in stream_video.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_video.encode(None):\ncontainer.mux(packet)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.to_bytes","title":"<code>to_bytes(audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Convert video tensor to <code>VideoBytes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p>a VideoBytes object</p> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def to_bytes(\nself: 'T',\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; 'VideoBytes':\n\"\"\"\n    Convert video tensor to [`VideoBytes`][docarray.typing.VideoBytes].\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    :return: a VideoBytes object\n    \"\"\"\nfrom docarray.typing.bytes.video_bytes import VideoBytes\nbytes = BytesIO()\nself.save(\nfile_path=bytes,\naudio_tensor=audio_tensor,\nvideo_frame_rate=video_frame_rate,\nvideo_codec=video_codec,\naudio_frame_rate=audio_frame_rate,\naudio_codec=audio_codec,\naudio_format=audio_format,\n)\nreturn VideoBytes(bytes.getvalue())\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into an NdArrayProto protobuf message.</p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into an NdArrayProto protobuf message.\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.tensor.numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_tensorflow_tensor.VideoTensorFlowTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original <code>tf.Tensor</code> without any memory copy.</p> <p>The original view rest intact and is still a Document <code>TensorFlowTensor</code> but the return object is a pure <code>tf.Tensor</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import TensorFlowTensor\nimport tensorflow as tf\nt1 = TensorFlowTensor.validate(tf.zeros((3, 224, 224)), None, None)\n# here t1 is a docarray TensorFlowTensor\nt2 = t1.unwrap()\n# here t2 is a pure tf.Tensor but t1 is still a Docarray TensorFlowTensor\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>a <code>tf.Tensor</code></p> Source code in <code>docarray/typing/tensor/tensorflow_tensor.py</code> <pre><code>def unwrap(self) -&gt; tf.Tensor:\n\"\"\"\n    Return the original `tf.Tensor` without any memory copy.\n    The original view rest intact and is still a Document `TensorFlowTensor`\n    but the return object is a pure `tf.Tensor` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import TensorFlowTensor\n    import tensorflow as tf\n    t1 = TensorFlowTensor.validate(tf.zeros((3, 224, 224)), None, None)\n    # here t1 is a docarray TensorFlowTensor\n    t2 = t1.unwrap()\n    # here t2 is a pure tf.Tensor but t1 is still a Docarray TensorFlowTensor\n    ```\n    ---\n    :return: a `tf.Tensor`\n    \"\"\"\nreturn self.tensor\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor","title":"<code>docarray.typing.tensor.video.video_torch_tensor</code>","text":""},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor","title":"<code>VideoTorchTensor</code>","text":"<p>             Bases: <code>TorchTensor</code>, <code>VideoTensorMixin</code></p> <p>Subclass of <code>TorchTensor</code>, to represent a video tensor. Adds video-specific features to the tensor.</p> <pre><code>from typing import Optional\nimport torch\nfrom docarray import BaseDoc\nfrom docarray.typing import VideoTorchTensor, VideoUrl\nclass MyVideoDoc(BaseDoc):\ntitle: str\nurl: Optional[VideoUrl] = None\nvideo_tensor: Optional[VideoTorchTensor] = None\ndoc_1 = MyVideoDoc(\ntitle='my_first_video_doc',\nvideo_tensor=torch.randn(size=(100, 224, 224, 3)),\n)\n# doc_1.video_tensor.save(file_path='file_1.mp4')\ndoc_2 = MyVideoDoc(\ntitle='my_second_video_doc',\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n)\ndoc_2.video_tensor = doc_2.url.load().video\n# doc_2.video_tensor.save(file_path='file_2.wav')\n</code></pre> Source code in <code>docarray/typing/tensor/video/video_torch_tensor.py</code> <pre><code>@_register_proto(proto_type_name='video_torch_tensor')\nclass VideoTorchTensor(TorchTensor, VideoTensorMixin, metaclass=metaTorchAndNode):\n\"\"\"\n    Subclass of [`TorchTensor`][docarray.typing.TorchTensor], to represent a video tensor.\n    Adds video-specific features to the tensor.\n    ---\n    ```python\n    from typing import Optional\n    import torch\n    from docarray import BaseDoc\n    from docarray.typing import VideoTorchTensor, VideoUrl\n    class MyVideoDoc(BaseDoc):\n        title: str\n        url: Optional[VideoUrl] = None\n        video_tensor: Optional[VideoTorchTensor] = None\n    doc_1 = MyVideoDoc(\n        title='my_first_video_doc',\n        video_tensor=torch.randn(size=(100, 224, 224, 3)),\n    )\n    # doc_1.video_tensor.save(file_path='file_1.mp4')\n    doc_2 = MyVideoDoc(\n        title='my_second_video_doc',\n        url='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true',\n    )\n    doc_2.video_tensor = doc_2.url.load().video\n    # doc_2.video_tensor.save(file_path='file_2.wav')\n    ```\n    ---\n    \"\"\"\n@classmethod\ndef _docarray_validate(\ncls: Type[T],\nvalue: Union[T, np.ndarray, List[Any], Tuple[Any], Any],\n) -&gt; T:\ntensor = super()._docarray_validate(value=value)\nreturn cls.validate_shape(value=tensor)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def __deepcopy__(self, memo):\n\"\"\"\n    Custom implementation of deepcopy for TorchTensor to avoid storage sharing issues.\n    \"\"\"\n# Create a new tensor with the same data and properties\nnew_tensor = self.clone()\n# Set the class to the custom TorchTensor class\nnew_tensor.__class__ = self.__class__\nreturn new_tensor\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__docarray_validate_getitem__","title":"<code>__docarray_validate_getitem__(item)</code>  <code>classmethod</code>","text":"<p>This method validates the input to <code>AbstractTensor.__class_getitem__</code>.</p> <p>It is called at \"class creation time\", i.e. when a class is created with syntax of the form AnyTensor[shape].</p> <p>The default implementation tries to cast any <code>item</code> to a tuple of ints. A subclass can override this method to implement custom validation logic.</p> <p>The output of this is eventually passed to <code>AbstractTensor.__docarray_validate_shape__</code> as its <code>shape</code> argument.</p> <p>Raises <code>ValueError</code> if the input <code>item</code> does not pass validation.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to validate, passed to <code>__class_getitem__</code> (<code>Tensor[item]</code>).</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The validated item == the target shape of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_getitem__(cls, item: Any) -&gt; Tuple[int]:\n\"\"\"This method validates the input to `AbstractTensor.__class_getitem__`.\n    It is called at \"class creation time\",\n    i.e. when a class is created with syntax of the form AnyTensor[shape].\n    The default implementation tries to cast any `item` to a tuple of ints.\n    A subclass can override this method to implement custom validation logic.\n    The output of this is eventually passed to\n    [`AbstractTensor.__docarray_validate_shape__`]\n    [docarray.typing.tensor.abstract_tensor.AbstractTensor.__docarray_validate_shape__]\n    as its `shape` argument.\n    Raises `ValueError` if the input `item` does not pass validation.\n    :param item: The item to validate, passed to `__class_getitem__` (`Tensor[item]`).\n    :return: The validated item == the target shape of this tensor.\n    \"\"\"\nif isinstance(item, int):\nitem = (item,)\ntry:\nitem = tuple(item)\nexcept TypeError:\nraise TypeError(f'{item} is not a valid tensor shape.')\nreturn item\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__docarray_validate_shape__","title":"<code>__docarray_validate_shape__(t, shape)</code>  <code>classmethod</code>","text":"<p>Every tensor has to implement this method in order to enable syntax of the form AnyTensor[shape]. It is called when a tensor is assigned to a field of this type. i.e. when a tensor is passed to a Document field of type AnyTensor[shape].</p> <p>The intended behaviour is as follows:</p> <ul> <li>If the shape of <code>t</code> is equal to <code>shape</code>, return <code>t</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>,     but can be reshaped to <code>shape</code>, return <code>t</code> reshaped to <code>shape</code>.</li> <li>If the shape of <code>t</code> is not equal to <code>shape</code>     and cannot be reshaped to <code>shape</code>, raise a ValueError.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>T</code> <p>The tensor to validate.</p> required <code>shape</code> <code>Tuple[Union[int, str], ...]</code> <p>The shape to validate against.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The validated tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@classmethod\ndef __docarray_validate_shape__(cls, t: T, shape: Tuple[Union[int, str], ...]) -&gt; T:\n\"\"\"Every tensor has to implement this method in order to\n    enable syntax of the form AnyTensor[shape].\n    It is called when a tensor is assigned to a field of this type.\n    i.e. when a tensor is passed to a Document field of type AnyTensor[shape].\n    The intended behaviour is as follows:\n    - If the shape of `t` is equal to `shape`, return `t`.\n    - If the shape of `t` is not equal to `shape`,\n        but can be reshaped to `shape`, return `t` reshaped to `shape`.\n    - If the shape of `t` is not equal to `shape`\n        and cannot be reshaped to `shape`, raise a ValueError.\n    :param t: The tensor to validate.\n    :param shape: The shape to validate against.\n    :return: The validated tensor.\n    \"\"\"\ncomp_be = t.get_comp_backend()\ntshape = comp_be.shape(t)\nif tshape == shape:\nreturn t\nelif any(isinstance(dim, str) or dim == Ellipsis for dim in shape):\nellipsis_occurrences = [\npos for pos, dim in enumerate(shape) if dim == Ellipsis\n]\nif ellipsis_occurrences:\nif len(ellipsis_occurrences) &gt; 1:\nraise ValueError(\nf'Cannot use Ellipsis (...) more than once for the shape {shape}'\n)\nellipsis_pos = ellipsis_occurrences[0]\n# Calculate how many dimensions to add. Should be at least 1.\ndimensions_needed = max(len(tshape) - len(shape) + 1, 1)\nshape = (\nshape[:ellipsis_pos]\n+ tuple(\nf'__dim_var_{index}__' for index in range(dimensions_needed)\n)\n+ shape[ellipsis_pos + 1 :]\n)\nif len(tshape) != len(shape):\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nknown_dims: Dict[str, int] = {}\nfor tdim, dim in zip(tshape, shape):\nif isinstance(dim, int) and tdim != dim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelif isinstance(dim, str):\nif dim in known_dims and known_dims[dim] != tdim:\nraise ValueError(\nf'Tensor shape mismatch. Expected {shape}, got {tshape}'\n)\nelse:\nknown_dims[dim] = tdim\nelse:\nreturn t\nelse:\nshape = cast(Tuple[int], shape)\nwarnings.warn(\nf'Tensor shape mismatch. Reshaping tensor '\nf'of shape {tshape} to shape {shape}'\n)\ntry:\nvalue = cls._docarray_from_native(comp_be.reshape(t, shape))\nreturn cast(T, value)\nexcept RuntimeError:\nraise ValueError(\nf'Cannot reshape tensor of shape {tshape} to shape {shape}'\n)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Get a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self: T, item) -&gt; T:\n\"\"\"Get a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over the elements of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n\"\"\"Iterate over the elements of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.__setitem__","title":"<code>__setitem__(index, value)</code>  <code>abstractmethod</code>","text":"<p>Set a slice of this tensor.</p> Source code in <code>docarray/typing/tensor/abstract_tensor.py</code> <pre><code>@abc.abstractmethod\ndef __setitem__(self, index, value):\n\"\"\"Set a slice of this tensor.\"\"\"\n...\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.display","title":"<code>display(audio=None)</code>","text":"<p>Display video data from tensor in notebook.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Optional[AudioTensor]</code> <p>sound to play with video tensor</p> <code>None</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def display(self, audio: Optional[AudioTensor] = None) -&gt; None:\n\"\"\"\n    Display video data from tensor in notebook.\n    :param audio: sound to play with video tensor\n    \"\"\"\nif is_notebook():\nfrom IPython.display import Video, display\nb = self.to_bytes(audio_tensor=audio)\ndisplay(Video(data=b, embed=True, mimetype='video/mp4'))\nelse:\nwarnings.warn('Display of video is only possible in a notebook.')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.from_ndarray","title":"<code>from_ndarray(value)</code>  <code>classmethod</code>","text":"<p>Create a <code>TorchTensor</code> from a numpy array</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>the numpy array</p> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_ndarray(cls: Type[T], value: np.ndarray) -&gt; T:\n\"\"\"Create a `TorchTensor` from a numpy array\n    :param value: the numpy array\n    :return: a `TorchTensor`\n    \"\"\"\nreturn cls._docarray_from_native(torch.from_numpy(value))\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.from_protobuf","title":"<code>from_protobuf(pb_msg)</code>  <code>classmethod</code>","text":"<p>Read ndarray from a proto msg</p> <p>Parameters:</p> Name Type Description Default <code>pb_msg</code> <code>NdArrayProto</code> required <p>Returns:</p> Type Description <code>T</code> <p>a <code>TorchTensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@classmethod\ndef from_protobuf(cls: Type[T], pb_msg: 'NdArrayProto') -&gt; 'T':\n\"\"\"\n    Read ndarray from a proto msg\n    :param pb_msg:\n    :return: a `TorchTensor`\n    \"\"\"\nsource = pb_msg.dense\nif source.buffer:\nx = np.frombuffer(bytearray(source.buffer), dtype=source.dtype)\nreturn cls.from_ndarray(x.reshape(source.shape))\nelif len(source.shape) &gt; 0:\nreturn cls.from_ndarray(np.zeros(source.shape))\nelse:\nraise ValueError(f'proto message {pb_msg} cannot be cast to a TorchTensor')\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.get_comp_backend","title":"<code>get_comp_backend()</code>  <code>staticmethod</code>","text":"<p>Return the computational backend of the tensor</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>@staticmethod\ndef get_comp_backend() -&gt; 'TorchCompBackend':\n\"\"\"Return the computational backend of the tensor\"\"\"\nfrom docarray.computation.torch_backend import TorchCompBackend\nreturn TorchCompBackend()\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.new_empty","title":"<code>new_empty(*args, **kwargs)</code>","text":"<p>This method enables the deepcopy of <code>TorchTensor</code> by returning another instance of this subclass. If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def new_empty(self, *args, **kwargs):\n\"\"\"\n    This method enables the deepcopy of `TorchTensor` by returning another instance of this subclass.\n    If this function is not implemented, the deepcopy will throw an RuntimeError from Torch.\n    \"\"\"\nreturn self.__class__(*args, **kwargs)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.save","title":"<code>save(file_path, audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Save video tensor to a .mp4 file.</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing.tensor.audio.audio_tensor import AudioTensor\nfrom docarray.typing.tensor.video.video_tensor import VideoTensor\nclass MyDoc(BaseDoc):\nvideo_tensor: VideoTensor\naudio_tensor: AudioTensor\ndoc = MyDoc(\nvideo_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\naudio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n)\ndoc.video_tensor.save(\nfile_path=\"/tmp/mp_.mp4\",\naudio_tensor=doc.audio_tensor,\naudio_format=\"flt\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, BytesIO]</code> <p>path to a .mp4 file. If file is a string, open the file by that name, otherwise treat it as a file-like object.</p> required <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def save(\nself: 'T',\nfile_path: Union[str, BytesIO],\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; None:\n\"\"\"\n    Save video tensor to a .mp4 file.\n    ---\n    ```python\n    import numpy as np\n    from docarray import BaseDoc\n    from docarray.typing.tensor.audio.audio_tensor import AudioTensor\n    from docarray.typing.tensor.video.video_tensor import VideoTensor\n    class MyDoc(BaseDoc):\n        video_tensor: VideoTensor\n        audio_tensor: AudioTensor\n    doc = MyDoc(\n        video_tensor=np.random.randint(low=0, high=256, size=(10, 200, 300, 3)),\n        audio_tensor=np.random.randn(100, 1, 1024).astype(\"float32\"),\n    )\n    doc.video_tensor.save(\n        file_path=\"/tmp/mp_.mp4\",\n        audio_tensor=doc.audio_tensor,\n        audio_format=\"flt\",\n    )\n    ```\n    ---\n    :param file_path: path to a .mp4 file. If file is a string, open the file by\n        that name, otherwise treat it as a file-like object.\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    \"\"\"\nif TYPE_CHECKING:\nimport av\nelse:\nav = import_library('av', raise_error=True)\nnp_tensor = self.get_comp_backend().to_numpy(array=self)\nvideo_tensor = np_tensor.astype('uint8')\nif isinstance(file_path, str):\nformat = file_path.split('.')[-1]\nelse:\nformat = 'mp4'\nwith av.open(file_path, mode='w', format=format) as container:\nif video_tensor.ndim == 3:\nvideo_tensor = np.expand_dims(video_tensor, axis=0)\nstream_video = container.add_stream(video_codec, rate=video_frame_rate)\nstream_video.height = video_tensor.shape[-3]\nstream_video.width = video_tensor.shape[-2]\nif audio_tensor is not None:\nstream_audio = container.add_stream(audio_codec)\naudio_np = audio_tensor.get_comp_backend().to_numpy(array=audio_tensor)\naudio_layout = 'stereo' if audio_np.shape[-2] == 2 else 'mono'\nfor i, audio in enumerate(audio_np):\nframe = av.AudioFrame.from_ndarray(\narray=audio, format=audio_format, layout=audio_layout\n)\nframe.rate = audio_frame_rate\nframe.pts = audio.shape[-1] * i\nfor packet in stream_audio.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_audio.encode(None):\ncontainer.mux(packet)\nfor vid in video_tensor:\nframe = av.VideoFrame.from_ndarray(vid, format='rgb24')\nfor packet in stream_video.encode(frame):\ncontainer.mux(packet)\nfor packet in stream_video.encode(None):\ncontainer.mux(packet)\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.to_bytes","title":"<code>to_bytes(audio_tensor=None, video_frame_rate=24, video_codec='h264', audio_frame_rate=48000, audio_codec='aac', audio_format='fltp')</code>","text":"<p>Convert video tensor to <code>VideoBytes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>audio_tensor</code> <code>Optional[AudioTensor]</code> <p>AudioTensor containing the video's soundtrack.</p> <code>None</code> <code>video_frame_rate</code> <code>int</code> <p>video frames per second.</p> <code>24</code> <code>video_codec</code> <code>str</code> <p>the name of a video decoder/encoder.</p> <code>'h264'</code> <code>audio_frame_rate</code> <code>int</code> <p>audio frames per second.</p> <code>48000</code> <code>audio_codec</code> <code>str</code> <p>the name of an audio decoder/encoder.</p> <code>'aac'</code> <code>audio_format</code> <code>str</code> <p>the name of one of the audio formats supported by PyAV, such as 'flt', 'fltp', 's16' or 's16p'.</p> <code>'fltp'</code> <p>Returns:</p> Type Description <code>VideoBytes</code> <p>a VideoBytes object</p> Source code in <code>docarray/typing/tensor/video/video_tensor_mixin.py</code> <pre><code>def to_bytes(\nself: 'T',\naudio_tensor: Optional[AudioTensor] = None,\nvideo_frame_rate: int = 24,\nvideo_codec: str = 'h264',\naudio_frame_rate: int = 48000,\naudio_codec: str = 'aac',\naudio_format: str = 'fltp',\n) -&gt; 'VideoBytes':\n\"\"\"\n    Convert video tensor to [`VideoBytes`][docarray.typing.VideoBytes].\n    :param audio_tensor: AudioTensor containing the video's soundtrack.\n    :param video_frame_rate: video frames per second.\n    :param video_codec: the name of a video decoder/encoder.\n    :param audio_frame_rate: audio frames per second.\n    :param audio_codec: the name of an audio decoder/encoder.\n    :param audio_format: the name of one of the audio formats supported by PyAV,\n        such as 'flt', 'fltp', 's16' or 's16p'.\n    :return: a VideoBytes object\n    \"\"\"\nfrom docarray.typing.bytes.video_bytes import VideoBytes\nbytes = BytesIO()\nself.save(\nfile_path=bytes,\naudio_tensor=audio_tensor,\nvideo_frame_rate=video_frame_rate,\nvideo_codec=video_codec,\naudio_frame_rate=audio_frame_rate,\naudio_codec=audio_codec,\naudio_format=audio_format,\n)\nreturn VideoBytes(bytes.getvalue())\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.to_protobuf","title":"<code>to_protobuf()</code>","text":"<p>Transform self into a <code>NdArrayProto</code> protobuf message</p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def to_protobuf(self) -&gt; 'NdArrayProto':\n\"\"\"\n    Transform self into a `NdArrayProto` protobuf message\n    \"\"\"\nfrom docarray.proto import NdArrayProto\nnd_proto = NdArrayProto()\nvalue_np = self.detach().cpu().numpy()\nnd_proto.dense.buffer = value_np.tobytes()\nnd_proto.dense.ClearField('shape')\nnd_proto.dense.shape.extend(list(value_np.shape))\nnd_proto.dense.dtype = value_np.dtype.str\nreturn nd_proto\n</code></pre>"},{"location":"API_reference/typing/tensor/video/#docarray.typing.tensor.video.video_torch_tensor.VideoTorchTensor.unwrap","title":"<code>unwrap()</code>","text":"<p>Return the original <code>torch.Tensor</code> without any memory copy.</p> <p>The original view rest intact and is still a Document <code>TorchTensor</code> but the return object is a pure <code>torch.Tensor</code> but both object share the same memory layout.</p> <pre><code>from docarray.typing import TorchTensor\nimport torch\nfrom pydantic import parse_obj_as\nt = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n# here t is a docarray TorchTensor\nt2 = t.unwrap()\n# here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n# But both share the same underlying memory\n</code></pre> <p>Returns:</p> Type Description <code>Tensor</code> <p>a <code>torch.Tensor</code></p> Source code in <code>docarray/typing/tensor/torch_tensor.py</code> <pre><code>def unwrap(self) -&gt; torch.Tensor:\n\"\"\"\n    Return the original `torch.Tensor` without any memory copy.\n    The original view rest intact and is still a Document `TorchTensor`\n    but the return object is a pure `torch.Tensor` but both object share\n    the same memory layout.\n    ---\n    ```python\n    from docarray.typing import TorchTensor\n    import torch\n    from pydantic import parse_obj_as\n    t = parse_obj_as(TorchTensor, torch.zeros(3, 224, 224))\n    # here t is a docarray TorchTensor\n    t2 = t.unwrap()\n    # here t2 is a pure torch.Tensor but t1 is still a Docarray TorchTensor\n    # But both share the same underlying memory\n    ```\n    ---\n    :return: a `torch.Tensor`\n    \"\"\"\nvalue = copy(self)  # as unintuitive as it sounds, this\n# does not do any relevant memory copying, just shallow\n# reference to the torch data\nvalue.__class__ = torch.Tensor  # type: ignore\nreturn value\n</code></pre>"},{"location":"API_reference/utils/filter/","title":"filter","text":""},{"location":"API_reference/utils/filter/#filter","title":"filter","text":""},{"location":"API_reference/utils/filter/#docarray.utils.filter","title":"<code>docarray.utils.filter</code>","text":""},{"location":"API_reference/utils/filter/#docarray.utils.filter.filter_docs","title":"<code>filter_docs(docs, query)</code>","text":"<p>Filter the Documents in the index according to the given filter query. Filter queries use the same syntax as the MongoDB query language (https://www.mongodb.com/docs/manual/tutorial/query-documents/#specify-conditions-using-query-operators).  You can see a list of the supported operators here (https://www.mongodb.com/docs/manual/reference/operator/query/#std-label-query-selectors)</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.documents import TextDoc, ImageDoc\nfrom docarray.utils.filter import filter_docs\nclass MyDocument(BaseDoc):\ncaption: TextDoc\nImageDoc: ImageDoc\nprice: int\ndocs = DocList[MyDocument](\n[\nMyDocument(\ncaption='A tiger in the jungle',\nImageDoc=ImageDoc(url='tigerphoto.png'),\nprice=100,\n),\nMyDocument(\ncaption='A swimming turtle',\nImageDoc=ImageDoc(url='turtlepic.png'),\nprice=50,\n),\nMyDocument(\ncaption='A couple birdwatching with binoculars',\nImageDoc=ImageDoc(url='binocularsphoto.png'),\nprice=30,\n),\n]\n)\nquery = {\n'$and': {\n'ImageDoc__url': {'$regex': 'photo'},\n'price': {'$lte': 50},\n}\n}\nresults = filter_docs(docs, query)\nassert len(results) == 1\nassert results[0].price == 30\nassert results[0].caption == 'A couple birdwatching with binoculars'\nassert results[0].ImageDoc.url == 'binocularsphoto.png'\n</code></pre> <p>:param docs: the DocList where to apply the filter  :param query: the query to filter by  :return: A DocList containing the Documents  in <code>docs</code> that fulfill the filter conditions in the <code>query</code></p> Source code in <code>docarray/utils/filter.py</code> <pre><code>def filter_docs(\ndocs: AnyDocArray,\nquery: Union[str, Dict, List[Dict]],\n) -&gt; AnyDocArray:\n\"\"\"\n     Filter the Documents in the index according to the given filter query.\n    Filter queries use the same syntax as the MongoDB query language (https://www.mongodb.com/docs/manual/tutorial/query-documents/#specify-conditions-using-query-operators).\n     You can see a list of the supported operators here (https://www.mongodb.com/docs/manual/reference/operator/query/#std-label-query-selectors)\n     ---\n     ```python\n     from docarray import DocList, BaseDoc\n     from docarray.documents import TextDoc, ImageDoc\n     from docarray.utils.filter import filter_docs\n     class MyDocument(BaseDoc):\n         caption: TextDoc\n         ImageDoc: ImageDoc\n         price: int\n     docs = DocList[MyDocument](\n         [\n             MyDocument(\n                 caption='A tiger in the jungle',\n                 ImageDoc=ImageDoc(url='tigerphoto.png'),\n                 price=100,\n             ),\n             MyDocument(\n                 caption='A swimming turtle',\n                 ImageDoc=ImageDoc(url='turtlepic.png'),\n                 price=50,\n             ),\n             MyDocument(\n                 caption='A couple birdwatching with binoculars',\n                 ImageDoc=ImageDoc(url='binocularsphoto.png'),\n                 price=30,\n             ),\n         ]\n     )\n     query = {\n         '$and': {\n             'ImageDoc__url': {'$regex': 'photo'},\n             'price': {'$lte': 50},\n         }\n     }\n     results = filter_docs(docs, query)\n     assert len(results) == 1\n     assert results[0].price == 30\n     assert results[0].caption == 'A couple birdwatching with binoculars'\n     assert results[0].ImageDoc.url == 'binocularsphoto.png'\n     ```\n     ---\n     :param docs: the DocList where to apply the filter\n     :param query: the query to filter by\n     :return: A DocList containing the Documents\n     in `docs` that fulfill the filter conditions in the `query`\n    \"\"\"\nfrom docarray.utils._internal.query_language.query_parser import QueryParser\nif query:\nquery = query if not isinstance(query, str) else json.loads(query)\nparser = QueryParser(query)\nreturn DocList.__class_getitem__(docs.doc_type)(\nd for d in docs if parser.evaluate(d)\n)\nelse:\nreturn docs\n</code></pre>"},{"location":"API_reference/utils/find/","title":"find","text":""},{"location":"API_reference/utils/find/#find","title":"find","text":""},{"location":"API_reference/utils/find/#docarray.utils.find","title":"<code>docarray.utils.find</code>","text":""},{"location":"API_reference/utils/find/#docarray.utils.find.find","title":"<code>find(index, query, search_field='', metric='cosine_sim', limit=10, device=None, descending=None, cache=None)</code>","text":"<p>Find the closest Documents in the index to the query. Supports PyTorch and NumPy embeddings.</p> <p>Note</p> <p>This is a simple implementation of exact search. If you need to do advance search using approximate nearest neighbours search or hybrid search or multi vector search please take a look at the <code>BaseDoc</code>.</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.typing import TorchTensor\nfrom docarray.utils.find import find\nimport torch\nclass MyDocument(BaseDoc):\nembedding: TorchTensor\nindex = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(100)])\n# use Document as query\nquery = MyDocument(embedding=torch.rand(128))\ntop_matches, scores = find(\nindex=index,\nquery=query,\nsearch_field='embedding',\nmetric='cosine_sim',\n)\n# use tensor as query\nquery = torch.rand(128)\ntop_matches, scores = find(\nindex=index,\nquery=query,\nsearch_field='embedding',\nmetric='cosine_sim',\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>AnyDocArray</code> <p>the index of Documents to search in</p> required <code>query</code> <code>Union[AnyTensor, BaseDoc]</code> <p>the query to search for</p> required <code>search_field</code> <code>str</code> <p>the tensor-like field in the index to use for the similarity computation</p> <code>''</code> <code>metric</code> <code>str</code> <p>the distance metric to use for the similarity computation. Can be one of the following strings: 'cosine_sim' for cosine similarity, 'euclidean_dist' for euclidean distance, 'sqeuclidean_dist' for squared euclidean distance</p> <code>'cosine_sim'</code> <code>limit</code> <code>int</code> <p>return the top <code>limit</code> results</p> <code>10</code> <code>device</code> <code>Optional[str]</code> <p>the computational device to use, can be either <code>cpu</code> or a <code>cuda</code> device.</p> <code>None</code> <code>descending</code> <code>Optional[bool]</code> <p>sort the results in descending order. Per default, this is chosen based on the <code>metric</code> argument.</p> <code>None</code> <code>cache</code> <code>Optional[Dict[str, Tuple[AnyTensor, Optional[List[int]]]]]</code> <p>Precomputed data storing the valid index data per search field together with the valid indexes to account for deleted entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>FindResult</code> <p>A named tuple of the form (DocList, AnyTensor), where the first element contains the closes matches for the query, and the second element contains the corresponding scores.</p> Source code in <code>docarray/utils/find.py</code> <pre><code>def find(\nindex: AnyDocArray,\nquery: Union[AnyTensor, BaseDoc],\nsearch_field: str = '',\nmetric: str = 'cosine_sim',\nlimit: int = 10,\ndevice: Optional[str] = None,\ndescending: Optional[bool] = None,\ncache: Optional[Dict[str, Tuple[AnyTensor, Optional[List[int]]]]] = None,\n) -&gt; FindResult:\n\"\"\"\n    Find the closest Documents in the index to the query.\n    Supports PyTorch and NumPy embeddings.\n    !!! note\n        This is a simple implementation of exact search. If you need to do advance\n        search using approximate nearest neighbours search or hybrid search or\n        multi vector search please take a look at the [`BaseDoc`][docarray.base_doc.doc.BaseDoc].\n    ---\n    ```python\n    from docarray import DocList, BaseDoc\n    from docarray.typing import TorchTensor\n    from docarray.utils.find import find\n    import torch\n    class MyDocument(BaseDoc):\n        embedding: TorchTensor\n    index = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(100)])\n    # use Document as query\n    query = MyDocument(embedding=torch.rand(128))\n    top_matches, scores = find(\n        index=index,\n        query=query,\n        search_field='embedding',\n        metric='cosine_sim',\n    )\n    # use tensor as query\n    query = torch.rand(128)\n    top_matches, scores = find(\n        index=index,\n        query=query,\n        search_field='embedding',\n        metric='cosine_sim',\n    )\n    ```\n    ---\n    :param index: the index of Documents to search in\n    :param query: the query to search for\n    :param search_field: the tensor-like field in the index to use\n        for the similarity computation\n    :param metric: the distance metric to use for the similarity computation.\n        Can be one of the following strings:\n        'cosine_sim' for cosine similarity, 'euclidean_dist' for euclidean distance,\n        'sqeuclidean_dist' for squared euclidean distance\n    :param limit: return the top `limit` results\n    :param device: the computational device to use,\n        can be either `cpu` or a `cuda` device.\n    :param descending: sort the results in descending order.\n        Per default, this is chosen based on the `metric` argument.\n    :param cache: Precomputed data storing the valid index data per search field together with the valid indexes to account for deleted entries.\n    :return: A named tuple of the form (DocList, AnyTensor),\n        where the first element contains the closes matches for the query,\n        and the second element contains the corresponding scores.\n    \"\"\"\nquery = _extract_embedding_single(query, search_field)\ndocs, scores = find_batched(\nindex=index,\nquery=query,\nsearch_field=search_field,\nmetric=metric,\nlimit=limit,\ndevice=device,\ndescending=descending,\ncache=cache,\n)\nreturn FindResult(documents=docs[0], scores=scores[0])\n</code></pre>"},{"location":"API_reference/utils/find/#docarray.utils.find.find_batched","title":"<code>find_batched(index, query, search_field='', metric='cosine_sim', limit=10, device=None, descending=None, cache=None)</code>","text":"<p>Find the closest Documents in the index to the queries. Supports PyTorch and NumPy embeddings.</p> <p>Note</p> <p>This is a simple implementation of exact search. If you need to do advance search using approximate nearest neighbours search or hybrid search or multi vector search please take a look at the <code>BaseDoc</code></p> <p>Note</p> <p>Only non-None embeddings will be considered from the <code>index</code> array</p> <pre><code>from docarray import DocList, BaseDoc\nfrom docarray.typing import TorchTensor\nfrom docarray.utils.find import find_batched\nimport torch\nclass MyDocument(BaseDoc):\nembedding: TorchTensor\nindex = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(100)])\n# use DocList as query\nquery = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(3)])\ndocs, scores = find_batched(\nindex=index,\nquery=query,\nsearch_field='embedding',\nmetric='cosine_sim',\n)\ntop_matches, scores = docs[0], scores[0]\n# use tensor as query\nquery = torch.rand(3, 128)\ndocs, scores = find_batched(\nindex=index,\nquery=query,\nsearch_field='embedding',\nmetric='cosine_sim',\n)\ntop_matches, scores = docs[0], scores[0]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>AnyDocArray</code> <p>the index of Documents to search in</p> required <code>query</code> <code>Union[AnyTensor, DocList]</code> <p>the query to search for</p> required <code>search_field</code> <code>str</code> <p>the tensor-like field in the index to use for the similarity computation</p> <code>''</code> <code>metric</code> <code>str</code> <p>the distance metric to use for the similarity computation. Can be one of the following strings: 'cosine_sim' for cosine similarity, 'euclidean_dist' for euclidean distance, 'sqeuclidean_dist' for squared euclidean distance</p> <code>'cosine_sim'</code> <code>limit</code> <code>int</code> <p>return the top <code>limit</code> results</p> <code>10</code> <code>device</code> <code>Optional[str]</code> <p>the computational device to use, can be either <code>cpu</code> or a <code>cuda</code> device.</p> <code>None</code> <code>descending</code> <code>Optional[bool]</code> <p>sort the results in descending order. Per default, this is chosen based on the <code>metric</code> argument.</p> <code>None</code> <code>cache</code> <code>Optional[Dict[str, Tuple[AnyTensor, Optional[List[int]]]]]</code> <p>Precomputed data storing the valid index data per search field together with the valid indexes to account for deleted entries.</p> <code>None</code> <p>Returns:</p> Type Description <code>FindResultBatched</code> <p>A named tuple of the form (DocList, AnyTensor), where the first element contains the closest matches for each query, and the second element contains the corresponding scores.</p> Source code in <code>docarray/utils/find.py</code> <pre><code>def find_batched(\nindex: AnyDocArray,\nquery: Union[AnyTensor, DocList],\nsearch_field: str = '',\nmetric: str = 'cosine_sim',\nlimit: int = 10,\ndevice: Optional[str] = None,\ndescending: Optional[bool] = None,\ncache: Optional[Dict[str, Tuple[AnyTensor, Optional[List[int]]]]] = None,\n) -&gt; FindResultBatched:\n\"\"\"\n    Find the closest Documents in the index to the queries.\n    Supports PyTorch and NumPy embeddings.\n    !!! note\n        This is a simple implementation of exact search. If you need to do advance\n        search using approximate nearest neighbours search or hybrid search or\n        multi vector search please take a look at the [`BaseDoc`][docarray.base_doc.doc.BaseDoc]\n    !!! note\n        Only non-None embeddings will be considered from the `index` array\n    ---\n    ```python\n    from docarray import DocList, BaseDoc\n    from docarray.typing import TorchTensor\n    from docarray.utils.find import find_batched\n    import torch\n    class MyDocument(BaseDoc):\n        embedding: TorchTensor\n    index = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(100)])\n    # use DocList as query\n    query = DocList[MyDocument]([MyDocument(embedding=torch.rand(128)) for _ in range(3)])\n    docs, scores = find_batched(\n        index=index,\n        query=query,\n        search_field='embedding',\n        metric='cosine_sim',\n    )\n    top_matches, scores = docs[0], scores[0]\n    # use tensor as query\n    query = torch.rand(3, 128)\n    docs, scores = find_batched(\n        index=index,\n        query=query,\n        search_field='embedding',\n        metric='cosine_sim',\n    )\n    top_matches, scores = docs[0], scores[0]\n    ```\n    ---\n    :param index: the index of Documents to search in\n    :param query: the query to search for\n    :param search_field: the tensor-like field in the index to use\n        for the similarity computation\n    :param metric: the distance metric to use for the similarity computation.\n        Can be one of the following strings:\n        'cosine_sim' for cosine similarity, 'euclidean_dist' for euclidean distance,\n        'sqeuclidean_dist' for squared euclidean distance\n    :param limit: return the top `limit` results\n    :param device: the computational device to use,\n        can be either `cpu` or a `cuda` device.\n    :param descending: sort the results in descending order.\n        Per default, this is chosen based on the `metric` argument.\n    :param cache: Precomputed data storing the valid index data per search field together with the valid indexes to account for deleted entries.\n    :return: A named tuple of the form (DocList, AnyTensor),\n        where the first element contains the closest matches for each query,\n        and the second element contains the corresponding scores.\n    \"\"\"\nif descending is None:\ndescending = metric.endswith('_sim')  # similarity metrics are descending\n# extract embeddings from query and index\nif cache is not None and search_field in cache:\nindex_embeddings, valid_idx = cache[search_field]\nelse:\nindex_embeddings, valid_idx = _extract_embeddings(index, search_field)\nif cache is not None:\ncache[search_field] = (\nindex_embeddings,\nvalid_idx,\n)  # cache embedding for next query\nquery_embeddings, _ = _extract_embeddings(query, search_field)\n_, comp_backend = _get_tensor_type_and_comp_backend_from_tensor(index_embeddings)\n# compute distances and return top results\nmetric_fn = getattr(comp_backend.Metrics, metric)\ndists = metric_fn(query_embeddings, index_embeddings, device=device)\ntop_scores, top_indices = comp_backend.Retrieval.top_k(\ndists, k=int(limit), device=device, descending=descending\n)\nbatched_docs: List[DocList] = []\ncandidate_index = index\nif valid_idx is not None and len(valid_idx) &lt; len(index):\ncandidate_index = index[valid_idx]\nscores = []\nfor _, (indices_per_query, scores_per_query) in enumerate(\nzip(top_indices, top_scores)\n):\ndocs_per_query: DocList = candidate_index[indices_per_query]\nbatched_docs.append(docs_per_query)\nscores.append(scores_per_query)\nreturn FindResultBatched(documents=batched_docs, scores=scores)\n</code></pre>"},{"location":"API_reference/utils/maps_docs/","title":"map","text":""},{"location":"API_reference/utils/maps_docs/#map","title":"map","text":""},{"location":"API_reference/utils/maps_docs/#docarray.utils.map","title":"<code>docarray.utils.map</code>","text":""},{"location":"API_reference/utils/maps_docs/#docarray.utils.map.map_docs","title":"<code>map_docs(docs, func, backend='thread', num_worker=None, pool=None, show_progress=False)</code>","text":"<p>Return an iterator that applies <code>func</code> to every Document in <code>docs</code> in parallel, yielding the results.</p> <pre><code>from docarray import DocList\nfrom docarray.documents import ImageDoc\nfrom docarray.utils.map import map_docs\ndef load_url_to_tensor(img: ImageDoc) -&gt; ImageDoc:\nimg.tensor = img.url.load()\nreturn img\nurl = (\n'https://upload.wikimedia.org/wikipedia/commons/8/80/'\n'Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg'\n)\ndocs = DocList[ImageDoc]([ImageDoc(url=url) for _ in range(100)])\ndocs = DocList[ImageDoc](\nlist(map_docs(docs, load_url_to_tensor, backend='thread'))\n)  # threading is usually a good option for IO-bound tasks such as loading an\n# ImageDoc from url\nfor doc in docs:\nassert doc.tensor is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>T</code> <p>DocList to apply function to</p> required <code>func</code> <code>Callable[[T_doc], T_doc]</code> <p>a function that takes a <code>BaseDoc</code> as input and outputs a <code>BaseDoc</code>.</p> required <code>backend</code> <code>str</code> <p><code>thread</code> for multithreading and <code>process</code> for multiprocessing. Defaults to <code>thread</code>. In general, if <code>func</code> is IO-bound then <code>thread</code> is a good choice. On the other hand, if <code>func</code> is CPU-bound, then you may use <code>process</code>. In practice, you should try yourselves to figure out the best value. However, if you wish to modify the elements in-place, regardless of IO/CPU-bound, you should always use <code>thread</code> backend. Note that computation that is offloaded to non-python code (e.g. through np/torch/tf) falls under the \"IO-bound\" category.  !!! warning When using <code>process</code> backend, your <code>func</code> should not modify elements in-place. This is because the multiprocessing backend passes the variable via pickle and works in another process. The passed object and the original object do not share the same memory.</p> <code>'thread'</code> <code>num_worker</code> <code>Optional[int]</code> <p>the number of parallel workers. If not given, the number of CPUs in the system will be used.</p> <code>None</code> <code>pool</code> <code>Optional[Union[Pool, ThreadPool]]</code> <p>use an existing/external process or thread pool. If given, you will be responsible for closing the pool.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show a progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[T_doc, None, None]</code> <p>yield Documents returned from <code>func</code></p> Source code in <code>docarray/utils/map.py</code> <pre><code>def map_docs(\ndocs: T,\nfunc: Callable[[T_doc], T_doc],\nbackend: str = 'thread',\nnum_worker: Optional[int] = None,\npool: Optional[Union[Pool, ThreadPool]] = None,\nshow_progress: bool = False,\n) -&gt; Generator[T_doc, None, None]:\n\"\"\"\n    Return an iterator that applies `func` to every Document in `docs` in parallel,\n    yielding the results.\n    ---\n    ```python\n    from docarray import DocList\n    from docarray.documents import ImageDoc\n    from docarray.utils.map import map_docs\n    def load_url_to_tensor(img: ImageDoc) -&gt; ImageDoc:\n        img.tensor = img.url.load()\n        return img\n    url = (\n        'https://upload.wikimedia.org/wikipedia/commons/8/80/'\n        'Dag_Sebastian_Ahlander_at_G%C3%B6teborg_Book_Fair_2012b.jpg'\n    )\n    docs = DocList[ImageDoc]([ImageDoc(url=url) for _ in range(100)])\n    docs = DocList[ImageDoc](\n        list(map_docs(docs, load_url_to_tensor, backend='thread'))\n    )  # threading is usually a good option for IO-bound tasks such as loading an\n    # ImageDoc from url\n    for doc in docs:\n        assert doc.tensor is not None\n    ```\n    ---\n    :param docs: DocList to apply function to\n    :param func: a function that takes a [`BaseDoc`][docarray.base_doc.doc.BaseDoc]\n        as input and outputs a [`BaseDoc`][docarray.base_doc.doc.BaseDoc].\n    :param backend: `thread` for multithreading and `process` for multiprocessing.\n        Defaults to `thread`.\n        In general, if `func` is IO-bound then `thread` is a good choice.\n        On the other hand, if `func` is CPU-bound, then you may use `process`.\n        In practice, you should try yourselves to figure out the best value.\n        However, if you wish to modify the elements in-place, regardless of IO/CPU-bound,\n        you should always use `thread` backend.\n        Note that computation that is offloaded to non-python code (e.g. through np/torch/tf)\n        falls under the \"IO-bound\" category.\n        !!! warning\n            When using `process` backend, your `func` should not modify elements in-place.\n            This is because the multiprocessing backend passes the variable via pickle\n            and works in another process.\n            The passed object and the original object do **not** share the same memory.\n    :param num_worker: the number of parallel workers. If not given, the number of CPUs\n        in the system will be used.\n    :param pool: use an existing/external process or thread pool. If given, you will\n        be responsible for closing the pool.\n    :param show_progress: show a progress bar. Defaults to False.\n    :return: yield Documents returned from `func`\n    \"\"\"\nif backend == 'process' and _is_lambda_or_partial_or_local_function(func):\nraise ValueError(\nf'Multiprocessing does not allow functions that are local, lambda or partial: {func}'\n)\ncontext_pool: Union[nullcontext, Union[Pool, ThreadPool]]\nif pool:\np = pool\ncontext_pool = nullcontext()\nelse:\np = _get_pool(backend, num_worker)\ncontext_pool = p\nwith context_pool:\nimap = p.imap(func, docs)\nfor x in track(imap, total=len(docs), disable=not show_progress):\nyield x\n</code></pre>"},{"location":"API_reference/utils/maps_docs/#docarray.utils.map.map_docs_batched","title":"<code>map_docs_batched(docs, func, batch_size, backend='thread', num_worker=None, shuffle=False, pool=None, show_progress=False)</code>","text":"<p>Return an iterator that applies <code>func</code> to every minibatch of iterable in parallel, yielding the results. Each element in the returned iterator is an <code>AnyDocArray</code>.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.utils.map import map_docs_batched\nclass MyDoc(BaseDoc):\nname: str\ndef upper_case_name(docs: DocList[MyDoc]) -&gt; DocList[MyDoc]:\ndocs.name = [n.upper() for n in docs.name]\nreturn docs\nbatch_size = 16\ndocs = DocList[MyDoc]([MyDoc(name='my orange cat') for _ in range(100)])\nit = map_docs_batched(docs, upper_case_name, batch_size=batch_size)\nfor i, d in enumerate(it):\ndocs[i * batch_size : (i + 1) * batch_size] = d\nassert len(docs) == 100\nprint(docs.name[:3])\n</code></pre> <pre><code>['MY ORANGE CAT', 'MY ORANGE CAT', 'MY ORANGE CAT']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>T</code> <p>DocList to apply function to</p> required <code>batch_size</code> <code>int</code> <p>Size of each generated batch (except the last one, which might be smaller).</p> required <code>shuffle</code> <code>bool</code> <p>If set, shuffle the Documents before dividing into minibatches.</p> <code>False</code> <code>func</code> <code>Callable[[T], Union[T, T_doc]]</code> <p>a function that takes an :class:<code>AnyDocArray</code> as input and outputs an :class:<code>AnyDocArray</code> or a :class:<code>BaseDoc</code>.</p> required <code>backend</code> <code>str</code> <p><code>thread</code> for multithreading and <code>process</code> for multiprocessing. Defaults to <code>thread</code>. In general, if <code>func</code> is IO-bound then <code>thread</code> is a good choice. On the other hand, if <code>func</code> is CPU-bound, then you may use <code>process</code>. In practice, you should try yourselves to figure out the best value. However, if you wish to modify the elements in-place, regardless of IO/CPU-bound, you should always use <code>thread</code> backend. Note that computation that is offloaded to non-python code (e.g. through np/torch/tf) falls under the \"IO-bound\" category.  !!! warning When using <code>process</code> backend, your <code>func</code> should not modify elements in-place. This is because the multiprocessing backend passes the variable via pickle and works in another process. The passed object and the original object do not share the same memory.</p> <code>'thread'</code> <code>num_worker</code> <code>Optional[int]</code> <p>the number of parallel workers. If not given, then the number of CPUs in the system will be used.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>show a progress bar</p> <code>False</code> <code>pool</code> <code>Optional[Union[Pool, ThreadPool]]</code> <p>use an existing/external pool. If given, <code>backend</code> is ignored and you will be responsible for closing the pool.</p> <code>None</code> <p>Returns:</p> Type Description <code>Generator[Union[T, T_doc], None, None]</code> <p>yield DocLists returned from <code>func</code></p> Source code in <code>docarray/utils/map.py</code> <pre><code>def map_docs_batched(\ndocs: T,\nfunc: Callable[[T], Union[T, T_doc]],\nbatch_size: int,\nbackend: str = 'thread',\nnum_worker: Optional[int] = None,\nshuffle: bool = False,\npool: Optional[Union[Pool, ThreadPool]] = None,\nshow_progress: bool = False,\n) -&gt; Generator[Union[T, T_doc], None, None]:\n\"\"\"\n    Return an iterator that applies `func` to every **minibatch** of iterable in parallel,\n    yielding the results.\n    Each element in the returned iterator is an `AnyDocArray`.\n    ---\n    ```python\n    from docarray import BaseDoc, DocList\n    from docarray.utils.map import map_docs_batched\n    class MyDoc(BaseDoc):\n        name: str\n    def upper_case_name(docs: DocList[MyDoc]) -&gt; DocList[MyDoc]:\n        docs.name = [n.upper() for n in docs.name]\n        return docs\n    batch_size = 16\n    docs = DocList[MyDoc]([MyDoc(name='my orange cat') for _ in range(100)])\n    it = map_docs_batched(docs, upper_case_name, batch_size=batch_size)\n    for i, d in enumerate(it):\n        docs[i * batch_size : (i + 1) * batch_size] = d\n    assert len(docs) == 100\n    print(docs.name[:3])\n    ```\n    ---\n    ```\n    ['MY ORANGE CAT', 'MY ORANGE CAT', 'MY ORANGE CAT']\n    ```\n    ---\n    :param docs: DocList to apply function to\n    :param batch_size: Size of each generated batch (except the last one, which might\n        be smaller).\n    :param shuffle: If set, shuffle the Documents before dividing into minibatches.\n    :param func: a function that takes an :class:`AnyDocArray` as input and outputs\n        an :class:`AnyDocArray` or a :class:`BaseDoc`.\n    :param backend: `thread` for multithreading and `process` for multiprocessing.\n        Defaults to `thread`.\n        In general, if `func` is IO-bound then `thread` is a good choice.\n        On the other hand, if `func` is CPU-bound, then you may use `process`.\n        In practice, you should try yourselves to figure out the best value.\n        However, if you wish to modify the elements in-place, regardless of IO/CPU-bound,\n        you should always use `thread` backend.\n        Note that computation that is offloaded to non-python code (e.g. through np/torch/tf)\n        falls under the \"IO-bound\" category.\n        !!! warning\n            When using `process` backend, your `func` should not modify elements in-place.\n            This is because the multiprocessing backend passes the variable via pickle\n            and works in another process.\n            The passed object and the original object do **not** share the same memory.\n    :param num_worker: the number of parallel workers. If not given, then the number of CPUs\n        in the system will be used.\n    :param show_progress: show a progress bar\n    :param pool: use an existing/external pool. If given, `backend` is ignored and you will\n        be responsible for closing the pool.\n    :return: yield DocLists returned from `func`\n    \"\"\"\nif backend == 'process' and _is_lambda_or_partial_or_local_function(func):\nraise ValueError(\nf'Multiprocessing does not allow functions that are local, lambda or partial: {func}'\n)\ncontext_pool: Union[nullcontext, Union[Pool, ThreadPool]]\nif pool:\np = pool\ncontext_pool = nullcontext()\nelse:\np = _get_pool(backend, num_worker)\ncontext_pool = p\nwith context_pool:\nimap = p.imap(func, docs._batch(batch_size=batch_size, shuffle=shuffle))\nfor x in track(\nimap, total=ceil(len(docs) / batch_size), disable=not show_progress\n):\nyield x\n</code></pre>"},{"location":"API_reference/utils/reduce/","title":"reduce","text":""},{"location":"API_reference/utils/reduce/#reduce","title":"reduce","text":""},{"location":"API_reference/utils/reduce/#docarray.utils.reduce","title":"<code>docarray.utils.reduce</code>","text":""},{"location":"API_reference/utils/reduce/#docarray.utils.reduce.reduce","title":"<code>reduce(left, right, left_id_map=None)</code>","text":"<p>Reduces left and right DocList into one DocList in-place. Changes are applied to the left DocList. Reducing 2 DocLists consists in adding Documents in the second DocList to the first DocList if they do not exist. If a Document exists in both DocLists (identified by ID), the data properties are merged with priority to the left Document.</p> <p>Nested DocLists are also reduced in the same way.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>DocList</code> <p>First DocList to be reduced. Changes will be applied to it in-place</p> required <code>right</code> <code>DocList</code> <p>Second DocList to be reduced</p> required <code>left_id_map</code> <code>Optional[Dict]</code> <p>Optional parameter to be passed in repeated calls for optimizations, keeping a map of the Document ID to its offset in the DocList</p> <code>None</code> <p>Returns:</p> Type Description <code>DocList</code> <p>Reduced DocList</p> Source code in <code>docarray/utils/reduce.py</code> <pre><code>def reduce(\nleft: DocList, right: DocList, left_id_map: Optional[Dict] = None\n) -&gt; 'DocList':\n\"\"\"\n    Reduces left and right DocList into one DocList in-place.\n    Changes are applied to the left DocList.\n    Reducing 2 DocLists consists in adding Documents in the second DocList\n    to the first DocList if they do not exist.\n    If a Document exists in both DocLists (identified by ID),\n    the data properties are merged with priority to the left Document.\n    Nested DocLists are also reduced in the same way.\n    :param left: First DocList to be reduced. Changes will be applied to it\n    in-place\n    :param right: Second DocList to be reduced\n    :param left_id_map: Optional parameter to be passed in repeated calls\n    for optimizations, keeping a map of the Document ID to its offset\n    in the DocList\n    :return: Reduced DocList\n    \"\"\"\nleft_id_map = left_id_map or {doc.id: i for i, doc in enumerate(left)}\nfor doc in right:\nif doc.id in left_id_map:\nleft[left_id_map[doc.id]].update(doc)\nelse:\ncasted = left.doc_type(**doc.__dict__)\nleft.append(casted)\nreturn left\n</code></pre>"},{"location":"API_reference/utils/reduce/#docarray.utils.reduce.reduce_all","title":"<code>reduce_all(docs)</code>","text":"<p>Reduces a list of DocLists into one DocList. Changes are applied to the first DocList in-place.</p> <p>The resulting DocList contains Documents of all DocLists. If a Document exists (identified by their ID) in many DocLists, data properties are merged with priority to the left-most DocLists (that is, if a data attribute is set in a Document belonging to many DocLists, the attribute value of the left-most  DocList is kept). Nested DocLists belonging to many DocLists  are also reduced in the same way.</p> <p>Note</p> <ul> <li>Nested DocLists order does not follow any specific rule. You might want to re-sort them in a later step.</li> <li>The final result depends on the order of DocLists when applying reduction.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[DocList]</code> <p>List of DocLists to be reduced</p> required <p>Returns:</p> Type Description <code>DocList</code> <p>the resulting DocList</p> Source code in <code>docarray/utils/reduce.py</code> <pre><code>def reduce_all(docs: List[DocList]) -&gt; DocList:\n\"\"\"\n    Reduces a list of DocLists into one DocList.\n    Changes are applied to the first DocList in-place.\n    The resulting DocList contains Documents of all DocLists.\n    If a Document exists (identified by their ID) in many DocLists,\n    data properties are merged with priority to the left-most\n    DocLists (that is, if a data attribute is set in a Document\n    belonging to many DocLists, the attribute value of the left-most\n     DocList is kept).\n    Nested DocLists belonging to many DocLists\n     are also reduced in the same way.\n    !!! note\n        - Nested DocLists order does not follow any specific rule.\n        You might want to re-sort them in a later step.\n        - The final result depends on the order of DocLists\n        when applying reduction.\n    :param docs: List of DocLists to be reduced\n    :return: the resulting DocList\n    \"\"\"\nif len(docs) &lt;= 1:\nraise Exception(\n'In order to reduce DocLists' ' we should have more than one DocList'\n)\nleft = docs[0]\nothers = docs[1:]\nleft_id_map = {doc.id: i for i, doc in enumerate(left)}\nfor other_docs in others:\nreduce(left, other_docs, left_id_map)\nreturn left\n</code></pre>"},{"location":"data_types/first_steps/","title":"Introduction","text":""},{"location":"data_types/first_steps/#introduction","title":"Introduction","text":"<p>With DocArray you can represent text, image, video, audio, and 3D meshes, whether separate, nested or combined,  and process them as a <code>DocList</code>. </p> <p>This section covers the following sections:</p> <ul> <li>Text</li> <li>Image</li> <li>Audio</li> <li>Video</li> <li>3D Mesh</li> <li>Table</li> <li>Multimodal data</li> <li>Tensor</li> </ul>"},{"location":"data_types/3d_mesh/3d_mesh/","title":"\ud83e\uddec 3D Mesh","text":""},{"location":"data_types/3d_mesh/3d_mesh/#3d-mesh","title":"\ud83e\uddec 3D Mesh","text":"<p>DocArray supports many different modalities including <code>3D Mesh</code>. This section will show you how to load and handle 3D data using DocArray.</p> <p>A 3D mesh is the structural build of a 3D model consisting of polygons. Most 3D meshes are created via professional software packages, such as commercial suites like Unity, or the open-source Blender.</p> <p>Note</p> <p>This feature requires <code>trimesh</code>. You can install all necessary dependencies via:</p> <pre><code>pip install \"docarray[mesh]\"\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#vertices-and-faces-representation","title":"Vertices and Faces representation","text":"<p>A 3D mesh can be represented by its vertices and faces:</p> <ul> <li>Vertices are points in a 3D space, represented as a tensor of shape <code>(n_points, 3)</code>. </li> <li>Faces are triangular surfaces that are defined by three points in 3D space, corresponding to the three vertices of a triangle. They can be represented as a tensor of shape <code>(n_faces, 3)</code>. Each number in that tensor refers to an index of a vertex in the tensor of vertices.</li> </ul>"},{"location":"data_types/3d_mesh/3d_mesh/#load-vertices-and-faces","title":"Load vertices and faces","text":"<p>First, let's define our class <code>MyMesh3D</code>, which extends <code>BaseDoc</code> and provides attributes to store our 3D data:</p> <ul> <li>The <code>mesh_url</code> attribute of type <code>Mesh3DUrl</code>. </li> <li>The optional <code>tensors</code> attribute, of type <code>VerticesAndFaces</code></li> <li>The <code>VerticesAndFaces</code> class has the attributes <code>vertices</code> and <code>faces</code>, both of type <code>AnyTensor</code>. This especially comes in handy later when we want to display our 3D mesh.</li> </ul> <p>Tip</p> <p>Check out our predefined <code>Mesh3D</code> to get started and play around with our 3D features.</p> <p>But for now, let's create a <code>MyMesh3D</code> instance with a URL to a remote <code>.obj</code> file:</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.documents.mesh.vertices_and_faces import VerticesAndFaces\nfrom docarray.typing import Mesh3DUrl\nclass MyMesh3D(BaseDoc):\nmesh_url: Mesh3DUrl\ntensors: Optional[VerticesAndFaces] = None\ndoc = MyMesh3D(mesh_url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n</code></pre> <p>To load the vertices and faces information, you can call <code>.load()</code> on the <code>Mesh3DUrl</code> instance. This will return a <code>VerticesAndFaces</code> object:</p> <pre><code>doc.tensors = doc.mesh_url.load()\ndoc.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 MyMesh3D : 9d8c26f ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute           \u2502 Value                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 mesh_url: Mesh3DUrl \u2502 https://people.sc.fsu.edu/~jburkardt/data/obj/al.o ... \u2502\n\u2502                     \u2502 (length: 52)                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2514\u2500\u2500 \ud83d\udd36 tensors: VerticesAndFaces\n    \u2514\u2500\u2500 \ud83d\udcc4 VerticesAndFaces : 8cae4c4 ...\n        \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n        \u2502 Attribute         \u2502 Value                                            \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 vertices: NdArray \u2502 NdArray of shape (3980, 3), dtype: float64       \u2502\n        \u2502 faces: NdArray    \u2502 NdArray of shape (7152, 3), dtype: int64         \u2502\n        \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#display-3d-mesh-in-notebook","title":"Display 3D mesh in notebook","text":"<p>You can display your 3D mesh interactively from its URL as well as a <code>VerticesAndFaces</code> instance, by calling <code>.display()</code> on either one. The latter will always display without color, whereas the display from the URL will show with color if this information is included in the file content.</p> <pre><code>doc.mesh_url.display()\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#point-cloud-representation","title":"Point cloud representation","text":"<p>A point cloud is a representation of a 3D mesh. It is made by repeatedly and uniformly sampling points within the surface of the 3D body. Compared to the mesh representation, the point cloud is a fixed size <code>ndarray</code> and hence easier for deep learning algorithms to handle. </p>"},{"location":"data_types/3d_mesh/3d_mesh/#load-point-cloud","title":"Load point cloud","text":"<p>Tip</p> <p>Check out our predefined <code>PointCloud3D</code> to get started and play around with our 3D features.</p> <p>In DocArray, loading a point cloud from a <code>PointCloud3DUrl</code> instance will return a <code>PointsAndColors</code> instance. Such an object has a <code>points</code> attribute containing the information about the points in 3D space as well as an optional <code>colors</code> attribute.</p> <p>First, let's define our class <code>MyPointCloud</code>, which extends <code>BaseDoc</code> and provides attributes to store the point cloud information:</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nfrom docarray.documents.point_cloud.points_and_colors import PointsAndColors\nfrom docarray.typing import PointCloud3DUrl\nclass MyPointCloud(BaseDoc):\nurl: PointCloud3DUrl\ntensors: Optional[PointsAndColors] = None\ndoc = MyPointCloud(url=\"https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj\")\n</code></pre> <p>Next, we can load a point cloud of size <code>samples</code> by simply calling <code>.load()</code> on the <code>PointCloud3DUrl</code> instance:</p> <pre><code>doc.tensors = doc.url.load(samples=1000)\ndoc.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 MyPointCloud : a63374d ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute                        \u2502 Value                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 url: PointCloud3DUrl             \u2502 https://people.sc.fsu.edu/~jburkardt/dat\u2026 \u2502\n\u2502                                  \u2502 ... (length: 52)                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2514\u2500\u2500 \ud83d\udd36 tensors: PointsAndColors\n    \u2514\u2500\u2500 \ud83d\udcc4 PointsAndColors : 70ae175 ...\n        \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n        \u2502 Attribute       \u2502 Value                                              \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 points: NdArray \u2502 NdArray of shape (1000, 3), dtype: float64         \u2502\n        \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#display-3d-point-cloud-in-notebook","title":"Display 3D point cloud in notebook","text":"<p>You can display your point cloud and interact with it from its URL as well as from a PointsAndColors instance. The first will always display without color, whereas the display from <code>PointsAndColors</code> will show with color if <code>.colors</code> is not None.</p> <pre><code>doc.url.display()\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#getting-started-predefined-documents","title":"Getting started - Predefined documents","text":"<p>To get started and play around with the 3D modalities, DocArray provides the predefined documents <code>Mesh3D</code> and <code>PointCloud3D</code>, which includes all of the previously mentioned functionalities.</p>"},{"location":"data_types/3d_mesh/3d_mesh/#mesh3d","title":"<code>Mesh3D</code>","text":"<p>The <code>Mesh3D</code> class provides a <code>Mesh3DUrl</code> field and <code>VerticesAndFaces</code> field.</p> <pre><code>class Mesh3D(BaseDoc):\nurl: Optional[Mesh3DUrl] = None\ntensors: Optional[VerticesAndFaces] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[bytes] = None\n</code></pre>"},{"location":"data_types/3d_mesh/3d_mesh/#pointcloud3d","title":"<code>PointCloud3D</code>","text":"<pre><code>class PointCloud3D(BaseDoc):\nurl: Optional[PointCloud3DUrl] = None\ntensors: Optional[PointsAndColors] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[bytes] = None\n</code></pre> <p>You can use them directly, extend or compose them:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.documents import Mesh3D, PointCloud3D\nclass My3DObject(BaseDoc):\ntitle: str\nmesh: Mesh3D\npc: PointCloud3D\nobj_file = 'https://people.sc.fsu.edu/~jburkardt/data/obj/al.obj'\ndoc = My3DObject(\ntitle='My first 3D object!',\nmesh=Mesh3D(url=obj_file),\npc=PointCloud3D(url=obj_file),\n)\ndoc.mesh.tensors = doc.mesh.url.load()\ndoc.pc.tensors = doc.pc.url.load(samples=100)\n</code></pre>"},{"location":"data_types/audio/audio/","title":"\ud83d\udd0a Audio","text":""},{"location":"data_types/audio/audio/#audio","title":"\ud83d\udd0a Audio","text":"<p>DocArray supports many different modalities including <code>Audio</code>. This section will show you how to load and handle audio data using DocArray.</p> <p>Moreover, you will learn about DocArray's audio-specific types, to represent your audio data ranging from <code>AudioUrl</code> to <code>AudioBytes</code> and <code>AudioNdArray</code>.</p> <p>Note</p> <p>This requires a <code>pydub</code> dependency. You can install all necessary dependencies via:</p> <pre><code>pip install \"docarray[audio]\"\n</code></pre> <p>Additionally, you have to install <code>ffmpeg</code> (see more info here):</p> <pre><code># on Mac with brew:\nbrew install ffmpeg\n</code></pre> <pre><code># on Linux with apt-get\napt-get install ffmpeg libavcodec-extra\n</code></pre>"},{"location":"data_types/audio/audio/#load-audio-file","title":"Load audio file","text":"<p>First, let's define a class which extends <code>BaseDoc</code> and has a <code>url</code> attribute of type <code>AudioUrl</code>, and an optional <code>tensor</code> attribute of type <code>AudioTensor</code>.</p> <p>Tip</p> <p>Check out our predefined <code>AudioDoc</code> to get started and play around with our audio features.</p> <p>Next, you can instantiate an object of that class with a local or remote URL:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioUrl, AudioNdArray\nclass MyAudio(BaseDoc):\nurl: AudioUrl\ntensor: AudioNdArray = None\nframe_rate: int = None\ndoc = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true'\n)\n</code></pre> <p>Loading the content of the audio file is as easy as calling <code>.load()</code> on the <code>AudioUrl</code> instance. </p> <p>This will return a tuple of:</p> <ul> <li>An <code>AudioNdArray</code> representing the audio file content </li> <li>An integer representing the frame rate (number of signals for a certain period of time)</li> </ul> <pre><code>doc.tensor, doc.frame_rate = doc.url.load()\ndoc.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 MyAudio : 2015696 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute            \u2502 Value                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 url: AudioUrl        \u2502 https://github.com/docarray/docarray/blob/main/tes    \u2502\n\u2502                      \u2502 ... (length: 90)                                      \u2502\n\u2502 tensor: AudioNdArray \u2502 AudioNdArray of shape (30833,), dtype: float64        \u2502\n\u2502 frame_rate: int      \u2502 44100                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/audio/audio/#audiotensor","title":"AudioTensor","text":"<p>DocArray offers several <code>AudioTensor</code>s to store your data to:</p> <ul> <li><code>AudioNdArray</code></li> <li><code>AudioTorchTensor</code></li> <li><code>AudioTensorFlowTensor</code></li> </ul> <p>If you specify the type of your tensor to one of the above, it will be cast to that automatically:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioTensorFlowTensor, AudioTorchTensor, AudioUrl\nclass MyAudio(BaseDoc):\nurl: AudioUrl\ntf_tensor: AudioTensorFlowTensor = None\ntorch_tensor: AudioTorchTensor = None\ndoc = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true'\n)\ndoc.tf_tensor, _ = doc.url.load()\ndoc.torch_tensor, _ = doc.url.load()\nassert isinstance(doc.tf_tensor, AudioTensorFlowTensor)\nassert isinstance(doc.torch_tensor, AudioTorchTensor)\n</code></pre>"},{"location":"data_types/audio/audio/#audiobytes","title":"AudioBytes","text":"<p>Alternatively, you can load your <code>AudioUrl</code> instance to <code>AudioBytes</code>, and your <code>AudioBytes</code> instance to an <code>AudioTensor</code> of your choice:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioBytes, AudioTensor, AudioUrl\nclass MyAudio(BaseDoc):\nurl: AudioUrl = None\nbytes_: AudioBytes = None\ntensor: AudioTensor = None\ndoc = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true'\n)\ndoc.bytes_ = doc.url.load_bytes()  # type(doc.bytes_) = AudioBytes\ndoc.tensor, _ = doc.bytes_.load()  # type(doc.tensor) = AudioNdarray\n</code></pre> <p>Vice versa, you can also transform an <code>AudioTensor</code> to <code>AudioBytes</code>:</p> <pre><code>from docarray.typing import AudioBytes\nbytes_from_tensor = doc.tensor.to_bytes()\nassert isinstance(bytes_from_tensor, AudioBytes)\n</code></pre>"},{"location":"data_types/audio/audio/#save-audio-to-file","title":"Save audio to file","text":"<p>You can save your <code>AudioTensor</code> to an audio file of any format as follows:</p> <pre><code>tensor_reversed = doc.tensor[::-1]\ntensor_reversed.save(\nfile_path='olleh.mp3',\nformat='mp3',\n)\n</code></pre>"},{"location":"data_types/audio/audio/#play-audio-in-a-notebook","title":"Play audio in a notebook","text":"<p>You can play your audio sound in a notebook from its URL or tensor, by calling <code>.display()</code> on either one.</p> <p>Play from <code>url</code>: </p><pre><code>doc.url.display()\n</code></pre> <p>Play from <code>tensor</code>:</p> <pre><code>tensor_reversed.display()\n</code></pre>"},{"location":"data_types/audio/audio/#getting-started-predefined-audiodoc","title":"Getting started - Predefined <code>AudioDoc</code>","text":"<p>To get started and play around with your audio data, DocArray provides a predefined <code>AudioDoc</code>, which includes all of the previously mentioned functionalities:</p> <pre><code>class AudioDoc(BaseDoc):\nurl: Optional[AudioUrl] = None\ntensor: Optional[AudioTensor] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[AudioBytes] = None\nframe_rate: Optional[int] = None\n</code></pre> <p>You can use this class directly or extend it to your preference:</p> <pre><code>from docarray.documents import AudioDoc\nfrom typing import Optional\n# extend AudioDoc\nclass MyAudio(AudioDoc):\nname: Optional[str] = None\naudio = MyAudio(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/hello.mp3?raw=true'\n)\naudio.name = 'My first audio doc!'\naudio.tensor, audio.frame_rate = audio.url.load()\n</code></pre>"},{"location":"data_types/image/image/","title":"\ud83d\uddbc\ufe0f Image","text":""},{"location":"data_types/image/image/#image","title":"\ud83d\uddbc\ufe0f Image","text":"<p>DocArray supports many different modalities including the widely used <code>Image</code> modality. This section will show you how to load and handle image data using DocArray.</p> <p>Moreover, we will introduce DocArray's image-specific types, to represent your image data ranging from <code>ImageUrl</code> to <code>ImageBytes</code> and <code>ImageNdArray</code>.</p> <p>Note</p> <p>This requires <code>Pillow</code> dependency. You can install all necessary dependencies via:</p> <pre><code>pip install \"docarray[image]\"\n</code></pre>"},{"location":"data_types/image/image/#load-image","title":"Load image","text":"<p>Tip</p> <p>Check out our predefined <code>ImageDoc</code> to get started and play around with our image features.</p> <p>First, let's define the class <code>MyImage</code>, which extends <code>BaseDoc</code> and has a <code>url</code> attribute of type <code>ImageUrl</code>, as well as an optional <code>tensor</code> attribute of type <code>ImageTensor</code>.</p> <p>Next, let's instantiate a <code>MyImage</code> object with a local or remote URL:</p> <pre><code>from docarray.typing import ImageTensor, ImageUrl\nfrom docarray import BaseDoc\nclass MyImage(BaseDoc):\nurl: ImageUrl\ntensor: ImageTensor = None\nimg = MyImage(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\n</code></pre> <p>To load the image data you can call <code>.load()</code> on the <code>url</code> attribute. By default, <code>ImageUrl.load()</code> returns an <code>ImageNdArray</code> object:</p> <pre><code>from docarray.typing import ImageNdArray\nimg.tensor = img.url.load()\nassert isinstance(img.tensor, ImageNdArray)\n</code></pre>"},{"location":"data_types/image/image/#imagetensor","title":"ImageTensor","text":"<p>DocArray offers several <code>ImageTensor</code>s to store your data to:</p> <ul> <li><code>ImageNdArray</code></li> <li><code>ImageTorchTensor</code></li> <li><code>ImageTensorFlowTensor</code></li> </ul> <p>If you specify the type of your tensor to one of the above, it will be cast to that automatically:</p> <pre><code>from docarray.typing import ImageTensorFlowTensor, ImageTorchTensor, ImageUrl\nfrom docarray import BaseDoc\nclass MyImage(BaseDoc):\nurl: ImageUrl = None\ntf_tensor: ImageTensorFlowTensor = None\ntorch_tensor: ImageTorchTensor = None\nimg = MyImage(url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true')\nimg.tf_tensor = img.url.load()\nimg.torch_tensor = img.url.load()\nassert isinstance(img.tf_tensor, ImageTensorFlowTensor)\nassert isinstance(img.torch_tensor, ImageTorchTensor)\n</code></pre> <p>You can also load the URL content as a <code>PIL.Image.Image</code> instance using <code>ImageUrl.load_pil()</code>:</p> <pre><code>from PIL.Image import Image as PILImage\nimg = MyImage(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true'\n)\npil_img = img.url.load_pil()\nassert isinstance(pil_img, PILImage)\n</code></pre>"},{"location":"data_types/image/image/#parameterized-imagetensor","title":"Parameterized ImageTensor","text":"<p>Like all of our tensors, the <code>ImageTensor</code>s can be used in a parametrized way, specifying the shape of the images. Let's say, for instance, all your images are of size <code>(200, 300, 3)</code>. </p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageNdArray\nclass MyImage(BaseDoc):\ntensor: ImageNdArray[200, 300, 3]\nimg = MyImage(tensor=np.ones(shape=(200, 300, 3)))\n# this would fail:\n# img = MyImage(tensor=np.ones(shape=(224, 224, 3)))\n</code></pre> <p>If you have RGB images of different shapes, you can specify only the dimensions and number of channels:</p> <pre><code>import numpy as np\nfrom docarray import BaseDoc\nfrom docarray.typing import ImageNdArray\nclass MyFlexibleImage(BaseDoc):\ntensor: ImageNdArray['h', 'w', 3]\nimg_1 = MyFlexibleImage(tensor=np.zeros(shape=(200, 300, 3)))\nimg_2 = MyFlexibleImage(tensor=np.ones(shape=(224, 224, 3)))\n</code></pre>"},{"location":"data_types/image/image/#imagebytes","title":"ImageBytes","text":"<p>Alternatively, you can load your <code>ImageUrl</code> instance to <code>ImageBytes</code>, and your <code>ImageBytes</code> instance to an <code>ImageTensor</code> of your choice.</p> <pre><code>from docarray.typing import ImageBytes, ImageTensor, ImageUrl\nfrom docarray import BaseDoc\nclass MyImage(BaseDoc):\nurl: ImageUrl = None\nbytes_: ImageBytes = None\ntensor: ImageTensor = None\nimg = MyImage(url='https://github.com/docarray/docarray/blob/main/tests/toydata/image-data/apple.png?raw=true')\nimg.bytes_ = img.url.load_bytes()  # type(img.bytes_) = ImageBytes\nimg.tensor = img.bytes_.load()  # type(img.tensor) = ImageNdarray\n</code></pre> <p>Vice versa, you can also transform an <code>ImageTensor</code> to <code>ImageBytes</code>:</p> <pre><code>from docarray.typing import ImageBytes\nbytes_from_tensor = img.tensor.to_bytes()\nassert isinstance(bytes_from_tensor, ImageBytes)\n</code></pre>"},{"location":"data_types/image/image/#display-image-in-a-notebook","title":"Display image in a notebook","text":"<p>You can display your image in a notebook from both an <code>ImageUrl</code> instance as well as an  <code>ImageTensor</code> instance.</p> <p></p>"},{"location":"data_types/image/image/#getting-started-predefined-imagedoc","title":"Getting started - Predefined <code>ImageDoc</code>","text":"<p>To get started and play around with the image modality, DocArray provides a predefined <code>ImageDoc</code>, which includes all of the previously mentioned functionalities:</p> <pre><code>class ImageDoc(BaseDoc):\nurl: Optional[ImageUrl] = None\ntensor: Optional[ImageTensor] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[ImageBytes] = None\n</code></pre> <p>You can use this class directly or extend it to your preference:</p> <pre><code>from docarray.documents import ImageDoc\nfrom docarray.typing import AnyEmbedding\nfrom typing import Optional\n# extending ImageDoc\nclass MyImage(ImageDoc):\nimage_title: str\nsecond_embedding: Optional[AnyEmbedding] = None\nimage = MyImage(\nimage_title='My first image',\nurl='http://www.jina.ai/image.jpg',\n)\nimage.tensor = image.url.load()\nmodel = SomeEmbeddingModel()\nimage.embedding = model(image.tensor)\nimage.second_embedding = model(image.tensor)\n</code></pre>"},{"location":"data_types/multimodal/multimodal/","title":"\ud83d\uddc3 Multimodal","text":""},{"location":"data_types/multimodal/multimodal/#multimodal","title":"\ud83d\uddc3 Multimodal","text":"<p>In this section, we will walk through how to use DocArray to process multiple data modalities in tandem. </p> <p>See also</p> <p>In this section, we will work with image and text data. If you are not yet familiar with how to process these  modalities individually, you may want to check out the <code>Image</code>  and <code>Text</code> examples first.</p>"},{"location":"data_types/multimodal/multimodal/#model-your-data","title":"Model your data","text":"<p>If you work with multiple modalities at the same time, most likely they stand in some relation with each other.  DocArray allows you to model your data and these relationships.</p>"},{"location":"data_types/multimodal/multimodal/#define-a-schema","title":"Define a schema","text":"<p>Suppose you want to model a page of a newspaper that contains a main text, an image URL, a corresponding tensor  as well as a description. You can model this example in the following way:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageTorchTensor, ImageUrl\nclass Page(BaseDoc):\nmain_text: str\nimg_url: ImageUrl = None\nimg_description: str = None\nimg_tensor: ImageTorchTensor = None\n</code></pre>"},{"location":"data_types/multimodal/multimodal/#instantiate-an-object","title":"Instantiate an object","text":"<p>After extending <code>BaseDoc</code> and defining your schema, you can instantiate an object with your actual  data.</p> <pre><code>page = Page(\nmain_text='Hello world',\nimg_url='https://github.com/docarray/docarray/blob/main/docs/assets/favicon.png?raw=true',\nimg_description='This is the image of an apple',\n)\npage.img_tensor = page.img_url.load()\npage.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 Page : 8f39674 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute                    \u2502 Value                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 main_text: str               \u2502 Hello world                                   \u2502\n\u2502 img_url: ImageUrl            \u2502 https://github.com/docarray/docarray/blob/ma\u2026 \u2502\n\u2502                              \u2502 ... (length: 90)                              \u2502\n\u2502 img_description: str         \u2502 This is DocArray                              \u2502\n\u2502 img_tensor: ImageTorchTensor \u2502 ImageTorchTensor of shape (320, 320, 3),      \u2502\n\u2502                              \u2502 dtype: torch.uint8                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/multimodal/multimodal/#access-data","title":"Access data","text":"<p>After instantiation, each modality can be accessed directly from the <code>Page</code> object:</p> <pre><code>print(page.main_text)\nprint(page.img_url)\nprint(page.img_description)\nprint(page.img_tensor)\n</code></pre> Output <pre><code>Hello world\nhttps://github.com/docarray/docarray/blob/main/docs/assets/favicon.png?raw=true\nThis is DocArray\nImageTorchTensor([[[0, 0, 0],\n                   [0, 0, 0],\n                   [0, 0, 0],\n                   ...,\n                   [0, 0, 0]]])\n</code></pre>"},{"location":"data_types/multimodal/multimodal/#nested-data","title":"Nested data","text":"<p>If the data you want to model requires a more complex structure, nesting your attributes may be a good solution.</p> <p>For this example, let's try to define a schema to represent a newspaper. The newspaper should consist of a cover page, any number of following pages, and some metadata. Further, each page contains a main text and can contain an image  and an image description.</p> <p>To implement this you can add a <code>Newspaper</code> class to the previous implementation. The newspaper has a required  <code>cover_page</code> attribute of type <code>Page</code> as well as a <code>pages</code> attribute, which is a <code>DocList</code> of <code>Page</code>s.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import ImageTorchTensor, ImageUrl\nclass Page(BaseDoc):\nmain_text: str\nimg_url: ImageUrl = None\nimg_description: str = None\nimg_tensor: ImageTorchTensor = None\nclass Newspaper(BaseDoc):\ncover: Page\npages: DocList[Page] = None\nmetadata: dict = None\n</code></pre> <p>You can instantiate this more complex <code>Newspaper</code> object the same way as before:</p> <pre><code>cover_page = Page(\nmain_text='DocArray Daily',\nimg_url='https://github.com/docarray/docarray/blob/main/docs/assets/favicon.png',\n)\npages = DocList[Page](\n[\nPage(\nmain_text='Hello world',\nimg_url='https://github.com/docarray/docarray/blob/main/docs/assets/favicon.png',\nimg_description='This is the image of an apple',\n),\nPage(main_text='Second page'),\nPage(main_text='Third page'),\n]\n)\ndocarray_daily = Newspaper(\ncover=cover_page,\npages=pages,\nmetadata={'author': 'DocArray and friends', 'issue': '0.30.0'},\n)\ndocarray_daily.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 Newspaper : 63189f7 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute      \u2502 Value                                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 metadata: dict \u2502 {'author': 'DocArray and friends', 'issue': '0.0.3 ... }    \u2502\n\u2502                \u2502 (length: 2)                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u251c\u2500\u2500 \ud83d\udd36 cover: Page\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 Page : ca164e3 ...\n\u2502       \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502       \u2502 Attribute         \u2502 Value                                            \u2502\n\u2502       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       \u2502 main_text: str    \u2502 DocArray Daily                                   \u2502\n\u2502       \u2502 img_url: ImageUrl \u2502 https://github.com/docarray/docarray/blob/main/\u2026 \u2502\n\u2502       \u2502                   \u2502 ... (length: 81)                                 \u2502\n\u2502       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2514\u2500\u2500 \ud83d\udca0 pages: DocList[Page]\n    \u251c\u2500\u2500 \ud83d\udcc4 Page : 64ed19c ...\n    \u2502   \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502   \u2502 Attribute            \u2502 Value                                         \u2502\n    \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   \u2502 main_text: str       \u2502 Hello world                                   \u2502\n    \u2502   \u2502 img_url: ImageUrl    \u2502 https://github.com/docarray/docarray/blob/ma\u2026 \u2502\n    \u2502   \u2502                      \u2502 ... (length: 81)                              \u2502\n    \u2502   \u2502 img_description: str \u2502 DocArray logoooo                              \u2502\n    \u2502   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u251c\u2500\u2500 \ud83d\udcc4 Page : 4bd7e45 ...\n    \u2502   \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502   \u2502 Attribute           \u2502 Value          \u2502\n    \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   \u2502 main_text: str      \u2502 Second page    \u2502\n    \u2502   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u2514\u2500\u2500 ... 1 more Page documents\n</code></pre>"},{"location":"data_types/table/table/","title":"\ud83d\udcca Table","text":""},{"location":"data_types/table/table/#table","title":"\ud83d\udcca Table","text":"<p>DocArray supports many different modalities including tabular data. This section will show you how to load and handle tabular data using DocArray.</p>"},{"location":"data_types/table/table/#load-csv-table","title":"Load CSV table","text":"<p>A common way to store tabular data is via <code>CSV</code> (comma-separated values) files. You can load such data from a given <code>CSV</code> file into a <code>DocList</code>. </p> <p>Let's take a look at the following example file, which includes data about books and their authors and year of publication:</p> <pre><code>title,author,year\nHarry Potter and the Philosopher's Stone,J. K. Rowling,1997\nKlara and the sun,Kazuo Ishiguro,2020\nA little life,Hanya Yanagihara,2015\n</code></pre> <p>First, define the Document schema describing the data:</p> <p></p><pre><code>from docarray import BaseDoc\nclass Book(BaseDoc):\ntitle: str\nauthor: str\nyear: int\n</code></pre> Next, load the content of the CSV file to a <code>DocList</code> instance of <code>Book</code>s via <code>.from_csv()</code>: <pre><code>from docarray import DocList\ndocs = DocList[Book].from_csv(\nfile_path='https://github.com/docarray/docarray/blob/main/tests/toydata/books.csv?raw=true'\n)\ndocs.summary()\n</code></pre> Output <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500 DocList Summary \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                             \u2502\n\u2502   Type     DocList[Book]    \u2502\n\u2502   Length   3                \u2502\n\u2502                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500 Document Schema \u2500\u2500\u256e\n\u2502                     \u2502\n\u2502   Book              \u2502\n\u2502   \u251c\u2500\u2500 title: str    \u2502\n\u2502   \u251c\u2500\u2500 author: str   \u2502\n\u2502   \u2514\u2500\u2500 year: int     \u2502\n\u2502                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The resulting <code>DocList</code> object contains three <code>Book</code>s, since each row of the CSV file corresponds to one book and is assigned to one <code>Book</code> instance.</p>"},{"location":"data_types/table/table/#save-to-csv-file","title":"Save to CSV file","text":"<p>Vice versa, you can also store your <code>DocList</code> data in a <code>.csv</code> file using <code>.to_csv()</code>:</p> <pre><code>docs.to_csv(file_path='/path/to/my_file.csv')\n</code></pre> <p>Tabular data is often not the best choice to represent nested Documents. Hence, nested Documents will be stored flattened and can be accessed by their <code>'__'</code>-separated access paths.</p> <p>Let's take a look at an example. We now want to store not only the book data but moreover book review data. To do so, we define a <code>BookReview</code> class that has a nested <code>book</code> attribute as well as the non-nested attributes <code>n_ratings</code> and <code>stars</code>:</p> <pre><code>class BookReview(BaseDoc):\nbook: Book\nn_ratings: int\nstars: float\nreview_docs = DocList[BookReview](\n[BookReview(book=book, n_ratings=12345, stars=5) for book in docs]\n)\nreview_docs.summary()\n</code></pre> Output <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DocList Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                   \u2502\n\u2502   Type     DocList[BookReview]    \u2502\n\u2502   Length   3                      \u2502\n\u2502                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500 Document Schema \u2500\u2500\u2500\u2500\u256e\n\u2502                         \u2502\n\u2502   BookReview            \u2502\n\u2502   \u251c\u2500\u2500 book: Book        \u2502\n\u2502   \u2502   \u251c\u2500\u2500 title: str    \u2502\n\u2502   \u2502   \u251c\u2500\u2500 author: str   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 year: int     \u2502\n\u2502   \u251c\u2500\u2500 n_ratings: int    \u2502\n\u2502   \u2514\u2500\u2500 stars: float      \u2502\n\u2502                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>As expected all nested attributes will be stored by their access path:</p> <pre><code>review_docs.to_csv(file_path='/path/to/nested_documents.csv')\n</code></pre> <pre><code>id,book__id,book__title,book__author,book__year,n_ratings,stars\nd6363aa3b78b4f4244fb976570a84ff7,8cd85fea52b3a3bc582cf56c9d612cbb,Harry Potter and the Philosopher's Stone,J. K. Rowling,1997,12345,5.0\n5b53fff67e6b6cede5870f2ee09edb05,87b369b93593967226c525cf226e3325,Klara and the sun,Kazuo Ishiguro,2020,12345,5.0\naddca0475756fc12cdec8faf8fb10d71,03194cec1b75927c2259b3c0fff1ab6f,A little life,Hanya Yanagihara,2015,12345,5.0\n</code></pre>"},{"location":"data_types/table/table/#handle-tsv-tables","title":"Handle TSV tables","text":"<p>Not only can you load and save comma-separated values (<code>CSV</code>) data, but also tab-separated values (<code>TSV</code>),  by adjusting the <code>dialect</code> parameter in <code>.from_csv()</code>  and <code>.to_csv()</code>.</p> <p>The dialect defaults to <code>'excel'</code>, which refers to comma-separated values. For tab-separated values, you can use  <code>'excel-tab'</code>.</p> <p>Let's take a look at what this would look like with a tab-separated file:</p> <pre><code>title   author  year\nTitle1  author1 2020\nTitle2  author2 1234\n</code></pre> <pre><code>docs = DocList[Book].from_csv(\nfile_path='https://github.com/docarray/docarray/blob/main/tests/toydata/books.tsv?raw=true',\ndialect='excel-tab',\n)\nfor doc in docs:\ndoc.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 Book : c1ac9d4 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute            \u2502 Value         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 title: str           \u2502 Title1        \u2502\n\u2502 author: str          \u2502 author1       \u2502\n\u2502 year: int            \u2502 2020          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udcc4 Book : c1ac9d4 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute            \u2502 Value         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 title: str           \u2502 Title1        \u2502\n\u2502 author: str          \u2502 author1       \u2502\n\u2502 year: int            \u2502 2020          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Great! All the data is correctly read and stored in <code>Book</code> instances.</p>"},{"location":"data_types/table/table/#other-separators","title":"Other separators","text":"<p>If your values are separated by yet another separator, you can create your own class that inherits from <code>csv.Dialect</code>. Within this class, you can define your dialect's behavior by setting the provided formatting parameters.</p> <p>For instance, let's assume you have a semicolon-separated table:</p> <pre><code>first_name;last_name;year\nJane;Austin;2020\nJohn;Doe;1234\n</code></pre> <p>Now, let's define our <code>SemicolonSeparator</code> class. Next to the <code>delimiter</code> parameter, we have to set some more formatting parameters such as <code>doublequote</code> and <code>lineterminator</code>.</p> <pre><code>import csv\nclass SemicolonSeparator(csv.Dialect):\ndelimiter = ';'\ndoublequote = True\nlineterminator = '\\r\\n'\nquotechar = '\"'\nquoting = csv.QUOTE_MINIMAL\n</code></pre> <p>Finally, you can load your data by setting the <code>dialect</code> parameter in <code>.from_csv()</code> to an instance of your <code>SemicolonSeparator</code>.</p> <pre><code>docs = DocList[Book].from_csv(\nfile_path='https://github.com/docarray/docarray/blob/main/tests/toydata/books_semicolon_sep.csv?raw=true',\ndialect=SemicolonSeparator(),\n)\nfor doc in docs:\ndoc.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 Book : 321e9fd ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute            \u2502 Value         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 title: str           \u2502 Title1        \u2502\n\u2502 author: str          \u2502 author1       \u2502\n\u2502 year: int            \u2502 2020          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udcc4 Book : 16d2097 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute            \u2502 Value         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 title: str           \u2502 Title2        \u2502\n\u2502 author: str          \u2502 author2       \u2502\n\u2502 year: int            \u2502 1234          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/tensor/tensor/","title":"\ud83d\udd22 Tensor","text":""},{"location":"data_types/tensor/tensor/#tensor","title":"\ud83d\udd22 Tensor","text":"<p>DocArray supports several tensor types that can you can use inside <code>BaseDoc</code>. </p> <p>The main ones are:</p> <ul> <li><code>NdArray</code> for NumPy tensors</li> <li><code>TorchTensor</code> for PyTorch tensors</li> <li><code>TensorFlowTensor</code> for TensorFlow tensors</li> </ul> <p>The three of them wrap their respective framework's tensor type. </p> <p>Note</p> <p><code>NdArray</code> and <code>TorchTensor</code> are a subclass of their native tensor type. This means that they can be used natively in their framework.</p> <p>Warning</p> <p><code>TensorFlowTensor</code> stores the pure <code>tf.Tensor</code> object inside the <code>tensor</code> attribute. This is due to a limitation of the TensorFlow framework that prevents you from subclassing the <code>tf.Tensor</code> object.</p> <p>DocArray also supports <code>AnyTensor</code>, which is the Union of the three previous tensor types.  This is a generic placeholder to specify that it can work with any tensor type (NumPy, PyTorch, TensorFlow).</p>"},{"location":"data_types/tensor/tensor/#tensor-shape-validation","title":"Tensor Shape validation","text":"<p>All three tensor types support shape validation. This means that you can specify the shape of the tensor using type hint syntax: <code>NdArray[100, 100]</code>, <code>TorchTensor[100, 100]</code>, <code>TensorFlowTensor[100, 100]</code>.</p> <p>Let's take an example:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\ntensor: NdArray[100, 100]\n</code></pre> <p>If you try to pass a tensor with a different shape, an error will be raised:</p> <pre><code>import numpy as np\ntry:\ndoc = MyDoc(tensor=np.zeros((100, 200)))\nexcept ValueError as e:\nprint(e)\n</code></pre> <pre><code>1 validation error for MyDoc\ntensor\n  cannot reshape array of size 20000 into shape (100,100) (type=value_error)\n</code></pre> <p>Whereas if you just pass a tensor with the correct shape, no error will be raised:</p> <pre><code>doc = MyDoc(tensor=np.zeros((100, 100)))\n</code></pre>"},{"location":"data_types/tensor/tensor/#axes-validation","title":"Axes validation","text":"<p>You can check that the number of axes is correct by specifying <code>NdArray['x','y']</code>, <code>TorchTensor['x','y']</code>, <code>TensorFlowTensor['x','y']</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\ntensor: NdArray['x', 'y']\n</code></pre> <p>Here you can only pass a tensor with two axes. <code>np.zeros(10, 12)</code> will work, but <code>np.zeros(10, 12, 3)</code> will raise an error.</p>"},{"location":"data_types/tensor/tensor/#axis-names","title":"Axis names","text":"<p>You can specify that two axes should have the same dimensions with the syntax <code>NdArray['x', 'x']</code>, <code>TorchTensor['x', 'x']</code>, <code>TensorFlowTensor['x', 'x']</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\ntensor: NdArray['x', 'x']\n</code></pre> <p>Here you can only pass a tensor with two axes that have the same dimensions. <code>np.zeros(10, 10)</code> will work but <code>np.zeros(10, 12)</code> will raise an error.</p>"},{"location":"data_types/tensor/tensor/#arbitrary-number-of-axis","title":"Arbitrary number of axis","text":"<p>To specify that your shape can have an arbitrary number of axes, use the syntax <code>NdArray['x', ...]</code>, or <code>NdArray[..., 'x']</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\ntensor: NdArray[100, ...]\n</code></pre> <p>Here you can only pass a tensor with at least one axis with dimension 100. <code>np.zeros(100, 10)</code> will work but <code>np.zeros(10, 12)</code> will raise an error.</p>"},{"location":"data_types/tensor/tensor/#tensor-type-validation","title":"Tensor type validation","text":"<p>You don't need to directly instantiate the  <code>NdArray</code> , <code>TorchTensor</code>, or <code>TensorFlowTensor</code> by yourself.</p> <p>Instead, you should use them as type hints on <code>BaseDoc</code> fields, where they perform data validation. During this process, <code>BaseDoc</code> will cast the native tensor type into the respective DocArray tensor type.</p> <p>Let's look at an example:</p> <p></p><pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nimport numpy as np\nclass MyDoc(BaseDoc):\ntensor: NdArray\ndoc = MyDoc(tensor=np.zeros(100))\nassert isinstance(doc.tensor, NdArray)  # True\n</code></pre> Here you see that the <code>doc.tensor</code> is an <code>NdArray</code>: <pre><code>assert isinstance(doc.tensor, np.ndarray)  # True as well\n</code></pre> <p>But since it inherits from <code>np.ndarray</code>, you can also use it as a normal NumPy array. The same holds for PyTorch and <code>TorchTensor</code>.</p>"},{"location":"data_types/tensor/tensor/#type-coercion-with-different-tensor-types","title":"Type coercion with different tensor types","text":"<p>DocArray also supports type coercion between different tensor types. This mean that if you pass a different tensor type to a tensor field, it will be converted to the correct tensor type.</p> <p>For instance, if you define a field of type <code>TorchTensor</code> and you pass a NumPy array to it, it will be converted to a <code>TorchTensor</code>.</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor\nimport numpy as np\nclass MyTensorsDoc(BaseDoc):\ntensor: TorchTensor\ndoc = MyTensorsDoc(tensor=np.zeros(512))\ndoc.summary()\n</code></pre> <pre><code>\ud83d\udcc4 MyTensorsDoc : 0a10f88 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute           \u2502 Value                                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 tensor: TorchTensor \u2502 TorchTensor of shape (512,), dtype: torch.float64      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>It also works in the other direction:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nimport torch\nclass MyTensorsDoc(BaseDoc):\ntensor: NdArray\ndoc = MyTensorsDoc(tensor=torch.zeros(512))\ndoc.summary()\n</code></pre> <pre><code>\ud83d\udcc4 MyTensorsDoc : 157e6f5 ...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute       \u2502 Value                                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 tensor: NdArray \u2502 NdArray of shape (512,), dtype: float32                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"data_types/tensor/tensor/#docvec-with-anytensor","title":"<code>DocVec</code> with <code>AnyTensor</code>","text":"<p><code>DocVec</code> can be used with a <code>BaseDoc</code> which has a field of <code>AnyTensor</code> or any other Union of tensor types. </p> <p>However, the <code>DocVec</code> needs to know the tensor type of the tensor field beforehand to create the correct column.</p> <p>You can specify these parameters with the <code>tensor_type</code> parameter of the <code>DocVec</code> constructor:</p> <pre><code>from docarray import BaseDoc, DocVec\nfrom docarray.typing import AnyTensor, NdArray\nimport numpy as np\nclass MyDoc(BaseDoc):\ntensor: AnyTensor\ndocs = DocVec[MyDoc](\n[MyDoc(tensor=np.zeros(100)) for _ in range(10)], tensor_type=NdArray\n)\nassert isinstance(docs.tensor, NdArray)\n</code></pre> <p>Note</p> <p><code>NdArray</code> will be used by default if:</p> <ul> <li>you don't specify the <code>tensor_type</code> parameter</li> <li>your tensor field is a Union of tensor or <code>AnyTensor</code></li> </ul>"},{"location":"data_types/tensor/tensor/#compatibility-of-torchtensor-and-torchcompile","title":"Compatibility of <code>TorchTensor</code> and <code>torch.compile()</code>","text":"<p>PyTorch 2 introduced compilation support in the form of <code>torch.compile()</code>.</p> <p>Currently, <code>torch.compile()</code> does not properly support subclasses of <code>torch.Tensor</code> such as <code>TorchTensor</code>. The PyTorch team is currently working on a fix for this issue.</p> <p>For a workaround to this issue, see the <code>TorchTensor</code> API reference.</p>"},{"location":"data_types/text/text/","title":"\ud83d\udd24 Text","text":""},{"location":"data_types/text/text/#text","title":"\ud83d\udd24 Text","text":"<p>DocArray supports many different modalities including <code>Text</code>. This section will show you how to load and handle text data using DocArray.</p> <p>Tip</p> <p>Check out our predefined <code>TextDoc</code> to get started and play around with our text features.</p> <p>You can store text in DocArray like this:</p> <pre><code>from docarray import BaseDoc\nclass MyText(BaseDoc):\ntext: str = None\ndoc = MyText(text='Hello world!')\n</code></pre> <p>Text can include any type of character, including emojis:</p> <pre><code>doc.text = '\ud83d\udc4b   \u0928\u092e\u0938\u094d\u0924\u0947 \u0926\u0941\u0928\u093f\u092f\u093e!  \u4f60\u597d\u4e16\u754c\uff01\u3053\u3093\u306b\u3061\u306f\u4e16\u754c\uff01   \u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440!'\n</code></pre>"},{"location":"data_types/text/text/#load-text-file","title":"Load text file","text":"<p>If your text data is too long to be written inline or if it is stored in a file, you can first define the URL as a <code>TextUrl</code> and then load the text data.</p> <p>Let's first define a schema:</p> <p></p><pre><code>from docarray import BaseDoc\nfrom docarray.typing import TextUrl\nclass MyText(BaseDoc):\ntext: str = None\nurl: TextUrl = None\n</code></pre> Next, instantiate a <code>MyText</code> object with a <code>url</code> attribute and load its content to the <code>text</code> field. <pre><code>doc = MyText(\nurl='https://www.w3.org/History/19921103-hypertext/hypertext/README.html',\n)\ndoc.text = doc.url.load()\nassert doc.text.startswith('&lt;TITLE&gt;Read Me&lt;/TITLE&gt;')\n</code></pre>"},{"location":"data_types/text/text/#segment-long-texts","title":"Segment long texts","text":"<p>When you index or search text data, you often don\u2019t want to consider thousands of words as one huge string.  Instead, some finer granularity would be nice. You can do this by leveraging nested fields. For example, let\u2019s split some page content into its sentences by <code>'.'</code>:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Sentence(BaseDoc):\ntext: str\nclass Page(BaseDoc):\ncontent: DocList[Sentence]\nlong_text = 'First sentence. Second sentence. And many many more sentences.'\npage = Page(content=[Sentence(text=t) for t in long_text.split('.')])\npage.summary()\n</code></pre> Output <pre><code>\ud83d\udcc4 Page : 13d909a ...\n\u2514\u2500\u2500 \ud83d\udca0 content: DocList[Sentence]\n    \u251c\u2500\u2500 \ud83d\udcc4 Sentence : 6725382 ...\n    \u2502   \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502   \u2502 Attribute      \u2502 Value               \u2502\n    \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   \u2502 text: str      \u2502 First sentence      \u2502\n    \u2502   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u251c\u2500\u2500 \ud83d\udcc4 Sentence : 17a934c ...\n    \u2502   \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502   \u2502 Attribute     \u2502 Value                \u2502\n    \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   \u2502 text: str     \u2502  Second sentence     \u2502\n    \u2502   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u2514\u2500\u2500 ... 2 more Sentence documents\n</code></pre>"},{"location":"data_types/text/text/#getting-started-predefined-textdoc","title":"Getting started - Predefined <code>TextDoc</code>","text":"<p>To get started and play around with your text data, DocArray provides a predefined <code>TextDoc</code>, which includes all of the previously mentioned functionalities:</p> <pre><code>class TextDoc(BaseDoc):\ntext: Optional[str] = None\nurl: Optional[TextUrl] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[bytes] = None\n</code></pre>"},{"location":"data_types/video/video/","title":"\ud83c\udfa5 Video","text":""},{"location":"data_types/video/video/#video","title":"\ud83c\udfa5 Video","text":"<p>DocArray supports many modalities including <code>Video</code>. This section will show you how to load and handle video data using DocArray.</p> <p>Moreover, you will learn about DocArray's video-specific types, to represent your video data ranging from <code>VideoUrl</code> to <code>VideoBytes</code> and <code>VideoNdArray</code>.</p> <p>Note</p> <p>This requires an <code>av</code> dependency. You can install all necessary dependencies via: </p><pre><code>pip install \"docarray[video]\"\n</code></pre>"},{"location":"data_types/video/video/#load-video-data","title":"Load video data","text":"<p>In DocArray video data is represented by a video tensor, an audio tensor, and the key frame indices. </p> <p></p> <p>Tip</p> <p>Check out our predefined <code>VideoDoc</code> to get started and play around with our video features.</p> <p>First, let's define a <code>MyVideo</code> class with all of those attributes and instantiate an object with a local or remote URL:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import AudioNdArray, NdArray, VideoNdArray, VideoUrl\nclass MyVideo(BaseDoc):\nurl: VideoUrl\nvideo: VideoNdArray = None\naudio: AudioNdArray = None\nkey_frame_indices: NdArray = None\ndoc = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\n</code></pre> <p>Now you can load the video file content by simply calling <code>.load()</code> on your <code>AudioUrl</code> instance. This will return a NamedTuple of a video tensor, an audio tensor, and the key frame indices:</p> <ul> <li>The video tensor is a 4-dimensional array of shape <code>(n_frames, height, width, channels)</code>. <ul> <li>The first dimension represents the frame id. </li> <li>The last three dimensions represent the image data of the corresponding frame. </li> </ul> </li> <li>If the video contains audio, it will be stored as an <code>AudioNdArray</code>.</li> <li>Additionally, the key frame indices will be stored. A key frame is defined as the starting point of any smooth transition.</li> </ul> <pre><code>doc.video, doc.audio, doc.key_frame_indices = doc.url.load()\nassert isinstance(doc.video, VideoNdArray)\nassert isinstance(doc.audio, AudioNdArray)\nassert isinstance(doc.key_frame_indices, NdArray)\nprint(doc.video.shape)\n</code></pre> <pre><code>(250, 176, 320, 3)\n</code></pre> <p>For the given example you can infer from <code>doc.video</code>'s shape that the video contains 250 frames of size 176x320 in RGB mode.  Based on the overall length of the video (10 seconds), you can infer the framerate is approximately 250/10 = 25 frames per second (fps).</p>"},{"location":"data_types/video/video/#videotensor","title":"VideoTensor","text":"<p>DocArray offers several <code>VideoTensor</code>s to store your data to:</p> <ul> <li><code>VideoNdArray</code></li> <li><code>VideoTorchTensor</code></li> <li><code>VideoTensorFlowTensor</code></li> </ul> <p>If you specify the type of your tensor as one of the above, it will be cast to that automatically:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import VideoTensorFlowTensor, VideoTorchTensor, VideoUrl\nclass MyVideo(BaseDoc):\nurl: VideoUrl\ntf_tensor: VideoTensorFlowTensor = None\ntorch_tensor: VideoTorchTensor = None\ndoc = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ndoc.tf_tensor = doc.url.load().video\ndoc.torch_tensor = doc.url.load().video\nassert isinstance(doc.tf_tensor, VideoTensorFlowTensor)\nassert isinstance(doc.torch_tensor, VideoTorchTensor)\n</code></pre>"},{"location":"data_types/video/video/#videobytes","title":"VideoBytes","text":"<p>Alternatively, you can load your <code>VideoUrl</code> instance to <code>VideoBytes</code>, and your <code>VideoBytes</code> instance to a <code>VideoTensor</code> of your choice:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import VideoTensor, VideoUrl, VideoBytes\nclass MyVideo(BaseDoc):\nurl: VideoUrl\nbytes_: VideoBytes = None\nvideo: VideoTensor = None\ndoc = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\ndoc.bytes_ = doc.url.load_bytes()\ndoc.video = doc.url.load().video\n</code></pre> <p>Vice versa, you can also transform a <code>VideoTensor</code> to  <code>VideoBytes</code>:</p> <pre><code>from docarray.typing import VideoBytes\nbytes_from_tensor = doc.video.to_bytes()\nassert isinstance(bytes_from_tensor, VideoBytes)\n</code></pre>"},{"location":"data_types/video/video/#key-frame-extraction","title":"Key frame extraction","text":"<p>A key frame is defined as the starting point of any smooth transition. Given the key frame indices, you can access selected scenes:</p> <pre><code>indices = doc.key_frame_indices\nfirst_scene = doc.video[indices[0] : indices[1]]\nassert (indices == [0, 95]).all()\nassert first_scene.shape == (95, 176, 320, 3)\n</code></pre> <p>Or you can access the first frame of all new scenes and display them in a notebook:</p> <pre><code>from docarray.typing import ImageNdArray\nfrom pydantic import parse_obj_as\nkey_frames = doc.video[doc.key_frame_indices]\nfor frame in key_frames:\nimg = parse_obj_as(ImageNdArray, frame)\nimg.display()\n</code></pre> <p></p>"},{"location":"data_types/video/video/#save-video-to-file","title":"Save video to file","text":"<p>You can save your video tensor to a file. In the example below you save the video with a framerate of 60 fps, which results in a 4-second video, instead of the original 10-second video with a frame rate of 25 fps. </p> <pre><code>doc.video.save(\nfile_path=\"/path/my_video.mp4\",\nvideo_frame_rate=60,\n)\n</code></pre>"},{"location":"data_types/video/video/#display-video-in-a-notebook","title":"Display video in a notebook","text":"<p>You can play a video in a notebook from its URL as well as its tensor, by calling <code>.display()</code> on either one. For the latter, you can optionally give the corresponding <code>AudioTensor</code> as a parameter.</p> <pre><code>doc_fast = MyAudio(url=\"/path/my_video.mp4\")\ndoc_fast.url.display()\n</code></pre> <p></p>"},{"location":"data_types/video/video/#getting-started-predefined-videodoc","title":"Getting started - Predefined <code>VideoDoc</code>","text":"<p>To get started and play around with your video data, DocArray provides a predefined <code>VideoDoc</code>, which includes all of the previously mentioned functionalities:</p> <pre><code>class VideoDoc(BaseDoc):\nurl: Optional[VideoUrl] = None\naudio: Optional[AudioDoc] = AudioDoc()\ntensor: Optional[VideoTensor] = None\nkey_frame_indices: Optional[AnyTensor] = None\nembedding: Optional[AnyEmbedding] = None\nbytes_: Optional[bytes] = None\n</code></pre> <p>You can use this class directly or extend it to your preference:</p> <pre><code>from typing import Optional\nfrom docarray.documents import VideoDoc\n# extend it\nclass MyVideo(VideoDoc):\nname: Optional[str] = None\nvideo = MyVideo(\nurl='https://github.com/docarray/docarray/blob/main/tests/toydata/mov_bbb.mp4?raw=true'\n)\nvideo.name = 'My first video doc!'\nvideo.tensor = video.url.load().video\n</code></pre>"},{"location":"how_to/add_doc_index/","title":"Add a new Document Index","text":""},{"location":"how_to/add_doc_index/#add-a-new-document-index","title":"Add a new Document Index","text":"<p>In DocArray a Document Index is a class that takes documents, optionally persists them, and makes them searchable. Different Document Indexes leverage different backends, like Weaviate, Qdrant, HNSWLib etc.</p> <p>This document shows covers adding a new Document Index to DocArray.</p> <p>This can be broken down into a number of steps:</p> <ol> <li>Install and user instructions</li> <li>Create a new class that inherits from <code>BaseDocIndex</code></li> <li>Declare default configurations for your Document Index</li> <li>Implement abstract methods for indexing, searching, and deleting</li> <li>Implement a Query Builder for your Document Index</li> </ol> <p>In general, the steps above can be followed in roughly that order.</p> <p>However, a Document Index implementation is usually very interconnected, so you will probably have to jump between these steps a bit, both in your implementation and in the guide below.</p> <p>For an end-to-end example of this process, check out the existing HNSWLib Document Index implementation.</p> <p>Warning</p> <p>Caution: The HNSWLib Document Index implementation can be used as a reference, but it is special in some key ways. For example, HNSWLib can only index vectors, so it uses SQLite to store the rest of the documents alongside it. This is not how you should store documents in your implementation! You can find guidance on how you should do it below.</p>"},{"location":"how_to/add_doc_index/#installation-and-user-instructions","title":"Installation and user instructions","text":"<p>Add the library required for your Index via poetry:</p> <pre><code>poetry add {my_index_lib}\n</code></pre> <p>The <code>pyproject.toml</code> file should now look like this:</p> <pre><code>[tool.poetry.dependencies]\nmy_index_lib = \"&gt;=123.456.789\"\n</code></pre> <p>Mark the library as optional and manually create an <code>extra</code> for it:</p> <pre><code>[tool.poetry.dependencies]\nmy_index_lib = {version = \"&gt;=0.6.2\", optional = true }\n[tool.poetry.extras]\nmy_index_extra = [\"my_index_lib\"]\n</code></pre> <p>In case the user tries to use your Index without the correct installs, we want to throw an error with corresponding instructions.</p> <p>To enable this, first, add instructions to the <code>INSTALL_INSTRUCTIONS</code> dictionary in <code>docarray/utils/misc.py</code>, such as </p> <pre><code>{'my_index_lib': '\"docarray[my_index_extra]\"'}\n</code></pre> <p>Next, ensure you add a case to the <code>__getattr__()</code> in <code>docarray/index/__init__.py</code> for your new Index. By doing so, the user will be given the instructions when trying to import <code>MyIndex</code> without the correct libraries installed.</p> <pre><code>if TYPE_CHECKING:\nfrom docarray.index.backends.my_index import MyIndex  # noqa: F401\ndef __getattr__(name: str):\nif name == 'HnswDocumentIndex':\nimport_library('hnswlib', raise_error=True)\nfrom docarray.index.backends.my_index import MyIndex  # noqa\n__all__.append('MyIndex')\nreturn MyIndex\n</code></pre> <p>Additionally, wrap the required imports in the file where the <code>MyIndex</code> class will be located, like it is done in <code>docarray/index/backends/hnswlib.py</code>.</p>"},{"location":"how_to/add_doc_index/#create-a-new-document-index-class","title":"Create a new Document Index class","text":"<p>To get started, create a new class that inherits from <code>BaseDocIndex</code> and <code>typing.Generic</code>:</p> <pre><code>TSchema = TypeVar('TSchema', bound=BaseDoc)\nclass MyDocumentIndex(BaseDocIndex, Generic[TSchema]):\n...\n</code></pre> <p>Here, <code>TSchema</code> is a type variable representing the schema of the Document Index, which is a <code>Document</code>. You can use it in type hints of methods and attributes.</p>"},{"location":"how_to/add_doc_index/#create-the-constructor","title":"Create the constructor","text":"<p>You can write an <code>__init__</code> method for your Document Index where you set up all the needed bells and whistles:</p> <pre><code>def __init__(self, db_config=None, **kwargs):\nsuper().__init__(db_config=db_config, **kwargs)\n...\n</code></pre> <p>Ensure you call the <code>super().__init__</code> method, which will do some basic initialization for you.</p>"},{"location":"how_to/add_doc_index/#set-up-your-backend","title":"Set up your backend","text":"<p>Your backend (database or similar) should represent documents in the following way:</p> <ul> <li>Every field of a document is a column in the database.</li> <li>Column types follow a default that you define, based on the type hint of the associated field, but can also be configured by the user.</li> <li>Every row in your database thus represents a document.</li> <li>Nesting: The most common way to handle nested documents (and the one where the <code>AbstractDocumentIndex</code> will hold your hand the most), is to flatten out nested documents. But if your backend natively supports nesting representations, then feel free to leverage those!</li> </ul> <p>Warning</p> <p>Don't take too much inspiration from the HNSWLib Document Index implementation on this point, as it is a bit of a special case.</p> <p>Also, check the Document Index is being set up \"fresh\", meaning no data was previously persisted. Then create a new database table (or the equivalent concept in you backend) for the documents, otherwise, the Document Index should connect to the existing database and table. You can determine this based on <code>self._db_config</code> (see below).</p> <p>Note</p> <p>If you are integrating a database, your Document Index should always assume there is already a database running that it can connect to. It should not spawn a new database instance.</p> <p>To help with all of this, <code>super().__init__</code> inject a few helpful attributes for you (more info in the dedicated sections below):</p> <ul> <li><code>self._schema</code></li> <li><code>self._db_config</code></li> <li><code>self._runtime_config</code></li> <li><code>self._column_infos</code></li> </ul>"},{"location":"how_to/add_doc_index/#the-_schema","title":"The <code>_schema</code>","text":"<p>When a user instantiates a Document Index, they do so in a parametric way:</p> <pre><code>class Inner(BaseDoc):\nembedding: NdArray[512]\nclass MyDoc(BaseDoc):\ntensor: NdArray[100]\nother_tensor: NdArray = Field(dim=10, space='cosine')\ndescription: str\ninner: Inner\nstore = MyDocumentIndex[MyDoc]()\n</code></pre> <p>In this case, <code>store</code> would have a class attribute <code>_schema</code> that is the <code>MyDoc</code> class. This is done automatically for you, and you can use it in your implementation.</p>"},{"location":"how_to/add_doc_index/#the-_db_config","title":"The <code>_db_config</code>","text":"<p>The <code>_db_config</code> is a dataclass that contains all \"static\" configurations of your Document Index. Users can pass these configurations to the <code>__init__</code> method of your Document Index, and <code>self._db_config</code> will be populated for you, so that you can use it in your implementation.</p> <p>You can declare allowed fields and default values for your <code>_db_config</code>, but you will see that later.</p>"},{"location":"how_to/add_doc_index/#the-_runtime_config","title":"The <code>_runtime_config</code>","text":"<p>The <code>_runtime_config</code> is a dataclass that contains all \"dynamic\" configurations of your Document Index. Users can pass these configurations to the <code>.configure()</code> method of your Document Index, and <code>self._runtime_config</code> will be populated for you, so that you can use it in your implementation.</p> <p>You can declare allowed fields and default values for your <code>_runtime_config</code>, but you will see that later.</p>"},{"location":"how_to/add_doc_index/#the-_column_infos","title":"The <code>_column_infos</code>","text":"<p><code>self._column_infos</code> is a dictionary that contains information about all columns in your Document Index instance.</p> <p>This information is automatically extracted from <code>self._schema</code>, and populated for you.</p> <p>Concretely, <code>self._column_infos: Dict[str, _ColumnInfo]</code> maps from a column name to a <code>_ColumnInfo</code> dataclass.</p> <p>For the <code>MyDoc</code> schema above, the column names would be <code>tensor</code>, <code>other_tensor</code>, <code>description</code>, <code>id</code>, <code>inner__embedding</code>, and <code>inner__id</code>. These are the key of <code>self._column_infos</code>.</p> <p>The values of <code>self._column_infos</code> are <code>_ColumnInfo</code> dataclasses, which have the following form:</p> <pre><code>@dataclass\nclass _ColumnInfo:\ndocarray_type: Type\ndb_type: Any\nn_dim: Optional[int] = None\nconfig: Dict[str, Any]\n</code></pre> <ul> <li><code>docarray_type</code> is the type of the column in DocArray, e.g. <code>AbstractTensor</code> or <code>str</code></li> <li><code>db_type</code> is the type of the column in the Document Index, e.g. <code>np.ndarray</code> or <code>str</code>. You can customize the mapping from <code>docarray_type</code> to <code>db_type</code>, as we will see later.</li> <li><code>config</code> is a dictionary of configurations for the column. For example, the <code>other_tensor</code> column above would contain the <code>space</code> and <code>dim</code> configurations.</li> <li><code>n_dim</code> is the dimensionality of the column, e.g. <code>100</code> for a 100-dimensional vector. See further guidance on this below.</li> </ul> <p>Again, these are automatically populated for you, so you can just use them in your implementation.</p> <p>Note</p> <p><code>_ColumnInfo.docarray_type</code> contains the python type as specified in <code>self._schema</code>, whereas  <code>_ColumnInfo.db_type</code> contains the data type of a particular database column.</p> <p>By default, it holds that <code>_ColumnInfo.docarray_type == self.python_type_to_db_type(_ColumnInfo.db_type)</code>, as we will see later. However, you should not rely on this, because a user can manually specify a different db_type.  Therefore, your implementation should rely on <code>_ColumnInfo.db_type</code> and not directly call <code>python_type_to_db_type()</code>.</p> <p>Warning</p> <p>If a subclass of <code>AbstractTensor</code> appears in the Document Index's schema (i.e. <code>TorchTensor</code>, <code>NdArray</code>, or <code>TensorFlowTensor</code>), then <code>_ColumnInfo.docarray_type</code> will simply show <code>AbstractTensor</code> instead of the specific subclass. This is because the abstract class normalizes all input data of type <code>AbstractTensor</code> to <code>np.ndarray</code> anyways, which should make your life easier. Just be sure to properly handle <code>AbstractTensor</code> as a possible value or <code>_ColumnInfo.docarray_type</code>, and you won't have to worry about the differences between torch, tf, and np.</p>"},{"location":"how_to/add_doc_index/#properly-handle-n_dim","title":"Properly handle <code>n_dim</code>","text":"<p><code>_ColumnInfo.n_dim</code> is automatically obtained from type parametrizations of the form <code>NdArray[100]</code>; if there isn't such a parametrization, <code>n_dim</code> of the columns will be <code>None</code>.</p> <p>You should also provide another way of defining the dimensionality of your columns, specifically by exposing a parameter in <code>Field(...)</code> (see example schema at the top).</p> <p>This leads to four possible scenarios:</p> <p>Scenario 1: Only <code>n_dim</code> is defined</p> <p>Imagine the user defines this schema:</p> <pre><code>class MyDoc(BaseDoc):\ntensor: NdArray[100]\nindex = MyDocumentIndex[MyDoc]()\n</code></pre> <p>In that case, the following will be true: <code>self._column_infos['tensor'].n_dim == 100</code> and <code>self._column_infos['tensor'].config == {}</code>. The <code>tensor</code> column in your backend should be configured to have dimensionality <code>100</code>.</p> <p>Scenario 2: Only <code>Field(...)</code> is defined</p> <p>Now, imagine the user defines this schema:</p> <pre><code>class MyDoc(BaseDoc):\ntensor: NdArray = Field(dim=50)\nindex = MyDocumentIndex[MyDoc]()\n</code></pre> <p>In that case, <code>self._column_infos['tensor'].n_dim is None</code> and <code>self._column_infos['tensor'].config['dim'] == 50</code>. The <code>tensor</code> column in your backend should be configured to have dimensionality <code>50</code>.</p> <p>Scenario 3: Both <code>n_dim</code> and <code>Field(...)</code> are defined</p> <p>Now, imagine this schema:</p> <pre><code>class MyDoc(BaseDoc):\ntensor: NdArray[100] = Field(dim=50)\nindex = MyDocumentIndex[MyDoc]()\n</code></pre> <p>In this case, <code>self._column_infos['tensor'].n_dim == 100</code> and <code>self._column_infos['tensor'].config['dim'] == 50</code>. The <code>tensor</code> column in your backend should be configured to have dimensionality <code>100</code>, as <code>n_dim</code> takes precedence over <code>Field(...)</code>.</p> <p>Scenario 4: Neither <code>n_dim</code> nor <code>Field(...)</code> are defined</p> <p>Finally, imagine this:</p> <pre><code>class MyDoc(BaseDoc):\ntensor: NdArray\nindex = MyDocumentIndex[MyDoc]()\n</code></pre> <p>In this case, <code>self._column_infos['tensor'].n_dim is None</code> and <code>self._column_infos['tensor'].config == {}</code>. If your backend can handle tensor/embedding columns without defined dimensionality, you should leverage that mechanism. Otherwise, raise an Exception.</p>"},{"location":"how_to/add_doc_index/#declare-default-configurations","title":"Declare default configurations","text":"<p>We have already made reference to the <code>_db_config</code> and <code>_runtime_config</code> attributes.</p> <p>To define what can be stored in them, and what the default values are, you need to create two inner classes:</p> <pre><code>@dataclass\nclass DBConfig(BaseDocIndex.DBConfig):\ndefault_column_config: Dict[Type, Dict[str, Any]] = ...\n@dataclass\nclass RuntimeConfig(BaseDocIndex.RuntimeConfig):\n...\n</code></pre> <p>Note</p> <ul> <li><code>DBConfig</code> inherits from <code>BaseDocIndex.DBConfig</code> and <code>RuntimeConfig</code> inherits from <code>BaseDocIndex.RuntimeConfig</code></li> <li>All fields in each dataclass need to have default values. Choose these sensibly, as they will be used if the user does not specify a value.</li> </ul>"},{"location":"how_to/add_doc_index/#the-dbconfig-class","title":"The <code>DBConfig</code> class","text":"<p>The <code>DBConfig</code> class defines the static configurations of your Document Index. These are configurations that are tied to the database (or library) running in the background, such as <code>host</code>, <code>port</code>, etc. Here you should put everything that the user cannot or should not change after initialization.</p> <p>Note</p> <p>Every <code>DBConfig</code> needs to contain a <code>default_column_config</code> field. This is a dictionary that, for each possible column type in your database, defines a default configuration for that column type. This will automatically be passed to a <code>_ColumnInfo</code> whenever a user does not manually specify a configuration for that column.</p> <p>For example, in the <code>MyDoc</code> schema above, the <code>tensor</code> <code>_ColumnInfo</code> would have a default configuration specified for <code>np.ndarray</code> columns.</p> <p>What is actually contained in these type-dependant configurations is up to you (and database specific). For example, for <code>np.ndarray</code> columns you could define the configurations <code>index_type</code> and <code>metric_type</code>, and for <code>varchar</code> columns you could define a <code>max_length</code> configuration.</p> <p>It is probably best to see this in action, so you should check out the <code>HnswDocumentIndex</code> implementation.</p>"},{"location":"how_to/add_doc_index/#the-runtimeconfig-class","title":"The <code>RuntimeConfig</code> class","text":"<p>The <code>RuntimeConfig</code> class defines the dynamic configurations of your Document Index. These are configurations that can be changed at runtime, for example default behaviours such as batch sizes, consistency levels, etc.</p> <p>It is a common pattern to allow such parameters both in the <code>RuntimeConfig</code>, where they will act as global defaults, and in specific methods (<code>index</code>, <code>find</code>, etc.), where they will act as local overrides.</p>"},{"location":"how_to/add_doc_index/#implement-abstract-methods-for-indexing-searching-and-deleting","title":"Implement abstract methods for indexing, searching, and deleting","text":"<p>After you've done the basic setup above, you can jump into the good stuff: implementing the actual indexing, searching, and deleting.</p> <p>In general, the following is true:</p> <ul> <li>For every method that you need to implement, there is a public variant (e.g. <code>index</code>) and a private variant (e.g. <code>_index</code>)</li> <li>You should usually implement the private variant, which is called by the already-implemented public variant. This should make your life easier, because some preprocessing and data normalization will already be done for you.</li> <li>You can, however, also implement the public variant directly, if you want to do something special.</li> </ul> <p>Warning</p> <p>While implementing the public variant directly is a perfectly fine thing to do, it may create more maintenance work for you in the future, because the public variant defined in the <code>BaseDocIndex</code> might change in the future, and you will have to update your implementation accordingly.</p> <p>Further:</p> <ul> <li>You don't absolutely have to implement everything. If a feature (e.g. <code>text_search</code>) is not supported by your backend, just raise a <code>NotImplementedError</code> in the corresponding method.</li> <li>Many methods come in a \"singular\" variant (e.g. <code>find</code>) and a \"batched\" variant (e.g. <code>find_batched</code>).</li> <li>The \"singular\" variant expects a single input, be it an ANN query, a text query, a filter, etc., and return matches and scores for that single input</li> <li>The \"batched\" variant expects a batch of inputs, and returns of matches and scores for each input</li> <li>Your implementations of, e.g., <code>_find()</code>, <code>_index()</code> etc. are allowed to take additional optional keyword arguments.   These can then be used to control DB specific behaviours, such as consistency levels, batch sizes, etc. As mentioned above, it is good practice to mirror these arguments in <code>self.RuntimeConfig</code>.</li> </ul> <p>Overall, you're asked to implement the methods that appear after the <code>Abstract methods; Subclasses must implement these</code> comment in the <code>BaseDocIndex</code> class. The details of each method should become clear from the docstrings and type hints.</p>"},{"location":"how_to/add_doc_index/#the-python_type_to_db_type-method","title":"The <code>python_type_to_db_type()</code> method","text":"<p>This method is slightly special, because </p> <ol> <li>It is not exposed to the user</li> <li>You absolutely have to implement it</li> </ol> <p>It is intended to take a type of a field in the store's schema (e.g. <code>AbstractTensor</code> for <code>tensor</code>), and return the corresponding type in the database (e.g. <code>np.ndarray</code>).</p> <p>The <code>BaseDocIndex</code> class uses this information to create and populate the <code>_ColumnInfo</code>s in <code>self._column_infos</code>.</p> <p>If the user wants to change the default behaviour, one can set the db type by using the <code>col_type</code> field:</p> <pre><code>class MySchema(BaseDoc):\nmy_num: float = Field(col_type='float64')\nmy_text: str = Field(..., col_type='varchar', max_len=2048)\n</code></pre> <p>In this case, the <code>db_type</code> of <code>my_num</code> will be <code>'float64'</code> and the <code>db_type</code> of <code>my_text</code> will be <code>'varchar'</code>.  Additional information regarding the <code>col_type</code>, such as <code>max_len</code> for <code>varchar</code> will be stored in the <code>_ColumnsInfo.config</code>. The given <code>col_type</code> has to be a valid <code>db_type</code>, meaning that has to be described in the index's <code>DBConfig.default_column_config</code>.</p>"},{"location":"how_to/add_doc_index/#the-_index-method","title":"The <code>_index()</code> method","text":"<p>When indexing documents, your implementation should behave in the following way:</p> <ul> <li>Every field in the Document is mapped to a column in the database</li> <li>This includes the <code>id</code> field, which is mapped to the primary key of the database (if your backend has such a concept)</li> <li>The configuration of that column can be found in <code>self._column_infos[field_name].config</code></li> <li>In DocArray &lt;=0.21, we used to store a serialized representation of every document. This is not needed anymore, as every row in your database table should fully represent a single indexed document.</li> </ul> <p>To handle nested documents, the public <code>index()</code> method already flattens every incoming document for you. This means that <code>_index()</code> already receives a flattened representation of the data, and you don't need to worry about that.</p> <p>Concretely, the <code>_index()</code> method takes as input a dictionary of column names to column data, flattened out.</p> <p>Note</p> <p>If you (or your backend) prefer to do bulk indexing on row-wise data, then you can use the <code>self._transpose_col_value_dict()</code> helper method. Inside of <code>_index()</code> you can use this to transform <code>column_to_data</code> into a row-wise view of the data.</p> <p>If your backend has native nesting capabilities: You can also ignore most of the above, and implement the public <code>index()</code> method directly. That way you have full control over whether the input data gets flattened or not.</p> <p>The <code>.id</code> field: Every Document has an <code>.id</code> field, which is intended to act as a unique identifier or primary key in your backend, if such a concept exists in your case. In your implementation you can assume that <code>.id</code>s are unique and non-empty. (Strictly speaking, this uniqueness property is not guaranteed, since a user could override the auto-generated <code>.id</code> field with a custom value. If your implementation encounters a duplicate <code>.id</code>, it is okay to fail and raise an Exception.)</p>"},{"location":"how_to/add_doc_index/#the-_filter_by_parent_id-method","title":"The <code>_filter_by_parent_id()</code> method","text":"<p>The default implementatin return <code>None</code>. You can choose to override this function with database specific filter API when needed.  This function should return a list of ids of subindex level documents given the id of root document.</p>"},{"location":"how_to/add_doc_index/#the-index_name-property","title":"The <code>index_name()</code> property","text":"<p>The <code>index_name</code> property is used in the initialization of subindices, and the default implementation is empty. This function should return the name of the index. And if the property of the index name in your backend is not <code>index_name</code>, you need to convert it as the first step in <code>__init__()</code>, like <code>index_name</code> is assigned to <code>work_dir</code> in <code>docarray/index/backends/hnswlib.py</code>.</p>"},{"location":"how_to/add_doc_index/#implement-a-query-builder-for-your-document-index","title":"Implement a Query Builder for your Document Index","text":"<p>Every Document Index exposes a Query Builder interface which the user can use to build composed, hybrid queries.</p> <p>For you as a backend integrator, there are three main things that are related to this:</p> <ul> <li>The <code>QueryBuilder</code> class that you need to implement</li> <li>The <code>execute_query()</code> method that you need to implement</li> <li>The <code>build_query()</code> method that just returns an instance of your <code>QueryBuilder</code>. You don't need to implement this yourself.</li> </ul> <p>Overall, this interface is very flexible, meaning that not a whole lot of structure is imposed on you. You can decide what happens in the <code>QueryBuilder</code> class, and how the query is executed in the <code>execute_query()</code> method. But there are a few things that you should stick to.</p>"},{"location":"how_to/add_doc_index/#implement-the-querybuilder-class","title":"Implement the <code>QueryBuilder</code> class","text":"<p>The QueryBuilder is what accumulates partial queries and builds them into a single query, ready to be executed.</p> <p>Your Query Builder has to be an inner class of your Document Index, its class name has to be <code>QueryBuilder</code>, and it has to inherit from the Base Query Builder:</p> <pre><code>class QueryBuilder(BaseDocIndex.QueryBuilder):\n...\n</code></pre> <p>The Query Builder exposes the following interface:</p> <ul> <li>The same query related methods as the <code>BaseDocIndex</code> class (e.g. <code>filter</code>, <code>find</code>, <code>text_search</code>, and their batched variants)</li> <li>The <code>build()</code> method</li> </ul> <p>Its goal is to enable an interface for composing complex queries, like this:</p> <pre><code>index = MyDocumentIndex[MyDoc]()\nq = index.build_query().find(...).filter(...).text_search(...).build()\nindex.execute_query(q)\n</code></pre> <p>How the individual calls to <code>find</code>, <code>filter</code>, and <code>text_search</code> are combined is up to your backend.</p>"},{"location":"how_to/add_doc_index/#implement-individual-query-methods","title":"Implement individual query methods","text":"<p>It is up to you how you implement the individual query methods, e.g. <code>find</code>, <code>filter</code>, and <code>text_search</code> of the query builder.</p> <p>However, there are a few common strategies that you could use: If your backend natively supports a query builder pattern, then you could wrap that with the DocumentIndex Query Builder interface; or you could set these methods to simply collect arguments passed to it and defer the actual query building to the <code>build()</code> method (the `HNSWLibIndex does this); or you could eagerly build intermediate queries at every call.</p> <p>No matter what you do, you should stick to one design principle: Every call to <code>find</code>, <code>filter</code>, <code>text_search</code> etc. should return a new instance of the Query Builder, with updated state.</p> <p>If your backend does not support all operations</p> <p>Most backends do not support compositions of all query operations, which is completely fine. If that is the case, you should handle that in the following way:</p> <ul> <li>If an operation is supported by the Document Index that you are implementing, but is not supported by the Query Builder, you should use the pre-defined <code>_raise_not_composable()</code> helper method to raise a <code>NotImplementedError</code>.</li> <li>If an operation is not supported by the Document Index that you are implementing, and is not supported by the Query Builder, you should use the pre-defined <code>_raise_not_supported()</code> helper method to raise a <code>NotImplementedError</code>.</li> <li>If an operation is supported by the Document Index that you are implementing, and is supported by the Query Builder, but is not supported in combination with a certain other operation, you should raise a <code>RuntimeError</code>. Depending on how your Query Builder is set up, you might want to do that either eagerly during the conflicting method call, or lazily inside of <code>.build()</code>.</li> </ul>"},{"location":"how_to/add_doc_index/#implement-the-build-method","title":"Implement the <code>build()</code> method","text":"<p>It is up to you how you implement the <code>build()</code> method, and this will depend on how you implemented the individual query methods in the section above.</p> <p>Depending on this, <code>build()</code> could wrap a similar method of an underlying native query builder; or it could combine the collected arguments and build an actual query; or it could even be a no-op. The important thing is that it returns a query object that can be immediately executed by the <code>execute_query()</code> method.</p> <p>What exactly this query object is, is up to you. It could be a string, a dictionary, a custom object, or anything else.</p>"},{"location":"how_to/add_doc_index/#implement-the-execute_query-method","title":"Implement the <code>execute_query()</code> method","text":"<p>The <code>execute_query()</code> method of your Document Index has to fulfill two requirements: 1. Be able to execute a query that was built by the <code>build()</code> method of the corresponding <code>QueryBuilder</code> class, and return the results. 2. Be able to execute a native query object of your backend, as a simple pass-through, and return the results. This is intended for users to be able to use the native query language of your backend and manually construct their own queries, if they want to.</p> <p>How you achieve this is up to you. If 1. and 2. condense down to the same thing because <code>build()</code> returns a native query is also up to you.</p> <p>The only thing to keep in mind is that any heavy lifting, combining of various inputs and queries, and potential validation should happen in the <code>build()</code> method, and not in <code>execute_query()</code>.</p>"},{"location":"how_to/multimodal_training_and_serving/","title":"Multimodal deep learning with DocArray","text":""},{"location":"how_to/multimodal_training_and_serving/#multimodal-deep-learning-with-docarray","title":"Multimodal deep learning with DocArray","text":"<p>DocArray is a library for representing, sending, and storing multimodal data that can be used for a variety of different use cases.</p> <p>Here we will focus on a workflow familiar to many ML engineers: Building and training a model, and then serving it to users.</p> <p>This document contains two parts:</p> <ol> <li>Representing: We will use DocArray to represent multimodal data while building and training a PyTorch model. We will see how DocArray can help to organize and group your modalities and tensors and make clear what methods to expect as inputs and return as outputs.</li> <li>Sending: We will take the model that we built and trained in part one, and serve it using FastAPI. We will see how DocArray narrows the gap between model development and model deployment, and how the same data models can be reused in both contexts. That part will be very short, but that's the point!</li> </ol> <p>So without further ado, let's dive into it!</p>"},{"location":"how_to/multimodal_training_and_serving/#1-representing-build-and-train-a-pytorch-model","title":"1. Representing: Build and train a PyTorch model","text":"<p>We will train a CLIP-like model on a dataset composed of text-image pairs. The goal is to obtain a model that can understand both text and images and project them into a common embedding space.</p> <p>We train the CLIP-like model on the Flickr8k dataset. To run this, you need to download and unzip the data into the same folder as your code.</p> <p>Note</p> <p>In this tutorial we do not aim to reproduce any CLIP results (our dataset is way too small anyway), but rather we want to show how DocArray data structures help researchers and practitioners write beautiful and  pythonic multimodal PyTorch code.</p> <pre><code>#!pip install \"docarray[torch,image]\"\n#!pip install torchvision\n#!pip install transformers\n#!pip install fastapi\n#!pip install pandas\n</code></pre> <pre><code>import itertools\nfrom typing import Callable, Dict, List, Optional\n</code></pre> <pre><code>import docarray\nimport torch\n</code></pre> <pre><code>import torchvision\nfrom torch import nn\nfrom transformers import AutoTokenizer, DistilBertModel\n</code></pre> <pre><code>DEVICE = \"cuda:0\"  # change to your favourite device\n</code></pre>"},{"location":"how_to/multimodal_training_and_serving/#create-documents-for-handling-multimodal-data","title":"Create documents for handling multimodal data","text":"<p>The first thing we want to achieve when using DocArray is to clearly model our data so that we never get confused about which tensors represent what.</p> <p>To do that we are using a concept that is at the core of DocArray: The document -- a collection of multimodal data. The <code>BaseDoc</code> class allows users to define their own (nested, multimodal) document schema to represent any kind of complex data.</p> <p>Let's start by defining a few documents to handle the different modalities that we will use during our training:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import TorchTensor, ImageUrl\n</code></pre> <p>Let's first create a document for our Text modality. It will contain a number of <code>Tokens</code>, which we also define:</p> <pre><code>from docarray.documents import TextDoc as BaseText\nclass Tokens(BaseDoc):\ninput_ids: TorchTensor[48]\nattention_mask: TorchTensor\n</code></pre> <pre><code>class Text(BaseText):\ntokens: Optional[Tokens] = None\n</code></pre> <p>Notice the <code>TorchTensor</code> type. It is a thin wrapper around <code>torch.Tensor</code> that can be used like any other Torch tensor,  but also enables additional features. One such feature is shape parametrization (<code>TorchTensor[48]</code>), which lets you hint and even enforce the desired shape of any tensor!</p> <p>To represent our image data, we use DocArray's <code>ImageDoc</code>:</p> <pre><code>from docarray.documents import ImageDoc\n</code></pre> <p>Under the hood, an <code>ImageDoc</code> looks something like this (with the only main difference that it can take tensors from any supported ML framework):</p> <pre><code>class ImageDoc(BaseDoc):\nurl: Optional[ImageUrl] = None\ntensor: Optional[TorchTesor] = None\nembedding: Optional[TorchTensor] = None\n</code></pre> <p>Actually, the <code>BaseText</code> above also already includes <code>tensor</code>, <code>url</code> and <code>embedding</code> fields, so we can use those on our <code>Text</code> document as well.</p> <p>The final document used for training here is the <code>PairTextImage</code>, which simply combines the Text and Image modalities:</p> <pre><code>class PairTextImage(BaseDoc):\ntext: TextDoc\nimage: ImageDoc\n</code></pre> <p>You then need to forward declare the following types. This will allow the objects to be properly pickled and unpickled.</p> <p>This will be unnecessary once this issue is resolved.</p> <pre><code>from docarray import DocVec\nDocVec[Tokens]\nDocVec[TextDoc]\nDocVec[ImageDoc]\nDocVec[PairTextImage]\n</code></pre>"},{"location":"how_to/multimodal_training_and_serving/#create-the-dataset","title":"Create the dataset","text":"<p>In this section we will create a multimodal pytorch dataset around the Flick8k dataset using DocArray.</p> <p>We will use DocArray's data loading functionality to load the data and use Torchvision and Transformers to preprocess the data before feeding it to our deep learning model:</p> <pre><code>from torch.utils.data import DataLoader, Dataset\n</code></pre> <pre><code>class VisionPreprocess:\ndef __init__(self):\nself.transform = torchvision.transforms.Compose(\n[\ntorchvision.transforms.ToTensor(),\ntorchvision.transforms.Resize(232),\ntorchvision.transforms.RandomCrop(224),\ntorchvision.transforms.Normalize(\nmean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n),\n]\n)\ndef __call__(self, image: Image) -&gt; None:\nimage.tensor = self.transform(image.url.load())\n</code></pre> <pre><code>class TextPreprocess:\ndef __init__(self):\nself.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\ndef __call__(self, text: Text) -&gt; None:\nassert isinstance(text, Text)\ntext.tokens = Tokens(\n**self.tokenizer(\ntext.text, padding=\"max_length\", truncation=True, max_length=48\n)\n)\n</code></pre> <p><code>VisionPreprocess</code> and <code>TextPreprocess</code> implement standard preprocessing steps for images and text, nothing special here.</p> <pre><code>import pandas as pd\ndef get_flickr8k_da(file: str = \"captions.txt\", N: Optional[int] = None):\ndf = pd.read_csv(file, nrows=N)\nda = DocList[PairTextImage](\nPairTextImage(text=Text(text=i.caption), image=Image(url=f\"Images/{i.image}\"))\nfor i in df.itertuples()\n)\nreturn da\n</code></pre> <p>In the <code>get_flickr8k_da</code> method we process the Flickr8k dataset into a <code>DocList</code>.</p> <p>Now let's instantiate this dataset using the <code>MultiModalDataset</code> class. The constructor takes in the <code>da</code> and a dictionary of preprocessing transformations:</p> <pre><code>da = get_flickr8k_da()\npreprocessing = {\"image\": VisionPreprocess(), \"text\": TextPreprocess()}\n</code></pre> <pre><code>from docarray.data import MultiModalDataset\ndataset = MultiModalDataset[PairTextImage](da=da, preprocessing=preprocessing)\nloader = DataLoader(\ndataset,\nbatch_size=128,\ncollate_fn=dataset.collate_fn,\nshuffle=True,\nnum_workers=4,\nmultiprocessing_context=\"fork\",\n)\n</code></pre>"},{"location":"how_to/multimodal_training_and_serving/#create-the-pytorch-model-that-works-on-docarray","title":"Create the Pytorch model that works on DocArray","text":"<p>In this section we will create two encoders, one per modality (Text and Image). These encoders are normal PyTorch <code>nn.Module</code>s. The only difference is that they operate on <code>DocList</code> rather that on torch.Tensor:</p> <pre><code>class TextEncoder(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ndef forward(self, texts: DocList[TextDoc]) -&gt; TorchTensor:\nlast_hidden_state = self.bert(\ninput_ids=texts.tokens.input_ids, attention_mask=texts.tokens.attention_mask\n).last_hidden_state\nreturn self._mean_pool(last_hidden_state, texts.tokens.attention_mask)\ndef _mean_pool(\nself, last_hidden_state: TorchTensor, attention_mask: TorchTensor\n) -&gt; TorchTensor:\nmasked_output = last_hidden_state * attention_mask.unsqueeze(-1)\nreturn masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n</code></pre> <p>The <code>TextEncoder</code> takes a <code>DocList</code> of <code>TextDoc</code>s as input, and returns an embedding <code>TorchTensor</code> as output. <code>DocList</code> can be seen as a list of <code>TextDoc</code> documents, and the encoder will treat it as one batch.</p> <pre><code>class VisionEncoder(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.backbone = torchvision.models.resnet18(pretrained=True)\nself.linear = nn.LazyLinear(out_features=768)\ndef forward(self, images: DocList[ImageDoc]) -&gt; TorchTensor:\nx = self.backbone(images.tensor)\nreturn self.linear(x)\n</code></pre> <p>Similarly, the <code>VisionEncoder</code> also takes a <code>DocList</code> of <code>ImageDoc</code>s as input, and returns an embedding <code>TorchTensor</code> as output. However, it operates on the <code>tensor</code> attribute of each document.</p> <p>Now we can instantiate our encoders:</p> <pre><code>vision_encoder = VisionEncoder().to(DEVICE)\ntext_encoder = TextEncoder().to(DEVICE)\n</code></pre> <p>As you can see, DocArray helps us clearly convey what data is expected as input and output for each method, all through Python type hints.</p>"},{"location":"how_to/multimodal_training_and_serving/#train-the-model-in-a-contrastive-way-between-text-and-image-clip","title":"Train the model in a contrastive way between Text and Image (CLIP)","text":"<p>Now that we have defined our dataloader and our models, we can train the two encoders is a contrastive way. The goal is to match the representation of the text and the image for each pair in the dataset.</p> <pre><code>optim = torch.optim.Adam(\nitertools.chain(vision_encoder.parameters(), text_encoder.parameters()), lr=3e-4\n)\n</code></pre> <pre><code>def cosine_sim(x_mat: TorchTensor, y_mat: TorchTensor) -&gt; TorchTensor:\na_n, b_n = x_mat.norm(dim=1)[:, None], y_mat.norm(dim=1)[:, None]\na_norm = x_mat / torch.clamp(a_n, min=1e-7)\nb_norm = y_mat / torch.clamp(b_n, min=1e-7)\nreturn torch.mm(a_norm, b_norm.transpose(0, 1)).squeeze()\n</code></pre> <pre><code>def clip_loss(image: DocList[Image], text: DocList[Text]) -&gt; TorchTensor:\nsims = cosine_sim(image.embedding, text.embedding)\nreturn torch.norm(sims - torch.eye(sims.shape[0], device=DEVICE))\n</code></pre> <p>In the type hints of <code>cosine_sim</code> and <code>clip_loss</code> you can again notice that we can treat a <code>TorchTensor</code> like any other <code>torch.Tensor</code>, and how we can make explicit what kind of data and data modalities the different functions expect.</p> <pre><code>num_epoch = 1  # here you should do more epochs to really learn something\n</code></pre> <p>One thing to notice here is that our dataloader does not return a <code>torch.Tensor</code> but a <code>DocList[PairTextImage]</code>, which is exactly what our model can operate on.</p> <p>So let's write a training loop and train our encoders:</p> <pre><code>from tqdm import tqdm\nwith torch.autocast(device_type=\"cuda\", dtype=torch.float16):\nfor epoch in range(num_epoch):\nfor i, batch in tqdm(\nenumerate(loader), total=len(loader), desc=f\"Epoch {epoch}\"\n):\nbatch.to(DEVICE)  # DocList can be moved to device\noptim.zero_grad()\n# FORWARD PASS:\nbatch.image.embedding = vision_encoder(batch.image)\nbatch.text.embedding = text_encoder(batch.text)\nloss = clip_loss(batch.image, batch.text)\nif i % 30 == 0:\nprint(f\"{i+epoch} steps , loss : {loss}\")\nloss.backward()\noptim.step()\n</code></pre> <p>Here we see how we can immediately group the output of each encoder with the document (and modality) it belong to.</p> <p>And with all that, we've successfully trained a CLIP-like model without ever getting confused about the meaning of any tensors!</p>"},{"location":"how_to/multimodal_training_and_serving/#2-sending-serve-the-model-using-fastapi","title":"2. Sending: Serve the model using FastAPI","text":"<p>Now that we have a trained CLIP model, let's see how we can serve this model with a REST API by reusing most of the code above.</p> <p>Let's use FastAPI for that!</p> <p>FastAPI is powerful because it allows you to define your Rest API data schema in pure Python. And DocArray is fully compatible with FastAPI and Pydantic, which means that as long as you have a function that takes a document as input,  FastAPI will be able to automatically translate it into a fully fledged API with documentation, OpenAPI specification and more:</p> <pre><code>from fastapi import FastAPI\nfrom docarray.base_doc import DocumentResponse\n</code></pre> <pre><code>app = FastAPI()\n</code></pre> <pre><code>vision_encoder = vision_encoder.eval()\ntext_encoder = text_encoder.eval()\n</code></pre> <p>Now all we need to do is to tell FastAPI what methods it should use to serve the model:</p> <pre><code>text_preprocess = TextPreprocess()\n</code></pre> <pre><code>@app.post(\"/embed_text/\", response_model=Text, response_class=DocumentResponse)\nasync def embed_text(doc: Text) -&gt; Text:\nwith torch.autocast(device_type=\"cuda\", dtype=torch.float16):\nwith torch.inference_mode():\ntext_preprocess(doc)\nda = DocList[Text]([doc], tensor_type=TorchTensor).to_doc_vec()\nda.to(DEVICE)\ndoc.embedding = text_encoder(da)[0].to('cpu')\nreturn doc\n</code></pre> <p>You can see that our earlier definition of the <code>Text</code> document now doubles as the API schema for the <code>/embed_text</code> endpoint.</p> <p>With this running, we can query our model over the network:</p> <pre><code>from httpx import AsyncClient\n</code></pre> <pre><code>text_input = Text(text='a picture of a rainbow')\n</code></pre> <pre><code>async with AsyncClient(\napp=app,\nbase_url=\"http://test\",\n) as ac:\nresponse = await ac.post(\"/embed_text/\", data=text_input.json())\n</code></pre> <pre><code>doc_resp = Text.parse_raw(response.content.decode())\n</code></pre> <pre><code>doc_resp.embedding.shape\n</code></pre> <p>And we're done! You have trained and served a multimodal ML model, with zero headaches and a lot of DocArray!</p>"},{"location":"how_to/optimize_performance_with_id_generation/","title":"Optimize performance","text":""},{"location":"how_to/optimize_performance_with_id_generation/#optimize-performance","title":"Optimize performance","text":""},{"location":"how_to/optimize_performance_with_id_generation/#basedocs-id","title":"<code>BaseDoc</code>'s <code>id</code>","text":"<p>DocArray's <code>BaseDoc</code> has an optional <code>id</code> field, which defaults to <code>ID(os.urandom(16).hex())</code>. This takes quite some time. If you don't rely on the <code>id</code> anywhere, you can instead set the default to <code>None</code>. This increases the performance by a factor of approximately 1.4:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ID\nclass MyDoc(BaseDoc):\nid: ID = None\ntitle: str\n</code></pre> <p>Since <code>BaseDoc.id</code> is optional, you could also set the value to <code>None</code>, but this turns out to be a bit less efficient than the option above, and increases performance by a factor of approximately 1.2:</p> <pre><code>class MyDoc2(BaseDoc):\ntitle: str\ndoc = MyDoc2(id=None, title='bye')\n</code></pre>"},{"location":"user_guide/intro/","title":"Introduction","text":""},{"location":"user_guide/intro/#introduction","title":"Introduction","text":"<p>This user guide shows you how to use <code>DocArray</code> with most of its features.</p> <p>There are three main sections:</p> <ul> <li>Representing data: This section will show you how to represent your data. This is a great starting point if you want to better organize the data in your ML models, or if you are looking for a \"Pydantic for ML\".</li> <li>Sending data: This section will show you how to send your data. This is a great starting point if you want to serve your ML model, for example through FastAPI.</li> <li>Storing data: This section will show you how to store your data. This is a great starting point if you are looking for an \"ORM for vector databases\".</li> </ul> <p>You should start by reading the Representing data section, and then the Sending data and Storing data sections can be read in any order.</p> <p>You will first need to install <code>DocArray</code> in your Python environment. </p>"},{"location":"user_guide/intro/#install-docarray","title":"Install DocArray","text":"<p>To install <code>DocArray</code>, you can use the following command:</p> <pre><code>pip install \"docarray[full]\"\n</code></pre> <p>This will install the main dependencies of <code>DocArray</code> and will work with all the supported data modalities.</p> <p>Note</p> <p>To install a very light version of <code>DocArray</code> with only the core dependencies, you can use the following command: </p><pre><code>pip install \"docarray\"\n</code></pre> <p>If you want to use <code>protobuf</code> and <code>DocArray</code>, you can run:</p> <pre><code>pip install \"docarray[proto]\"\n</code></pre> <p>Depending on your usage you might want to use <code>DocArray</code> with only a couple of specific modalities and their dependencies.  For instance, if you only want to work with images, you can install <code>DocArray</code> using the following command:</p> <pre><code>pip install \"docarray[image]\"\n</code></pre> <p>...or with images and audio:</p> <pre><code>pip install \"docarray[image, audio]\"\n</code></pre>"},{"location":"user_guide/representing/array/","title":"Array of documents","text":""},{"location":"user_guide/representing/array/#array-of-documents","title":"Array of documents","text":"<p>DocArray allows users to represent and manipulate multimodal data to build AI applications such as neural search and generative AI. </p> <p>As you have seen in the previous section, the fundamental building block of DocArray is the <code>BaseDoc</code> class which represents a single document, a single datapoint.</p> <p>However, in machine learning we often need to work with an array of documents, and an array of data points.</p> <p>This section introduces the concept of <code>AnyDocArray</code> which is an (abstract) collection of <code>BaseDoc</code>. This name of this library -- <code>DocArray</code> -- is derived from this concept and is short for <code>DocumentArray</code>.</p>"},{"location":"user_guide/representing/array/#anydocarray","title":"AnyDocArray","text":"<p><code>AnyDocArray</code> is an abstract class that represents an array of <code>BaseDoc</code>s which is not meant to be used directly, but to be subclassed.</p> <p>We provide two concrete implementations of <code>AnyDocArray</code> :</p> <ul> <li><code>DocList</code> which is a Python list of <code>BaseDoc</code>s</li> <li><code>DocVec</code> which is a column based representation of <code>BaseDoc</code>s</li> </ul> <p>We will go into the difference between <code>DocList</code> and <code>DocVec</code> in the next section, but let's first focus on what they have in common.</p> <p>The spirit of <code>AnyDocArray</code>s is to extend the <code>BaseDoc</code> and <code>BaseModel</code> concepts to the array level in a seamless way.</p>"},{"location":"user_guide/representing/array/#example","title":"Example","text":"<p>Before going into detail let's look at a code example.</p> <p>Note</p> <p><code>DocList</code> and <code>DocVec</code> are both <code>AnyDocArray</code>s. The following section will use <code>DocList</code> as an example, but the same  applies to <code>DocVec</code>.</p> <p>First you need to create a <code>Doc</code> class, our data schema. Let's say you want to represent a banner with an image, a title and a description:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import ImageUrl\nclass BannerDoc(BaseDoc):\nimage: ImageUrl\ntitle: str\ndescription: str\n</code></pre> <p>Let's instantiate several <code>BannerDoc</code>s:</p> <pre><code>banner1 = BannerDoc(\nimage='https://example.com/image1.png',\ntitle='Hello World',\ndescription='This is a banner',\n)\nbanner2 = BannerDoc(\nimage='https://example.com/image2.png',\ntitle='Bye Bye World',\ndescription='This is (distopic) banner',\n)\n</code></pre> <p>You can now collect them into a <code>DocList</code> of <code>BannerDoc</code>s:</p> <pre><code>docs = DocList[BannerDoc]([banner1, banner2])\ndocs.summary()\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DocList Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                 \u2502\n\u2502   Type     DocList[BannerDoc]   \u2502\n\u2502   Length   2                    \u2502\n\u2502                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500 Document Schema \u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                          \u2502\n\u2502   BannerDoc              \u2502\n\u2502   \u251c\u2500\u2500 image: ImageUrl    \u2502\n\u2502   \u251c\u2500\u2500 title: str         \u2502\n\u2502   \u2514\u2500\u2500 description: str   \u2502\n\u2502                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p><code>docs</code> here is an array-like collection of <code>BannerDoc</code>.</p> <p>You can access documents inside it with the usual Python array API:</p> <pre><code>print(docs[0])\n</code></pre> <pre><code>BannerDoc(image='https://example.com/image1.png', title='Hello World', description='This is a banner')\n</code></pre> <p>or iterate over it:</p> <pre><code>for doc in docs:\nprint(doc)\n</code></pre> <pre><code>BannerDoc(image='https://example.com/image1.png', title='Hello World', description='This is a banner')\nBannerDoc(image='https://example.com/image2.png', title='Bye Bye World', description='This is (distopic) banner')\n</code></pre> <p>Note</p> <p>The syntax <code>DocList[BannerDoc]</code> might surprise you in this context. It is actually at the heart of DocArray, but we'll come back to it later and continue with this example for now.</p> <p>As we said earlier, <code>DocList</code> (or more generally <code>AnyDocArray</code>) extends the <code>BaseDoc</code> API at the array level.</p> <p>What this means concretely is you can access your data at the Array level in just the same way you would access your data at the  document level.</p> <p>Let's see what that looks like:</p> <p>At the document level:</p> <pre><code>print(banner1.image)\n</code></pre> <pre><code>https://example.com/image1.png'\n</code></pre> <p>At the Array level:</p> <pre><code>print(docs.image)\n</code></pre> <pre><code>['https://example.com/image1.png', 'https://example.com/image2.png']\n</code></pre> <p>Important</p> <p>All the attributes of <code>BannerDoc</code> are accessible at the Array level.</p> <p>Warning</p> <p>Whereas this is true at runtime, static type analyzers like Mypy or IDEs like PyCharm will not be be aware of it. This limitation is known and will be fixed in the future by the introduction of plugins for Mypy, PyCharm and VSCode. </p> <p>This even works when you have a nested <code>BaseDoc</code>:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import ImageUrl\nclass BannerDoc(BaseDoc):\nimage: ImageUrl\ntitle: str\ndescription: str\nclass PageDoc(BaseDoc):\nbanner: BannerDoc\ncontent: str\npage1 = PageDoc(\nbanner=BannerDoc(\nimage='https://example.com/image1.png',\ntitle='Hello World',\ndescription='This is a banner',\n),\ncontent='Hello world is the most used example in programming, but do you know that? ...',\n)\npage2 = PageDoc(\nbanner=BannerDoc(\nimage='https://example.com/image2.png',\ntitle='Bye Bye World',\ndescription='This is (distopic) banner',\n),\ncontent='What if the most used example in programming was Bye Bye World, would programming be that much fun? ...',\n)\ndocs = DocList[PageDoc]([page1, page2])\ndocs.summary()\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DocList Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                               \u2502\n\u2502   Type     DocList[PageDoc]   \u2502\n\u2502   Length   2                  \u2502\n\u2502                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500 Document Schema \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                              \u2502\n\u2502   PageDoc                    \u2502\n\u2502   \u251c\u2500\u2500 banner: BannerDoc      \u2502\n\u2502   \u2502   \u251c\u2500\u2500 image: ImageUrl    \u2502\n\u2502   \u2502   \u251c\u2500\u2500 title: str         \u2502\n\u2502   \u2502   \u2514\u2500\u2500 description: str   \u2502\n\u2502   \u2514\u2500\u2500 content: str           \u2502\n\u2502                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>print(docs.banner)\n</code></pre> <pre><code>&lt;DocList[BannerDoc] (length=2)&gt;\n</code></pre> <p>Yes, <code>docs.banner</code> returns a nested <code>DocList</code> of <code>BannerDoc</code>s! </p> <p>You can even access the attributes of the nested <code>BaseDoc</code> at the Array level:</p> <pre><code>print(docs.banner.image)\n</code></pre> <pre><code>['https://example.com/image1.png', 'https://example.com/image2.png']\n</code></pre> <p>This is just the same way that you would do it with BaseDoc:</p> <pre><code>print(page1.banner.image)\n</code></pre> <pre><code>'https://example.com/image1.png'\n</code></pre>"},{"location":"user_guide/representing/array/#doclistdoctype-syntax","title":"<code>DocList[DocType]</code> syntax","text":"<p>As you have seen in the previous section, <code>AnyDocArray</code> will expose the same attributes as the <code>BaseDoc</code>s it contains.</p> <p>But this concept only works if (and only if) all of the <code>BaseDoc</code>s in the <code>AnyDocArray</code> have the same schema.</p> <p>If one of your <code>BaseDoc</code>s has an attribute that the others don't, you will get an error if you try to access it at the Array level.</p> <p>Note</p> <p>To extend your schema to the Array level, <code>AnyDocArray</code> needs to contain a homogenous Document.</p> <p>This is where the custom syntax <code>DocList[DocType]</code> comes into play.</p> <p>Note</p> <p><code>DocList[DocType]</code> creates a custom <code>DocList</code> that can only contain <code>DocType</code> Documents.</p> <p>This syntax is inspired by more statically typed languages, and even though it might offend Python purists, we believe that it is a good user experience to think of an Array of <code>BaseDoc</code>s rather than just an array of heterogeneous <code>BaseDoc</code>s.</p> <p>That said, <code>AnyDocArray</code> can also be used to create a heterogeneous <code>AnyDocArray</code>:</p> <p>Note</p> <p>The default <code>DocList</code> can be used to create a heterogeneous list of <code>BaseDoc</code>.</p> <p>Warning</p> <p><code>DocVec</code> cannot store heterogeneous <code>BaseDoc</code> and always needs the <code>DocVec[DocType]</code> syntax.</p> <p>The usage of a heterogeneous <code>DocList</code> is similar to a normal Python list but still offers DocArray functionality like serialization and sending over the wire. However, it won't be able to extend the API of your custom schema to the Array level.</p> <p>Here is how you can instantiate a heterogeneous <code>DocList</code>:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import ImageUrl, AudioUrl\nclass ImageDoc(BaseDoc):\nurl: ImageUrl\nclass AudioDoc(BaseDoc):\nurl: AudioUrl\ndocs = DocList(\n[\nImageDoc(url='https://example.com/image1.png'),\nAudioDoc(url='https://example.com/audio1.mp3'),\n]\n)\n</code></pre> <p>But this is not possible:</p> <pre><code>try:\ndocs = DocList[ImageDoc](\n[\nImageDoc(url='https://example.com/image1.png'),\nAudioDoc(url='https://example.com/audio1.mp3'),\n]\n)\nexcept ValueError as e:\nprint(e)\n</code></pre> <pre><code>ValueError: AudioDoc(\n    id='e286b10f58533f48a0928460f0206441',\n    url=AudioUrl('https://example.com/audio1.mp3', host_type='domain')\n) is not a &lt;class '__main__.ImageDoc'&gt;\n</code></pre>"},{"location":"user_guide/representing/array/#doclist-vs-docvec","title":"<code>DocList</code> vs <code>DocVec</code>","text":"<p><code>DocList</code> and <code>DocVec</code> are both <code>AnyDocArray</code> but they have different use cases, and differ in how they store data in memory.</p> <p>They share almost everything that has been said in the previous sections, but they have some conceptual differences.</p> <p><code>DocList</code> is based on Python Lists. You can append, extend, insert, pop, and so on. In DocList, data is individually owned by each <code>BaseDoc</code> collect just different Document references. Use <code>DocList</code> when you want to be able to rearrange or re-rank your data. One flaw of <code>DocList</code> is that none of the data is contiguous in memory, so you cannot  leverage functions that require contiguous data without first copying the data in a continuous array.</p> <p><code>DocVec</code> is a columnar data structure. <code>DocVec</code> is always an array of homogeneous Documents. The idea is that every attribute of the <code>BaseDoc</code> will be stored in a contiguous array: a column.</p> <p>This means that when you access the attribute of a <code>BaseDoc</code> at the Array level, we don't collect the data under the hood  from all the documents (like <code>DocList</code>) before giving it back to you. We just return the column that is stored in memory.</p> <p>This really matters when you need to handle multimodal data that you will feed into an algorithm that requires contiguous data, like matrix multiplication which is at the heart of Machine Learning, especially in Deep Learning.</p> <p>Let's take an example to illustrate the difference:</p> <p>Let's say you want to work with an Image:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import NdArray\nclass ImageDoc(BaseDoc):\nimage: NdArray[\n3, 224, 224\n] = None  # [3, 224, 224] this just mean we know in advance the shape of the tensor\n</code></pre> <p>And that you have a function that takes a contiguous array of images as input (like a deep learning model):</p> <pre><code>def predict(image: NdArray['batch_size', 3, 224, 224]):\n...\n</code></pre> <p>Let's create a <code>DocList</code> of <code>ImageDoc</code>s and pass it to the function:</p> <pre><code>from docarray import DocList\nimport numpy as np\ndocs = DocList[ImageDoc](\n[ImageDoc(image=np.random.rand(3, 224, 224)) for _ in range(10)]\n)\npredict(np.stack(docs.image))\n...\npredict(np.stack(docs.image))\n</code></pre> <p>When you call <code>docs.image</code>, <code>DocList</code> loops over the ten documents and collects the image attribute of each document in a list. It is similar to doing:</p> <pre><code>images = []\nfor doc in docs:\nimages.append(doc.image)\n</code></pre> <p>this means that if you call <code>docs.image</code> multiple times, under the hood you will collect the image from each document and stack them several times. This is not optimal.</p> <p>Let's see how it will work with <code>DocVec</code>:</p> <pre><code>from docarray import DocVec\nimport numpy as np\ndocs = DocVec[ImageDoc](\n[ImageDoc(image=np.random.rand(3, 224, 224)) for _ in range(10)]\n)\npredict(docs.image)\n...\npredict(docs.image)\n</code></pre> <p>The first difference is that you don't need to call <code>np.stack</code> on <code>docs.image</code> because <code>docs.image</code> is already a contiguous array. The second difference is that you just get the column and don't need to create it at each call.</p> <p>One of the other main differences between both of them is how you can access documents inside them.</p> <p>If you access a document inside a <code>DocList</code> you will get a <code>BaseDoc</code> instance, i.e. a document.</p> <p>If you access a document inside a <code>DocVec</code> you will get a document view. A document view is a view of the columnar data structure which looks and behaves like a <code>BaseDoc</code> instance. It is a <code>BaseDoc</code> instance but with a different way to access the data.</p> <p>When you make a change at the view level it will be reflected at the DocVec level:</p> <pre><code>from docarray import DocVec\ndocs = DocVec[ImageDoc](\n[ImageDoc(image=np.random.rand(3, 224, 224)) for _ in range(10)]\n)\nmy_doc = docs[0]\nassert my_doc.is_view()  # True\n</code></pre> <p>whereas with DocList:</p> <pre><code>docs = DocList[ImageDoc](\n[ImageDoc(image=np.random.rand(3, 224, 224)) for _ in range(10)]\n)\nmy_doc = docs[0]\nassert not my_doc.is_view()  # False\n</code></pre> <p>Note</p> <p>To summarize: you should use <code>DocVec</code> when you need to work with contiguous data, and you should use <code>DocList</code> when you need to rearrange or extend your data.</p>"},{"location":"user_guide/representing/array/#dealing-with-optional-fields","title":"Dealing with Optional fields","text":"<p>Both <code>DocList</code> and <code>DocVec</code> support nested optional fields but they behave slightly differently.</p> <p>Nested optional field</p> <p>By a \"nested optional field\" we mean a document that is contained within another document, and declared as <code>Optional</code>:</p> <pre><code>from typing import Optional\nfrom docarray import BaseDoc\nclass MyDoc(BaseDoc):\nnested_doc: Optional[BaseDoc] = None\n</code></pre> <p>Using nested optional fields differs slightly between DocList and DocVes, so watch out. But in a nutshell:</p> <p>When accessing a nested BaseDoc:</p> <ul> <li>DocList will return a list of documents if the field is optional and a DocList if the field is not optional</li> <li>DocVec will return a DocVec if all documents are there, or None if all docs are None. No mix of docs and None allowed!</li> <li>DocVec will behave the same for a tensor field instead of a BaseDoc</li> </ul>"},{"location":"user_guide/representing/array/#doclist-with-nested-optional-field","title":"DocList with nested optional Field","text":"<p>Let's take an example to illustrate the exact behavior:</p> <pre><code>from typing import Optional\nfrom docarray.typing import NdArray\nimport numpy as np\nclass ImageDoc(BaseDoc):\ntensor: NdArray\nclass ArticleDoc(BaseDoc):\nimage: Optional[ImageDoc] = None\ntitle: str\n</code></pre> <p>In this example <code>ArticleDoc</code> has an optional field <code>image</code> which is an <code>ImageDoc</code>. This means that this field can either be None or be a <code>ImageDoc</code> instance.</p> <p>Remember that for both DocList and DocVec calling <code>docs.image</code> will return a list-like object of all the images of the documents.</p> <p>For DocList this call will iterate over all the documents and collect the image attribute of each document in a sequence, and for DocVec it will return the already stacked column of the <code>.image</code> attribute.</p> <p>For DocList it will return a list of <code>Optional[ImageDoc]</code> instead of a <code>DocList[ImageDoc]</code>, this is because the list can contain None and DocList can't.</p> <pre><code>from docarray import DocList\ndocs = DocList[ArticleDoc](\n[\nArticleDoc(image=ImageDoc(tensor=np.ones((3, 224, 224))), title=\"Hello\"),\nArticleDoc(image=None, title=\"World\"),\n]\n)\nassert docs.image == [ImageDoc(tensor=np.ones((3, 224, 224))), None]\n</code></pre>"},{"location":"user_guide/representing/array/#docvec-with-nested-optional-field","title":"DocVec with nested optional Field","text":"<p>For DocVec it is a bit different. Indeed, a DocVec stores the data for each filed as contiguous column.  This means that DocVec can create a column in only two cases: either all the data for a field is None or all the data is not None.</p> <p>For the first case the whole column will just be None. In the second case the column will be a <code>DocList[ImageDoc]</code></p> <pre><code>from docarray import DocVec\ndocs = DocVec[ArticleDoc](\n[\nArticleDoc(image=ImageDoc(tensor=np.zeros((3, 224, 224))), title=\"Hello\")\nfor _ in range(10)\n]\n)\nassert (docs.image.tensor == np.zeros((3, 224, 224))).all()\n</code></pre> <p>Or it can be None:</p> <pre><code>docs = DocVec[ArticleDoc]([ArticleDoc(title=\"Hello\") for _ in range(10)])\nassert docs.image is None\n</code></pre> <p>But if you try a mix you will get an error:</p> <pre><code>try:\ndocs = DocVec[ArticleDoc](\n[\nArticleDoc(image=ImageDoc(tensor=np.ones((3, 224, 224))), title=\"Hello\"),\nArticleDoc(image=None, title=\"World\"),\n]\n)\nexcept ValueError as e:\nprint(e)\n</code></pre> <pre><code>None is not a &lt;class '__main__.ImageDoc'&gt;\n</code></pre> <p>See also:</p> <ul> <li>First step of the representing section</li> <li>API Reference for the <code>DocList</code> class</li> <li>API Reference for the <code>DocVec</code> class</li> <li>The Storing section on how to store your data </li> <li>The Sending section on how to send your data</li> </ul>"},{"location":"user_guide/representing/first_step/","title":"Document","text":""},{"location":"user_guide/representing/first_step/#document","title":"Document","text":"<p>At the heart of <code>DocArray</code> lies the concept of <code>BaseDoc</code>.</p> <p>A BaseDoc is very similar to a Pydantic <code>BaseModel</code> -- in fact it is a specialized Pydantic <code>BaseModel</code>. It allows you to define custom <code>Document</code> schemas (or <code>Model</code>s in the Pydantic world) to represent your data.</p> <p>Note</p> <p>Naming convention: When we refer to a <code>BaseDoc</code>, we refer to a class that inherits from BaseDoc.  When we refer to a <code>Document</code> we refer to an instance of a <code>BaseDoc</code> class.</p>"},{"location":"user_guide/representing/first_step/#basic-doc-usage","title":"Basic <code>Doc</code> usage","text":"<p>Before going into detail about what we can do with BaseDoc and how to use it, let's see what it looks like in practice.</p> <p>The following Python code defines a <code>BannerDoc</code> class that can be used to represent the data of a website banner:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageUrl\nclass BannerDoc(BaseDoc):\nimage_url: ImageUrl\ntitle: str\ndescription: str\n</code></pre> <p>You can then instantiate a <code>BannerDoc</code> object and access its attributes:</p> <pre><code>banner = BannerDoc(\nimage_url='https://example.com/image.png',\ntitle='Hello World',\ndescription='This is a banner',\n)\nassert banner.image_url == 'https://example.com/image.png'\nassert banner.title == 'Hello World'\nassert banner.description == 'This is a banner'\n</code></pre>"},{"location":"user_guide/representing/first_step/#basedoc-is-a-pydantic-basemodel","title":"<code>BaseDoc</code> is a Pydantic <code>BaseModel</code>","text":"<p>The BaseDoc class inherits from Pydantic BaseModel. This means you can use all the features of <code>BaseModel</code> in your <code>Doc</code> class. <code>BaseDoc</code>:</p> <ul> <li>Will perform data validation: <code>BaseDoc</code> will check that the data you pass to it is valid. If not, it will raise an  error. Data being \"valid\" is actually defined by the type used in the type hint itself, but we will come back to this concept later.</li> <li>Can be configured using a nested <code>Config</code> class, see Pydantic documentation for more detail on what kind of config Pydantic offers.</li> <li>Can be used as a drop-in replacement for <code>BaseModel</code> in your code and is compatible with tools that use Pydantic, like FastAPI.</li> </ul>"},{"location":"user_guide/representing/first_step/#representing-multimodal-and-nested-data","title":"Representing multimodal and nested data","text":"<p>Let's say you want to represent a YouTube video in your application, perhaps to build a search system for YouTube videos. A YouTube video is not only composed of a video, but also has a title, description, thumbnail (and more, but let's keep it simple).</p> <p>All of these elements are from different <code>modalities</code>: the title and description are text, the thumbnail is an image, and the video itself is, well, a video.</p> <p>DocArray lets you represent all of this multimodal data in a single object. </p> <p>Let's first create a <code>BaseDoc</code> for each of the elements that compose the YouTube video.</p> <p>First for the thumbnail image:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import ImageUrl, ImageBytes\nclass ImageDoc(BaseDoc):\nurl: ImageUrl\nbytes: ImageBytes = (\nNone  # bytes are not always loaded in memory, so we make it optional\n)\n</code></pre> <p>Then for the video itself:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import VideoUrl, VideoBytes\nclass VideoDoc(BaseDoc):\nurl: VideoUrl\nbytes: VideoBytes = (\nNone  # bytes are not always loaded in memory, so we make it optional\n)\n</code></pre> <p>Then for the title and description (which are text) we'll just use a <code>str</code> type.</p> <p>All the elements that compose a YouTube video are ready:</p> <pre><code>from docarray import BaseDoc\nclass YouTubeVideoDoc(BaseDoc):\ntitle: str\ndescription: str\nthumbnail: ImageDoc\nvideo: VideoDoc\n</code></pre> <p>We now have <code>YouTubeVideoDoc</code> which is a pythonic representation of a YouTube video. </p> <p>This representation can be used to send or store data. You can even use it directly to train a machine learning Pytorch model on this representation. </p> <p>Note</p> <p>You see here that <code>ImageDoc</code> and <code>VideoDoc</code> are also BaseDoc, and they are later used inside another BaseDoc`. This is what we call nested data representation. </p> <p>BaseDoc can be nested to represent any kind of data hierarchy.</p>"},{"location":"user_guide/representing/first_step/#setting-a-pydantic-config-class","title":"Setting a Pydantic <code>Config</code> class","text":"<p>Documents support setting a custom <code>configuration</code> like any other Pydantic <code>BaseModel</code>.</p> <p>Here is an example to extend the Config of a Document dependong on which version of Pydantic you are using.</p> Pydantic v1Pydantic v2 <pre><code>from docarray import BaseDoc\nclass MyDoc(BaseDoc):\nclass Config(BaseDoc.Config):\narbitrary_types_allowed = True  # just an example setting\n</code></pre> <pre><code>from docarray import BaseDoc\nclass MyDoc(BaseDoc):\nmodel_config = BaseDoc.ConfigDocArray.ConfigDict(\narbitrary_types_allowed=True\n)  # just an example setting\n</code></pre> <p>See also:</p> <ul> <li>The next part of the representing section</li> <li>API reference for the BaseDoc class</li> <li>The Storing section on how to store your data </li> <li>The Sending section on how to send your data</li> </ul>"},{"location":"user_guide/sending/first_step/","title":"Introduction","text":""},{"location":"user_guide/sending/first_step/#introduction","title":"Introduction","text":"<p>In the representation section we saw how to use <code>BaseDoc</code>, <code>DocList</code> and <code>DocVec</code> to represent multimodal data. In this section we will see how to send such data over the wire.</p> <p>This section is divided into two parts:</p> <ul> <li>Serializing <code>BaseDoc</code>, <code>DocList</code> and <code>DocVec</code></li> <li>Using DocArray with a web framework to build a multimodal API</li> </ul>"},{"location":"user_guide/sending/serialization/","title":"Serialization","text":""},{"location":"user_guide/sending/serialization/#serialization","title":"Serialization","text":"<p>DocArray offers various serialization options for all of its main data classes: BaseDoc, DocList, and DocVec</p>"},{"location":"user_guide/sending/serialization/#basedoc","title":"BaseDoc","text":"<p>You need to serialize a BaseDoc before you can store or send it.</p> <p>Note</p> <p>BaseDoc supports serialization to <code>protobuf</code> and <code>json</code> formats.</p>"},{"location":"user_guide/sending/serialization/#json","title":"JSON","text":"<ul> <li><code>json</code> serializes a <code>BaseDoc</code> to a JSON string.</li> <li><code>parse_raw</code> deserializes a <code>BaseDoc</code> from a JSON string.</li> </ul> <pre><code>from typing import List\nfrom docarray import BaseDoc\nclass MyDoc(BaseDoc):\ntext: str\ntags: List[str]\ndoc = MyDoc(text='hello world', tags=['hello', 'world'])\njson_str = doc.json()\nnew_doc = MyDoc.parse_raw(json_str)\nassert doc == new_doc  # True\n</code></pre>"},{"location":"user_guide/sending/serialization/#protobuf","title":"Protobuf","text":"<ul> <li><code>to_protobuf</code> serializes a <code>BaseDoc</code> to a <code>protobuf</code> message object.</li> <li><code>from_protobuf</code> deserializes a <code>BaseDoc</code> from a <code>protobuf</code> object.</li> </ul> <pre><code>from typing import List\nfrom docarray import BaseDoc\nclass MyDoc(BaseDoc):\ntext: str\ntags: List[str]\ndoc = MyDoc(text='hello world', tags=['hello', 'world'])\nproto_message = doc.to_protobuf()\nnew_doc = MyDoc.from_protobuf(proto_message)\nassert doc == new_doc  # True\n</code></pre>"},{"location":"user_guide/sending/serialization/#doclist","title":"DocList","text":"<p>When sending or storing <code>DocList</code>, you need to use serialization. <code>DocList</code> supports multiple ways to serialize the data.</p>"},{"location":"user_guide/sending/serialization/#json_1","title":"JSON","text":"<ul> <li><code>to_json()</code> serializes a <code>DocList</code> to JSON. It returns the binary representation of the JSON object. </li> <li><code>from_json()</code> deserializes a <code>DocList</code> from JSON. It can load from either a <code>str</code> or <code>binary</code> representation of the JSON object.</li> </ul> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\nwith open('simple-dl.json', 'wb') as f:\njson_dl = dl.to_json()\nprint(json_dl)\nf.write(json_dl.encode())\nwith open('simple-dl.json', 'r') as f:\ndl_load_from_json = DocList[SimpleDoc].from_json(f.read())\nprint(dl_load_from_json)\n</code></pre> <pre><code>'[{\"id\":\"5540e72d407ae81abb2390e9249ed066\",\"text\":\"doc 0\"},{\"id\":\"fbe9f80d2fa03571e899a2887af1ac1b\",\"text\":\"doc 1\"}]'\n</code></pre>"},{"location":"user_guide/sending/serialization/#protobuf_1","title":"Protobuf","text":"<ul> <li><code>to_protobuf()</code> serializes a <code>DocList</code> to <code>protobuf</code>. It returns a <code>protobuf</code> object of <code>docarray_pb2.DocListProto</code> class.</li> <li><code>from_protobuf()</code> deserializes a <code>DocList</code> from <code>protobuf</code>. It accepts a <code>protobuf</code> message object to construct a <code>DocList</code>.</li> </ul> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\nproto_message_dl = dl.to_protobuf()\ndl_from_proto = DocList[SimpleDoc].from_protobuf(proto_message_dl)\nprint(type(proto_message_dl))\nprint(dl_from_proto)\n</code></pre>"},{"location":"user_guide/sending/serialization/#base64","title":"Base64","text":"<p>When transferring data over the network, use <code>Base64</code> format to serialize the <code>DocList</code>. Serializing a <code>DocList</code> in Base64 supports both the <code>pickle</code> and <code>protobuf</code> protocols. You can also choose different compression methods.</p> <ul> <li><code>to_base64()</code> serializes a <code>DocList</code> to Base64</li> <li><code>from_base64()</code> deserializes a <code>DocList</code> from Base64:</li> </ul> <p>You can multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\nbase64_repr_dl = dl.to_base64(compress=None, protocol='pickle')\ndl_from_base64 = DocList[SimpleDoc].from_base64(\nbase64_repr_dl, compress=None, protocol='pickle'\n)\n</code></pre>"},{"location":"user_guide/sending/serialization/#save-binary","title":"Save binary","text":"<p>These methods serialize and save your data:</p> <ul> <li><code>save_binary()</code> saves a <code>DocList</code> to a binary file.</li> <li><code>load_binary()</code> loads a <code>DocList</code> from a binary file.</li> </ul> <p>You can choose between multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\ndl.save_binary('simple-dl.pickle', compress=None, protocol='pickle')\ndl_from_binary = DocList[SimpleDoc].load_binary(\n'simple-dl.pickle', compress=None, protocol='pickle'\n)\n</code></pre> <p>In the above snippet, the <code>DocList</code> is stored as the file <code>simple-dl.pickle</code>.</p>"},{"location":"user_guide/sending/serialization/#bytes","title":"Bytes","text":"<p>These methods just serialize your data, without saving it to a file:</p> <ul> <li>to_bytes() saves a <code>DocList</code> to a byte object.</li> <li>from_bytes() loads a <code>DocList</code> from a byte object.  </li> </ul> <p>Note</p> <p>These methods are used under the hood by save_binary() and <code>load_binary()</code> to prepare/load/save to a binary file. You can also use them directly to work with byte files.</p> <p>Like working with binary files:</p> <ul> <li>You can use <code>protocol</code> to choose between <code>pickle</code> and <code>protobuf</code>. </li> <li>You can use multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</li> </ul> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\nbytes_dl = dl.to_bytes(protocol='pickle', compress=None)\ndl_from_bytes = DocList[SimpleDoc].from_bytes(\nbytes_dl, compress=None, protocol='pickle'\n)\n</code></pre>"},{"location":"user_guide/sending/serialization/#csv","title":"CSV","text":"<ul> <li><code>to_csv()</code> serializes a <code>DocList</code> to a CSV file.</li> <li><code>from_csv()</code> deserializes a <code>DocList</code> from a CSV file.</li> </ul> <p>Use the <code>dialect</code> parameter to choose the dialect of the CSV format:</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\ndl.to_csv('simple-dl.csv')\ndl_from_csv = DocList[SimpleDoc].from_csv('simple-dl.csv')\nprint(dl_from_csv)\n</code></pre>"},{"location":"user_guide/sending/serialization/#pandasdataframe","title":"Pandas.Dataframe","text":"<ul> <li><code>from_dataframe()</code> loads a <code>DocList</code> from a Pandas Dataframe.</li> <li><code>to_dataframe()</code> saves a <code>DocList</code> to a Pandas Dataframe.</li> </ul> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\ndf = dl.to_dataframe()\ndl_from_dataframe = DocList[SimpleDoc].from_dataframe(df)\nprint(dl_from_dataframe)\n</code></pre>"},{"location":"user_guide/sending/serialization/#docvec","title":"DocVec","text":"<p>For sending or storing <code>DocVec</code> it offers a very similar interface to that of <code>DocList</code>.</p> <p>Tensor type and (de)serialization</p> <p>You can deserialize any serialized DocVec to any tensor type (<code>NdArray</code>, <code>TorchTensor</code>, or <code>TensorFlowTensor</code>), by passing the <code>tensor_type=...</code> parameter to the appropriate deserialization method. This is analogous to the <code>tensor_type=...</code> parameter in the DocVec constructor.</p> <p>This means that you can choose at deserialization time if you are working with numpy, PyTorch, or TensorFlow tensors.</p> <p>If no <code>tensor_type</code> is passed, the default is <code>NdArray</code>.</p>"},{"location":"user_guide/sending/serialization/#json_2","title":"JSON","text":"<ul> <li><code>to_json()</code> serializes a <code>DocVec</code> to JSON. It returns the binary representation of the JSON object. </li> <li><code>from_json()</code> deserializes a <code>DocList</code> from JSON. It can load from either a <code>str</code> or <code>binary</code> representation of the JSON object.</li> </ul> <p>In contrast to DocList's JSON format, <code>DocVec.to_json()</code> outputs a column oriented JSON file:</p> <pre><code>import torch\nfrom docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor\nclass SimpleDoc(BaseDoc):\ntext: str\ntensor: TorchTensor\ndv = DocVec[SimpleDoc](\n[SimpleDoc(text=f'doc {i}', tensor=torch.rand(64)) for i in range(2)]\n)\nwith open('simple-dv.json', 'wb') as f:\njson_dv = dv.to_json()\nprint(json_dv)\nf.write(json_dv.encode())\nwith open('simple-dv.json', 'r') as f:\ndv_load_from_json = DocVec[SimpleDoc].from_json(f.read(), tensor_type=TorchTensor)\nprint(dv_load_from_json)\n</code></pre> <pre><code>'{\"tensor_columns\":{},\"doc_columns\":{},\"docs_vec_columns\":{},\"any_columns\":{\"id\":[\"005a208a0a9a368c16bf77913b710433\",\"31d65f02cb94fc9756c57b0dbaac3a2c\"],\"text\":[\"doc 0\",\"doc 1\"]}}'\n&lt;DocVec[SimpleDoc] (length=2)&gt;\n</code></pre>"},{"location":"user_guide/sending/serialization/#protobuf_2","title":"Protobuf","text":"<ul> <li><code>to_protobuf</code> serializes a DocVec to <code>protobuf</code>. It returns a <code>protobuf</code> object of <code>docarray_pb2.DocVecProto</code> class. </li> <li><code>from_protobuf</code> deserializes a DocVec from <code>protobuf</code>. It accepts a protobuf message object to construct a DocVec.</li> </ul> <pre><code>import numpy as np\nfrom docarray import BaseDoc, DocVec\nfrom docarray.typing import AnyTensor\nclass SimpleVecDoc(BaseDoc):\ntensor: AnyTensor\ndv = DocVec[SimpleVecDoc]([SimpleVecDoc(tensor=np.ones(16)) for _ in range(8)])\nproto_message_dv = dv.to_protobuf()\ndv_from_proto = DocVec[SimpleVecDoc].from_protobuf(proto_message_dv)\n</code></pre> <p>You can deserialize any DocVec protobuf message to any tensor type, by passing the <code>tensor_type=...</code> parameter to <code>from_protobuf</code></p> <p>This means that you can choose at deserialization time if you are working with numpy, PyTorch, or TensorFlow tensors.</p> <p>If no <code>tensor_type</code> is passed, the default is <code>NdArray</code>.</p> <pre><code>import torch\nfrom docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor, NdArray, AnyTensor\nclass AnyTensorDoc(BaseDoc):\ntensor: AnyTensor\ndv = DocVec[AnyTensorDoc](\n[AnyTensorDoc(tensor=torch.ones(16)) for _ in range(8)], tensor_type=TorchTensor\n)\nproto_message_dv = dv.to_protobuf()\n# deserialize to torch\ndv_from_proto_torch = DocVec[AnyTensorDoc].from_protobuf(\nproto_message_dv, tensor_type=TorchTensor\n)\nassert dv_from_proto_torch.tensor_type == TorchTensor\nassert isinstance(dv_from_proto_torch.tensor, TorchTensor)\n# deserialize to numpy (default)\ndv_from_proto_numpy = DocVec[AnyTensorDoc].from_protobuf(proto_message_dv)\nassert dv_from_proto_numpy.tensor_type == NdArray\nassert isinstance(dv_from_proto_numpy.tensor, NdArray)\n</code></pre> <p>Note</p> <p>Serialization to protobuf is not supported for union types involving <code>BaseDoc</code> types.</p>"},{"location":"user_guide/sending/serialization/#base64_1","title":"Base64","text":"<p>When transferring data over the network, use <code>Base64</code> format to serialize the DocVec. Serializing a DocVec in Base64 supports both the <code>pickle</code> and <code>protobuf</code> protocols. You can also choose different compression methods.</p> <ul> <li><code>to_base64()</code> serializes a DocVec to Base64</li> <li><code>from_base64()</code> deserializes a DocVec from Base64:</li> </ul> <p>You can multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</p> <pre><code>from docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor\nimport torch\nclass SimpleDoc(BaseDoc):\ntext: str\ntensor: TorchTensor\ndv = DocVec[SimpleDoc](\n[SimpleDoc(text=f'doc {i}', tensor=torch.rand(64)) for i in range(2)]\n)\nbase64_repr_dv = dv.to_base64(compress=None, protocol='pickle')\ndl_from_base64 = DocVec[SimpleDoc].from_base64(\nbase64_repr_dv, compress=None, protocol='pickle', tensor_type=TorchTensor\n)\n</code></pre>"},{"location":"user_guide/sending/serialization/#save-binary_1","title":"Save binary","text":"<p>These methods serialize and save your data:</p> <ul> <li><code>save_binary()</code> saves a DocVec to a binary file.</li> <li><code>load_binary()</code> loads a DocVec from a binary file.</li> </ul> <p>You can choose between multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</p> <pre><code>from docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor\nimport torch\nclass SimpleDoc(BaseDoc):\ntext: str\ntensor: TorchTensor\ndv = DocVec[SimpleDoc](\n[SimpleDoc(text=f'doc {i}', tensor=torch.rand(64)) for i in range(2)]\n)\ndv.save_binary('simple-dl.pickle', compress=None, protocol='pickle')\ndv_from_binary = DocVec[SimpleDoc].load_binary(\n'simple-dv.pickle', compress=None, protocol='pickle', tensor_type=TorchTensor\n)\n</code></pre> <p>In the above snippet, the DocVec is stored as the file <code>simple-dv.pickle</code>.</p>"},{"location":"user_guide/sending/serialization/#bytes_1","title":"Bytes","text":"<p>These methods just serialize your data, without saving it to a file:</p> <ul> <li>to_bytes() saves a DocVec to a byte object.</li> <li>from_bytes() loads a DocVec from a byte object.  </li> </ul> <p>Note</p> <p>These methods are used under the hood by save_binary() and <code>load_binary()</code> to prepare/load/save to a binary file. You can also use them directly to work with byte files.</p> <p>Like working with binary files:</p> <ul> <li>You can use <code>protocol</code> to choose between <code>pickle</code> and <code>protobuf</code>. </li> <li>You can use multiple compression methods: <code>lz4</code>, <code>bz2</code>, <code>lzma</code>, <code>zlib</code>, and <code>gzip</code>.</li> </ul> <pre><code>from docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor\nimport torch\nclass SimpleDoc(BaseDoc):\ntext: str\ntensor: TorchTensor\ndv = DocVec[SimpleDoc](\n[SimpleDoc(text=f'doc {i}', tensor=torch.rand(64)) for i in range(2)]\n)\nbytes_dv = dv.to_bytes(protocol='pickle', compress=None)\ndv_from_bytes = DocVec[SimpleDoc].from_bytes(\nbytes_dv, compress=None, protocol='pickle', tensor_type=TorchTensor\n)\n</code></pre>"},{"location":"user_guide/sending/serialization/#csv_1","title":"CSV","text":"<p>Warning</p> <p><code>DocVec</code> does not support <code>.to_csv()</code> or <code>from_csv()</code>. This is because CSV is a row-based format while DocVec has a column-based data layout. To overcome this, you can convert your <code>DocVec</code> to a <code>DocList</code>.</p> <pre><code>from docarray import BaseDoc, DocList, DocVec\nclass SimpleDoc(BaseDoc):\ntext: str\ndv = DocVec[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(2)])\ndv.to_doc_list().to_csv('simple-dl.csv')\ndv_from_csv = DocList[SimpleDoc].from_csv('simple-dl.csv').to_doc_vec()\n</code></pre> <p>For more details you can check the DocList section on CSV serialization</p>"},{"location":"user_guide/sending/serialization/#pandasdataframe_1","title":"Pandas.Dataframe","text":"<ul> <li><code>from_dataframe()</code> loads a DocVec from a Pandas Dataframe.</li> <li><code>to_dataframe()</code> saves a DocVec to a Pandas Dataframe.</li> </ul> <pre><code>from docarray import BaseDoc, DocVec\nfrom docarray.typing import TorchTensor\nimport torch\nclass SimpleDoc(BaseDoc):\ntext: str\ntensor: TorchTensor\ndv = DocVec[SimpleDoc](\n[SimpleDoc(text=f'doc {i}', tensor=torch.rand(64)) for i in range(2)]\n)\ndf = dv.to_dataframe()\ndv_from_dataframe = DocVec[SimpleDoc].from_dataframe(df, tensor_type=TorchTensor)\nprint(dv_from_dataframe)\n</code></pre>"},{"location":"user_guide/sending/api/fastAPI/","title":"Send over FastAPI","text":""},{"location":"user_guide/sending/api/fastAPI/#send-over-fastapi","title":"Send over FastAPI","text":"<p>FastAPI is a high-performance web framework for building APIs with Python based on Python type hints. It's designed to be easy to use and supports asynchronous programming.  Since <code>DocArray</code> documents are Pydantic Models (with a twist) they can be easily integrated with FastAPI,  and provide a seamless and efficient way to work with multimodal data in FastAPI-powered APIs.</p> <p>Note</p> <p>you need to install FastAPI to follow this section </p><pre><code>pip install fastapi\n</code></pre>"},{"location":"user_guide/sending/api/fastAPI/#define-schemas","title":"Define schemas","text":"<p>First, you should define schemas for your input and/or output documents: </p><pre><code>from docarray import BaseDoc\nfrom docarray.documents import ImageDoc\nfrom docarray.typing import NdArray\nclass InputDoc(BaseDoc):\nimg: ImageDoc\nclass OutputDoc(BaseDoc):\nembedding_clip: NdArray\nembedding_bert: NdArray\n</code></pre>"},{"location":"user_guide/sending/api/fastAPI/#use-documents-with-fastapi","title":"Use documents with FastAPI","text":"<p>After creating your schemas, you can use your documents with FastAPI:</p> <pre><code>import numpy as np\nfrom fastapi import FastAPI\nfrom httpx import AsyncClient\nfrom docarray.documents import ImageDoc\nfrom docarray.base_doc import DocArrayResponse\ninput_doc = InputDoc(img=ImageDoc(tensor=np.zeros((3, 224, 224))))\napp = FastAPI()\n@app.post(\"/doc/\", response_model=OutputDoc, response_class=DocArrayResponse)\nasync def create_item(doc: InputDoc) -&gt; OutputDoc:\n## call my fancy model to generate the embeddings\ndoc = OutputDoc(\nembedding_clip=np.zeros((100, 1)), embedding_bert=np.zeros((100, 1))\n)\nreturn doc\nasync with AsyncClient(app=app, base_url=\"http://test\") as ac:\nresponse = await ac.post(\"/doc/\", data=input_doc.json())\ndoc = OutputDoc.parse_raw(response.content.decode())\n</code></pre> <p>The big advantage here is first-class support for ML centric data, such as <code>TorchTensor</code>, <code>TensorFlowTensor</code>, <code>Embedding</code>, etc.</p> <p>This includes handy features such as validating the shape of a tensor:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor\nimport torch\nclass MyDoc(BaseDoc):\ntensor: TorchTensor[3, 224, 224]\ndoc = MyDoc(tensor=torch.zeros(3, 224, 224))  # works\ndoc = MyDoc(tensor=torch.zeros(224, 224, 3))  # works by reshaping\ndoc = MyDoc(tensor=torch.zeros(224))  # fails validation\nclass Image(BaseDoc):\ntensor: TorchTensor[3, 'x', 'x']\nImage(tensor=torch.zeros(3, 224, 224))  # works\nImage(\ntensor=torch.zeros(3, 64, 128)\n)  # fails validation because second dimension does not match third\nImage(\ntensor=torch.zeros(4, 224, 224)\n)  # fails validation because of the first dimension\nImage(\ntensor=torch.zeros(3, 64)\n)  # fails validation because it does not have enough dimensions\n</code></pre>"},{"location":"user_guide/sending/api/fastAPI/#use-doclist-with-fastapi","title":"Use DocList with FastAPI","text":"<p>Further, you can send and receive lists of documents represented as a <code>DocList</code> object.</p> <p>To do that, you need to receive a list of documents (<code>List[TextDoc]</code>) in your FastAPI function, and then convert it to a <code>DocList</code> object. To return a <code>DocList</code> object, similarly, you need to convert it to a list first.</p> <p>Why is there no native support for <code>DocList</code>?</p> <p>We would love to natively support <code>DocList</code> in FastAPI, but it's not possible at the moment due to some behaviour stemming from Pydantic. This should be resolved once Pydantic v2 is released.</p> <p>If you are curious about the root cause of this, you can check out the following issues: - Pydantic issue #1457 - Should be resolved in Pydantic v2 (#4161) - DocArray needs the above (#1521)</p> <pre><code>from typing import List\nimport numpy as np\nfrom fastapi import FastAPI\nfrom httpx import AsyncClient\nfrom docarray import DocList\nfrom docarray.base_doc import DocArrayResponse\nfrom docarray.documents import TextDoc\n# Create a docarray\ndocs = DocList[TextDoc]([TextDoc(text='first'), TextDoc(text='second')])\napp = FastAPI()\n# Always use our custom response class (needed to dump tensors)\n@app.post(\"/doc/\", response_class=DocArrayResponse)\nasync def create_embeddings(docs: List[TextDoc]) -&gt; List[TextDoc]:\n# The docs FastAPI will receive will be treated as List[TextDoc]\n# so you need to cast it to DocList\ndocs = DocList[TextDoc].construct(docs)\n# Embed docs\nfor doc in docs:\ndoc.embedding = np.zeros((3, 224, 224))\n# Return your DocList as a list\nreturn list(docs)\nasync with AsyncClient(app=app, base_url=\"http://test\") as ac:\nresponse = await ac.post(\"/doc/\", data=docs.to_json())  # sending docs as json\nassert response.status_code == 200\n# You can read FastAPI's response in the following way\ndocs = DocList[TextDoc].from_json(response.content.decode())\n</code></pre>"},{"location":"user_guide/sending/api/fastAPI/#specify-tensor-shapes","title":"Specify tensor shapes","text":"<p>DocArray enables you to serve web apps that work on tensors (from numpy, PyTorch, or TensorFlow) as input and/or output, and lets you specify and validate the shapes of said tensors.</p> <p>To do that, you have to specify the expected shape in the type hint of your document. For example, you can create the following FastAPI app in <code>main.py</code>:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.typing import TorchTensor, NdArray\nfrom fastapi import FastAPI\nfrom docarray.base_doc import DocArrayResponse\nclass Doc(BaseDoc):\n# specify shapes of tensors\nembedding_torch: TorchTensor[10]\nembedding_np: NdArray[3, 4]\napp = FastAPI()\n@app.post(\"/foo\", response_model=Doc, response_class=DocArrayResponse)\nasync def foo(doc: Doc) -&gt; Doc:\nreturn Doc(embedding=doc.embedding_np)\n</code></pre> <p>You can start the app using: </p><pre><code>uvicorn main:app --reload\n</code></pre> <p>This API will now only accept an <code>embedding_torch</code> of shape <code>(10,)</code> and an <code>embedding_np</code> of shape <code>(3, 4)</code>.</p> <p>This is also reflected in the OpenAPI specification and SwaggerUI. Navigate to http://127.0.0.1:8000/docs to see the API documentation:</p> Example payloadSchema definition <p></p> <p></p> <p>Large tensors</p> <p>Rendering an example payload of a large (say, 3x224x224) tensor would prohibitively slow down the API documentation. Therefore, only tensors with a maximum of 256 elements generate a valid payload example.</p> <p>If you specify a larget tensor (e.g. <code>TorchTensor[3, 224, 224]</code>), the example payload will show a tensor with a single elemnt. But data validation will stil work as expected.</p>"},{"location":"user_guide/sending/api/jina/","title":"Send over Jina","text":""},{"location":"user_guide/sending/api/jina/#send-over-jina","title":"Send over Jina","text":"<p>In this example we'll build an audio-to-text app using Jina, DocArray and Whisper.</p> <p>We will use: </p> <ul> <li>DocArray &gt;=0.30: To load and preprocess multimodal data such as image, text and audio.</li> <li>Jina: To serve the model quickly and create a client.</li> </ul>"},{"location":"user_guide/sending/api/jina/#install-packages","title":"Install packages","text":"<p>First let's install requirements:</p> <pre><code>pip install transformers\npip install openai-whisper\npip install jina\n</code></pre>"},{"location":"user_guide/sending/api/jina/#import-libraries","title":"Import libraries","text":"<p>Let's import the necessary libraries:</p> <pre><code>import whisper\nfrom jina import Executor, requests, Deployment\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import AudioUrl\n</code></pre>"},{"location":"user_guide/sending/api/jina/#create-schemeas","title":"Create schemeas","text":"<p>Now we need to create the schema of our input and output documents. Since our input is an audio URL, our input schema should contain an <code>AudioUrl</code>:</p> <pre><code>class AudioURL(BaseDoc):\naudio: AudioUrl\n</code></pre> <p>For the output schema we would like to receive the transcribed text:</p> <pre><code>class Response(BaseDoc):\ntext: str\n</code></pre>"},{"location":"user_guide/sending/api/jina/#create-executor","title":"Create Executor","text":"<p>To create our model, we wrap our model into a Jina Executor, allowing us to serve the model later and expose the endpoint <code>/transcribe</code>:</p> <pre><code>class WhisperExecutor(Executor):\ndef __init__(self, device: str, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.model = whisper.load_model(\"medium.en\", device=device)\n@requests\ndef transcribe(self, docs: DocList[AudioURL], **kwargs) -&gt; DocList[Response]:\nresponse_docs = DocList[Response]()\nfor doc in docs:\ntranscribed_text = self.model.transcribe(str(doc.audio))['text']\nresponse_docs.append(Response(text=transcribed_text))\nreturn response_docs\n</code></pre>"},{"location":"user_guide/sending/api/jina/#deploy-executor-and-get-results","title":"Deploy Executor and get results","text":"<p>Now we can leverage Jina's Deployment object to deploy this Executor, then send a request to the <code>/transcribe</code> endpoint. </p> <p>Here we are using an audio file that says, \"A man reading a book\", saved as <code>resources/audio.mp3</code>:</p> <pre><code>dep = Deployment(\nuses=WhisperExecutor, uses_with={'device': \"cpu\"}, port=12349, timeout_ready=-1\n)\nwith dep:\ndocs = d.post(\non='/transcribe',\ninputs=[AudioURL(audio='resources/audio.mp3')],\nreturn_type=DocList[Response],\n)\nprint(docs[0].text)\n</code></pre> <p>And we get the transcribed result:</p> <pre><code>A man reading a book\n</code></pre>"},{"location":"user_guide/storing/docindex/","title":"Introduction","text":""},{"location":"user_guide/storing/docindex/#introduction","title":"Introduction","text":"<p>A Document Index lets you store your documents and search through them using vector similarity.</p> <p>This is useful if you want to store a bunch of data, and at a later point retrieve documents that are similar to some query that you provide. Relevant concrete examples are neural search applications, augmenting LLMs and chatbots with domain knowledge (Retrieval-Augmented Generation), or recommender systems.</p> <p>How does vector similarity search work?</p> <p>Without going into too much detail, the idea behind vector similarity search is the following:</p> <p>You represent every data point that you have (in our case, a document) as a vector, or embedding. This vector should represent as much semantic information about your data as possible: Similar data points should be represented by similar vectors.</p> <p>These vectors (embeddings) are usually obtained by passing the data through a suitable neural network that has been trained to produce such semantic representations - this is the encoding step.</p> <p>Once you have your vectors that represent your data, you can store them, for example in a vector database.</p> <p>To perform similarity search, you take your input query and encode it in the same way as the data in your database. Then, the database will search through the stored vectors and return those that are most similar to your query. This similarity is measured by a similarity metric, which can be cosine similarity, Euclidean distance, or any other metric that you can think of.</p> <p>If you store a lot of data, performing this similarity computation for every data point in your database is expensive. Therefore, vector databases usually perform approximate nearest neighbor (ANN) search. There are various algorithms for doing this, such as HNSW, but in a nutshell, they allow you to search through a large database of vectors very quickly, at the expense of a small loss in accuracy.</p> <p>DocArray's Document Index concept achieves this by providing a unified interface to a number of vector databases. In fact, you can think of Document Index as an ORM for vector databases.</p> <p>Currently, DocArray supports the following vector databases:</p> <ul> <li>Weaviate  |  Docs</li> <li>Qdrant  |  Docs</li> <li>Elasticsearch v7 and v8  |  Docs</li> <li>Epsilla  |  Docs</li> <li>Redis  |  Docs</li> <li>Milvus  |  Docs</li> <li>HNSWlib  |  Docs</li> <li>InMemoryExactNNIndex  |  Docs</li> </ul>"},{"location":"user_guide/storing/docindex/#basic-usage","title":"Basic usage","text":"<p>Let's learn the basic capabilities of Document Index with InMemoryExactNNIndex.  This doesn't require a database server - rather, it saves your data locally.</p> <p>Using a different vector database</p> <p>You can easily use Weaviate, Qdrant, Redis, Milvus or Elasticsearch instead -- their APIs are largely identical! To do so, check their respective documentation sections.</p> <p>InMemoryExactNNIndex in more detail</p> <p>The following section only covers the basics of InMemoryExactNNIndex.  For a deeper understanding, please look into its documentation.</p>"},{"location":"user_guide/storing/docindex/#define-document-schema-and-create-data","title":"Define document schema and create data","text":"<p>The following code snippet defines a document schema using the <code>BaseDoc</code> class. Each document consists of a title (a string),  a price (an integer), and an embedding (a 128-dimensional array). It also creates a list of ten documents with dummy titles,  prices ranging from 0 to 9, and randomly generated embeddings. </p><pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nimport numpy as np\nclass MyDoc(BaseDoc):\ntitle: str\nprice: int\nembedding: NdArray[128]\ndocs = DocList[MyDoc](\nMyDoc(title=f\"title #{i}\", price=i, embedding=np.random.rand(128))\nfor i in range(10)\n)\n</code></pre>"},{"location":"user_guide/storing/docindex/#initialize-the-document-index-and-add-data","title":"Initialize the Document Index and add data","text":"<p>Here we initialize an <code>InMemoryExactNNIndex</code> instance with the document schema we defined previously, and add the created documents to this index. </p><pre><code>doc_index = InMemoryExactNNIndex[MyDoc]()\ndoc_index.index(docs)\n</code></pre>"},{"location":"user_guide/storing/docindex/#perform-a-vector-similarity-search","title":"Perform a vector similarity search","text":"<p>Now, let's perform a similarity search on the document embeddings.  As a result, we'll retrieve the ten most similar documents and their corresponding similarity scores. </p><pre><code>query = np.ones(128)\nretrieved_docs, scores = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/docindex/#filter-documents","title":"Filter documents","text":"<p>In this snippet, we filter the indexed documents based on their price field, specifically retrieving documents with a price less than 5: </p><pre><code>query = {'price': {'$lt': 5}}\nfiltered_docs = doc_index.filter(query, limit=10)\n</code></pre>"},{"location":"user_guide/storing/docindex/#combine-different-search-methods","title":"Combine different search methods","text":"<p>The final snippet combines the vector similarity search and filtering operations into a single query.  We first perform a similarity search on the document embeddings and then apply a filter to return only those documents with a price greater than or equal to 2: </p><pre><code>query = (\ndoc_index.build_query()  # get empty query object\n.find(query=np.ones(128), search_field='embedding')  # add vector similarity search\n.filter(filter_query={'price': {'$gte': 2}})  # add filter search\n.build()  # build the query\n)\nretrieved_docs, scores = doc_index.execute_query(query)\n</code></pre>"},{"location":"user_guide/storing/docindex/#learn-more","title":"Learn more","text":"<p>The code snippets above just scratch the surface of what a Document Index can do.  To learn more and get the most out of <code>DocArray</code>, take a look at the detailed guides for the vector database backends you're interested in:</p> <ul> <li>Weaviate  |  Docs</li> <li>Qdrant  |  Docs</li> <li>Elasticsearch v7 and v8  |  Docs</li> <li>Epsilla  |  Docs</li> <li>Redis  |  Docs</li> <li>Milvus  |  Docs</li> <li>HNSWlib  |  Docs</li> <li>InMemoryExactNNIndex  |  Docs</li> </ul>"},{"location":"user_guide/storing/first_step/","title":"Introduction","text":""},{"location":"user_guide/storing/first_step/#introduction","title":"Introduction","text":"<p>In the previous sections we saw how to use <code>BaseDoc</code>, <code>DocList</code> and <code>DocVec</code> to represent multimodal data and send it over the wire. In this section we will see how to store and persist this data.</p> <p>DocArray offers two ways of storing your data, each of which have their own documentation sections:</p> <ol> <li>Document Store for simple long-term storage</li> <li>Document Index for fast retrieval using vector similarity</li> </ol>"},{"location":"user_guide/storing/first_step/#document-store","title":"Document Store","text":"<p>DocList can be persisted using the <code>.push()</code> and  <code>.pull()</code> methods.  Under the hood, DocStore is used to persist a <code>DocList</code>.  You can either store your documents on-disk or upload them to AWS S3 or minio. </p> <p>This section covers the following three topics:</p> <ul> <li>Storing <code>BaseDoc</code>, <code>DocList</code> and <code>DocVec</code> on-disk</li> <li>Storing on S3</li> </ul>"},{"location":"user_guide/storing/first_step/#document-index","title":"Document Index","text":"<p>A Document Index lets you store your documents and search through them using vector similarity.</p> <p>This is useful if you want to store a bunch of data, and at a later point retrieve documents that are similar to a query that you provide. Relevant concrete examples are neural search applications, augmenting LLMs and chatbots with domain knowledge (Retrieval-Augmented Generation)]), or recommender systems.</p> <p>DocArray's Document Index concept achieves this by providing a unified interface to a number of vector databases. In fact, you can think of Document Index as an ORM for vector databases.</p> <p>Currently, DocArray supports the following vector indexes. Some of them wrap vector databases (Weaviate, Qdrant, ElasticSearch) and act as a client for them, while others use a vector search library locally (HNSWLib, Exact NN search):</p> <ul> <li>Weaviate  |  Docs</li> <li>Qdrant  |  Docs</li> <li>Elasticsearch v7 and v8  |  Docs</li> <li>Redis  |  Docs</li> <li>Milvus  |  Docs</li> <li>Hnswlib  |  Docs</li> <li>InMemoryExactNNSearch  |  Docs</li> </ul>"},{"location":"user_guide/storing/index_elastic/","title":"Elasticsearch Document Index","text":""},{"location":"user_guide/storing/index_elastic/#elasticsearch-document-index","title":"Elasticsearch Document Index","text":"<p>DocArray comes with two Document Indexes for Elasticsearch:</p> <ul> <li>ElasticDocIndex, based on Elasticsearch 8.</li> <li>ElasticV7DocIndex, based on Elasticsearch 7.10.</li> </ul> <p>Should you use ES v7 or v8?</p> <p>Elasticsearch v8 is the current version of ES and offers native vector search (ANN) support, alongside text and range search.</p> <p>Elasticsearch v7.10 can store vectors, but does not support native ANN vector search, but only exhaustive (i.e. slow) vector search, alongside text and range search.</p> <p>Some users prefer to use ES v7.10 because it is available under a different license to ES v8.0.0.</p> <p>Installation</p> <p>To use ElasticDocIndex, you need to install the following dependencies:</p> <pre><code>pip install elasticsearch==8.6.2\npip install elastic-transport\n</code></pre> <p>To use ElasticV7DocIndex, you need to install the following dependencies:</p> <pre><code>pip install elasticsearch==7.10.1\npip install elastic-transport\n</code></pre> <p>The following example is based on ElasticDocIndex, but will also work for ElasticV7DocIndex.</p>"},{"location":"user_guide/storing/index_elastic/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of ElasticDocIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of ElasticDocIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import ElasticDocIndex  # or ElasticV7DocIndex\nfrom docarray.typing import NdArray\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new ElasticDocIndex instance and add the documents to the index.\ndoc_index = ElasticDocIndex[MyDoc](index_name='my_index')\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#initialize","title":"Initialize","text":"<p>You can use docker-compose to create a local Elasticsearch service with the following <code>docker-compose.yml</code>.</p> <pre><code>version: \"3.3\"\nservices:\nelastic:\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.6.2\nenvironment:\n- xpack.security.enabled=false\n- discovery.type=single-node\n- ES_JAVA_OPTS=-Xmx1024m\nports:\n- \"9200:9200\"\nnetworks:\n- elastic\nnetworks:\nelastic:\nname: elastic\n</code></pre> <p>Run the following command in the folder of the above <code>docker-compose.yml</code> to start the service:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#schema-definition","title":"Schema definition","text":"<p>To construct an index, you first need to define a schema in the form of a <code>Document</code>.</p> <p>There are a number of configurations you can pack into your schema:</p> <ul> <li>Every field in your schema will become one column in the database</li> <li>For vector fields, such as <code>NdArray</code>, <code>TorchTensor</code>, or <code>TensorflowTensor</code>, you need to specify a dimensionality to be able to perform vector search</li> <li>You can override the default column type for every field by passing any ES field data type to <code>field_name: Type = Field(col_type=...)</code>. You can see an example of this in the section on keyword filters.</li> </ul> <p>Additionally, you can pass a <code>hosts</code> argument to the <code>__init__()</code> method to connect to an ES instance. By default, it is <code>http://localhost:9200</code>. </p> <pre><code>import numpy as np\nfrom pydantic import Field\nfrom docarray import BaseDoc\nfrom docarray.index import ElasticDocIndex\nfrom docarray.typing import NdArray\nclass SimpleDoc(BaseDoc):\n# specify tensor field with dimensionality 128\ntensor: NdArray[128]\n# alternative and equivalent definition:\n# tensor: NdArray = Field(dims=128)\ndoc_index = ElasticDocIndex[SimpleDoc](hosts='http://localhost:9200')\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import ElasticDocIndex\nclass MyDoc(TextDoc):\nembedding: NdArray[128]\ndb = ElasticDocIndex[MyDoc](index_name='test_db')\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import ElasticDocIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128)\ndb = ElasticDocIndex[MyDoc](index_name='test_db3')\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[TextDoc](\n[\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndb.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method. The <code>.num_docs()</code> method returns the total number of documents in the index.</p> <pre><code>from docarray import DocList\n# create some random data\ndocs = DocList[SimpleDoc]([SimpleDoc(tensor=np.ones(128)) for _ in range(64)])\ndoc_index.index(docs)\nprint(f'number of docs in the index: {doc_index.num_docs()}')\n</code></pre> <p>As you can see, <code>DocList[SimpleDoc]</code> and <code>ElasticDocIndex[SimpleDoc]</code> both have <code>SimpleDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_elastic/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can use the <code>find()</code> function with a document of the type <code>MyDoc</code>  to find similar documents within the Document Index:</p> <p>You can use the <code>limit</code> argument to configure how many documents to return.</p> <p>Note</p> <p>ElasticV7DocIndex uses Elasticsearch v7.10.1, which does not support approximate nearest neighbour algorithms such as HNSW. This can lead to poor performance when the search involves many vectors. ElasticDocIndex does not have this limitation.</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = SimpleDoc(tensor=np.ones(128))\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='tensor', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='tensor', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>To peform a vector search, you need to specify a <code>search_field</code>. This is the field that serves as the basis of comparison between your query and the documents in the Document Index.</p> <p>In this example you only have one field (<code>tensor</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_elastic/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by DocumentsSearch by raw vectors <pre><code># create some query Documents\nqueries = DocList[SimpleDoc](\nSimpleDoc(tensor=np.random.rand(128)) for i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, search_field='tensor', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, search_field='tensor', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_elastic/#filter","title":"Filter","text":"<p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding filter query.  The query should follow Elastic's query language.</p> <p>The <code>filter()</code> method accepts queries that follow the Elasticsearch Query DSL and consists of leaf and compound clauses.</p> <p>Using this, you can perform keyword filters, geolocation filters and range filters.</p>"},{"location":"user_guide/storing/index_elastic/#keyword-filter","title":"Keyword filter","text":"<p>To filter documents in your index by keyword, you can use <code>Field(col_type='keyword')</code> to enable keyword search for given fields:</p> <pre><code>class NewsDoc(BaseDoc):\ntext: str\ncategory: str = Field(col_type='keyword')  # enable keyword filtering\ndoc_index = ElasticDocIndex[NewsDoc]()\nindex_docs = [\nNewsDoc(id='0', text='this is a news for sport', category='sport'),\nNewsDoc(id='1', text='this is a news for finance', category='finance'),\nNewsDoc(id='2', text='this is another news for sport', category='sport'),\n]\ndoc_index.index(index_docs)\n# search with filer\nquery_filter = {'terms': {'category': ['sport']}}\ndocs = doc_index.filter(query_filter)\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#geolocation-filter","title":"Geolocation filter","text":"<p>To filter documents in your index by geolocation, you can use <code>Field(col_type='geo_point')</code> on a given field:</p> <pre><code>class NewsDoc(BaseDoc):\ntext: str\nlocation: dict = Field(col_type='geo_point')  # enable geolocation filtering\ndoc_index = ElasticDocIndex[NewsDoc]()\nindex_docs = [\nNewsDoc(text='this is from Berlin', location={'lon': 13.24, 'lat': 50.31}),\nNewsDoc(text='this is from Beijing', location={'lon': 116.22, 'lat': 39.55}),\nNewsDoc(text='this is from San Jose', location={'lon': -121.89, 'lat': 37.34}),\n]\ndoc_index.index(index_docs)\n# filter the eastern hemisphere\nquery = {\n'bool': {\n'filter': {\n'geo_bounding_box': {\n'location': {\n'top_left': {'lon': 0, 'lat': 90},\n'bottom_right': {'lon': 180, 'lat': 0},\n}\n}\n}\n}\n}\ndocs = doc_index.filter(query)\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#range-filter","title":"Range filter","text":"<p>You can have range field types in your document schema and set <code>Field(col_type='integer_range')</code>(or also <code>date_range</code>, etc.) to filter documents based on the range of the field. </p> <pre><code>class NewsDoc(BaseDoc):\ntime_frame: dict = Field(\ncol_type='date_range', format='yyyy-MM-dd'\n)  # enable range filtering\ndoc_index = ElasticDocIndex[NewsDoc]()\nindex_docs = [\nNewsDoc(time_frame={'gte': '2023-01-01', 'lt': '2023-02-01'}),\nNewsDoc(time_frame={'gte': '2023-02-01', 'lt': '2023-03-01'}),\nNewsDoc(time_frame={'gte': '2023-03-01', 'lt': '2023-04-01'}),\n]\ndoc_index.index(index_docs)\nquery = {\n'bool': {\n'filter': {\n'range': {\n'time_frame': {\n'gte': '2023-02-05',\n'lt': '2023-02-10',\n'relation': 'contains',\n}\n}\n}\n}\n}\ndocs = doc_index.filter(query)\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#text-search","title":"Text search","text":"<p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p> <p>As in \"pure\" Elasticsearch, you can use text search directly on the field of type <code>str</code>:</p> <pre><code>class NewsDoc(BaseDoc):\ntext: str\ndoc_index = ElasticDocIndex[NewsDoc]()\nindex_docs = [\nNewsDoc(id='0', text='this is a news for sport'),\nNewsDoc(id='1', text='this is a news for finance'),\nNewsDoc(id='2', text='this is another news for sport'),\n]\ndoc_index.index(index_docs)\nquery = 'finance'\n# search with text\ndocs, scores = doc_index.text_search(query, search_field='text')\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <p>For example, you can build a hybrid serach query that performs range filtering, vector search and text search:</p> <pre><code>class MyDoc(BaseDoc):\ntens: NdArray[10] = Field(similarity='l2_norm')\nnum: int\ntext: str\ndoc_index = ElasticDocIndex[MyDoc]()\nindex_docs = [\nMyDoc(id=f'{i}', tens=np.ones(10) * i, num=int(i / 2), text=f'text {int(i/2)}')\nfor i in range(10)\n]\ndoc_index.index(index_docs)\nq = (\ndoc_index.build_query()\n.filter({'range': {'num': {'lte': 3}}})\n.find(index_docs[-1], search_field='tens')\n.text_search('0', search_field='text')\n.build()\n)\ndocs, _ = doc_index.execute_query(q)\n</code></pre> <p>You can also manually build a valid ES query and directly pass it to the <code>execute_query()</code> method.</p>"},{"location":"user_guide/storing/index_elastic/#access-documents","title":"Access documents","text":"<p>To access a document, you need to specify its <code>id</code>. You can also pass a list of <code>id</code>s to access multiple documents.</p> <pre><code># access a single Doc\ndoc_index[index_docs[1].id]\n# access multiple Docs\ndoc_index[index_docs[2].id, index_docs[3].id]\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#delete-documents","title":"Delete documents","text":"<p>To delete documents, use the built-in function <code>del</code> with the <code>id</code> of the documents that you want to delete. You can also pass a list of <code>id</code>s to delete multiple documents.</p> <pre><code># delete a single Doc\ndel doc_index[index_docs[1].id]\n# delete multiple Docs\ndel doc_index[index_docs[2].id, index_docs[3].id]\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#configuration","title":"Configuration","text":""},{"location":"user_guide/storing/index_elastic/#dbconfig","title":"DBConfig","text":"<p>The following configs can be set in <code>DBConfig</code>:</p> Name Description Default <code>hosts</code> Hostname of the Elasticsearch server <code>http://localhost:9200</code> <code>es_config</code> Other ES configuration options in a Dict and pass to <code>Elasticsearch</code> client constructor, e.g. <code>cloud_id</code>, <code>api_key</code> <code>None</code> <code>index_name</code> Elasticsearch index name, the name of Elasticsearch index object <code>None</code>. Data will be stored in an index named after the Document type used as schema. <code>index_settings</code> Other index settings in a Dict for creating the index dict <code>index_mappings</code> Other index mappings in a Dict for creating the index dict <code>default_column_config</code> The default configurations for every column type. dict <p>You can pass any of the above as keyword arguments to the <code>__init__()</code> method or pass an entire configuration object. See here for more information.</p> <p><code>default_column_config</code> is the default configurations for every column type. Since there are many column types in Elasticsearch, you can also consider changing the column config when defining the schema.</p> <pre><code>class SimpleDoc(BaseDoc):\ntensor: NdArray[128] = Field(similarity='l2_norm', m=32, num_candidates=5000)\ndoc_index = ElasticDocIndex[SimpleDoc](index_name='my_index_1')\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#runtimeconfig","title":"RuntimeConfig","text":"<p>The <code>RuntimeConfig</code> dataclass of <code>ElasticDocIndex</code> consists of <code>chunk_size</code>. You can change <code>chunk_size</code> for batch operations:</p> <pre><code>doc_index = ElasticDocIndex[SimpleDoc](index_name='my_index_2')\ndoc_index.configure(ElasticDocIndex.RuntimeConfig(chunk_size=1000))\n</code></pre> <p>You can pass the above as keyword arguments to the <code>configure()</code> method or pass an entire configuration object. See here for more information.</p>"},{"location":"user_guide/storing/index_elastic/#persistence","title":"Persistence","text":"<p>You can hook into a database index that was persisted during a previous session by  specifying the <code>index_name</code> and <code>hosts</code>:</p> <pre><code>doc_index = ElasticDocIndex[MyDoc](\nhosts='http://localhost:9200', index_name='previously_stored'\n)\ndoc_index.index(index_docs)\ndoc_index2 = ElasticDocIndex[MyDoc](\nhosts='http://localhost:9200', index_name='previously_stored'\n)\nprint(f'number of docs in the persisted index: {doc_index2.num_docs()}')\n</code></pre>"},{"location":"user_guide/storing/index_elastic/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_epsilla/","title":"Epsilla Document Index","text":""},{"location":"user_guide/storing/index_epsilla/#epsilla-document-index","title":"Epsilla Document Index","text":"<p>Install dependencies</p> <p>To use EpsillaDocumentIndex, you need to install extra dependencies with the following command:</p> <pre><code>    pip install \"docarray[epsilla]\"\n    pip install --upgrade pyepsilla\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of EpsillaDocumentIndex:</p> <ol> <li>Define a document schema with two fields: title and embedding</li> <li>Create ten dummy documents with random embeddings</li> <li>Set the db config and initialize the index</li> <li>Add dummy documents to the index</li> <li>Finally, perform a vector similarity search to retrieve the ten most similar documents to a given query vector</li> </ol> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index.backends.epsilla import EpsillaDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str\nembedding: NdArray[128] = Field(is_embedding=True)\n# Create dummy documents.\ndocs = DocList[MyDoc](\nMyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10)\n)\n# db_config, see the initialize section below\ndb_config = EpsillaDocumentIndex.DBConfig(\nis_self_hosted=True,\nprotocol=\"http\",\nhost=\"localhost\",\nport=8888,\ndb_path=\"/epsilla\",\ndb_name=\"test\",\n)\n# Initialize a new EpsillaDocumentIndex instance\ndoc_index = EpsillaDocumentIndex[MyDoc](db_config=db_config)\n# Add the documents to the index.\ndoc_index.index(docs)\n# Perform a vector search.\nquery = MyDoc(title=\"test\", embedding=np.ones(128))\nretrieved_docs = doc_index.find(query, limit=10, search_field=\"embedding\")\nprint(f'{retrieved_docs=}')\nretrieved_docs[0].summary()\n</code></pre> <p>The following sections will cover details of the individual steps.</p>"},{"location":"user_guide/storing/index_epsilla/#initialize","title":"Initialize","text":""},{"location":"user_guide/storing/index_epsilla/#start-and-connect-to-epsilla","title":"Start and connect to Epsilla","text":"<p>To use EpsillaDocumentIndex, DocArray needs to hook into a running Epsilla service. There are multiple ways to start a Epsilla instance, depending on your use case.</p> <p>Options - Overview</p> Instance type General use case Configurability Notes Epsilla Cloud  Development and production Limited Recommended for most users Docker Self hosted Full <p>Connect via Epsilla Cloud</p> <p>Check out Epsilla's documentation to create an instance, and for information on obtaining your credentials.</p> <p>Connect via Docker (self-managed)</p> <pre><code>docker pull epsilla/vectordb\n</code></pre> <p>Start the docker as the backend service</p> <pre><code>docker run --pull=always -d -p 8888:8888 epsilla/vectordb\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#connecting-to-epsilla","title":"Connecting to Epsilla","text":"<p>Cloud instance</p> <p>Check out Epsilla's documentation for credentials.</p> <pre><code>from docarray.index.backends.epsilla import EpsillaDocumentIndex\ndb = EpsillaDocumentIndex.DBConfig(\nis_self_hosted=False,\ncloud_project_id=\"your-project-id\",\ncloud_db_id=\"your-database-id\",\napi_key=\"your-epsilla-api-key\",\n)\n</code></pre> <p>Self hosted</p> <pre><code>from docarray.index.backends.epsilla import EpsillaDocumentIndex\ndb = EpsillaDocumentIndex.DBConfig(\nis_self_hosted=True,\nprotocol=None,\nhost=\"localhost\",\nport=8888,\ndb_path=None,\ndb_name=None,\n)\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#create-an-instance","title":"Create an instance","text":"<p>Let's connect to a local Epsilla container and instantiate a <code>EpsillaDocumentIndex</code> instance for a given schema:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.index.backends.epsilla import EpsillaDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str\nembedding: NdArray[128] = Field(is_embedding=True)\n# Set the database configuration.\ndb_config = EpsillaDocumentIndex.DBConfig(\nis_self_hosted=True,\nprotocol=\"http\",\nhost=\"localhost\",\nport=8888,\ndb_path=\"/epsilla\",\ndb_name=\"test\",\n)\n# Initialize a new EpsillaDocumentIndex instance\ndoc_index = EpsillaDocumentIndex[MyDoc](db_config=db_config)\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>EpsillaDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_epsilla/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hint <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import EpsillaDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: NdArray[128] = Field(is_embedding=True)\ndoc_index = EpsillaDocumentIndex[MyDoc]()\n</code></pre> Using Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import EpsillaDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128, is_embedding=True)\ndoc_index = EpsillaDocumentIndex[MyDoc]()\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code> , <code>ImageDoc</code> etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\ndata = DocList[MyDoc](\n[\nMyDoc(title='hello world', embedding=np.random.rand(128)),\nMyDoc(title='hello world', embedding=np.random.rand(128)),\nMyDoc(title='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndoc_index.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index.backends.epsilla import EpsillaDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\nclass MyDoc(BaseDoc):\ntitle: str\nembedding: NdArray[128] = Field(is_embedding=True)\n# Create dummy documents.\ndocs = DocList[MyDoc](\nMyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10)\n)\ndb_config = \"...\"  # see the initialize section above\ndoc_index = EpsillaDocumentIndex[MyDoc](db_config=db_config, index_name='mydoc_index')\n# add the data\ndoc_index.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all Documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[Document]</code> and <code>EpsillaDocumentIndex[Document]</code> both have <code>Document</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_epsilla/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can perform a similarity search and find relevant documents by passing <code>MyDoc</code> or a raw vector to the <code>find()</code> method:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = Document(\ntext=\"Hello world\",\nembedding=np.array([1, 2]),\nfile=np.random.rand(100),\n)\n# find similar documents\nmatches, scores = doc_index.find(query, limit=5)\nprint(f\"{matches=}\")\nprint(f\"{matches.text=}\")\nprint(f\"{scores=}\")\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(2)\n# find similar documents\nmatches, scores = store.find(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_epsilla/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nDocument(\ntext=f\"Hello world {i}\",\nembedding=np.array([i, i + 1]),\nfile=np.random.rand(100),\n)\nfor i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, limit=5)\nprint(f\"{matches=}\")\nprint(f\"{matches[0].text=}\")\nprint(f\"{scores=}\")\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 2)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_epsilla/#filter","title":"Filter","text":"<p>To perform filtering, follow the below syntax.</p> <p>This will perform a filtering on the field <code>title</code>:</p> <pre><code>docs = doc_index.filter(\"title = 'test'\", limit=5)\n</code></pre> <p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding filter query. The query should follow the filters supported by Epsilla.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index.backends.epsilla import EpsillaDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\nclass Book(BaseDoc):\nprice: int\nembedding: NdArray[10] = Field(is_embedding=True)\nbooks = DocList[Book](\n[Book(price=i * 10, embedding=np.random.rand(10)) for i in range(10)]\n)\ndb_config = \"...\"  # see the initialize section above\nbook_index = EpsillaDocumentIndex[Book](db_config=db_config, index_name='tmp_index')\nbook_index.index(books)\n# filter for books that are cheaper than 29 dollars\nquery = \"price &lt; 29\"\ncheap_books = book_index.filter(filter_query=query)\nprint(f\"{cheap_books=}\")\ncheap_books[0].summary()\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#text-search","title":"Text search","text":"<p>Warning</p> <p>The EpsillaDocumentIndex implementation does not support text search.</p>"},{"location":"user_guide/storing/index_epsilla/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <pre><code># Define the document schema.\nclass SimpleSchema(BaseDoc):\nyear: int\nprice: int\nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[SimpleSchema](\nSimpleSchema(year=2000 - i, price=i, embedding=np.random.rand(128))\nfor i in range(10)\n)\ndoc_index = EpsillaDocumentIndex[SimpleSchema]()\ndoc_index.index(docs)\nquery = (\ndoc_index.build_query()  # get empty query object\n.filter(filter_query=\"year&gt;1994\")  # pre-filtering\n.find(\nquery=np.random.rand(128), search_field='embedding'\n)  # add vector similarity search\n.filter(filter_query=\"price&lt;3\")  # post-filtering\n.build()\n)\n# execute the combined query and return the results\nresults = doc_index.execute_query(query)\nprint(f'{results=}')\n</code></pre> <p>In the example above you can see how to form a hybrid query that combines vector similarity search and filtered search to obtain a combined set of results.</p> <p>The kinds of atomic queries that can be combined in this way depends on the backend. Some backends can combine text search and vector search, while others can perform filters and vectors search, etc.</p>"},{"location":"user_guide/storing/index_epsilla/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), title=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the documents by id\ndoc = doc_index[ids[0]]  # get by single id\ndocs = doc_index[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), title=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the documents by id\ndel doc_index[ids[0]]  # del by single id\ndel doc_index[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#count-documents","title":"Count documents","text":"<p>Warning</p> <p>Unlike other index backends, Epsilla does not provide a count API. When using it with docarray, calling the <code>num_docs</code> method will raise errors.</p> <pre><code># will raise errors\ndoc_index.num_docs()\n</code></pre> <p>If you need to count how many documents there are in the index, you can try to use the filter method.</p> <pre><code># use a larger limit as needed\ndoc_index.filter(filter_query=\"\", limit=100)\n</code></pre>"},{"location":"user_guide/storing/index_epsilla/#configuration","title":"Configuration","text":""},{"location":"user_guide/storing/index_epsilla/#dbconfig","title":"DBConfig","text":"<p>The following configs can be set in <code>DBConfig</code>:</p> Name Description Default <code>is_self_hosted</code> If using Epsilla cloud or running self hosted <code>false</code> <code>cloud_project_id</code> If using Epsilla cloud; found in the console <code>None</code> <code>cloud_db_id</code> If using Epsilla cloud; found in the console <code>None</code> <code>api_key</code> If using Epsilla cloud; found in the console <code>None</code> <code>host</code> Address or 'localhost' <code>None</code> <code>port</code> The port number for the Epsilla server 8888 <code>protocol</code> Protocol to connect, e.g. 'http' <code>None</code> <code>db_path</code> Path to the database on disk <code>None</code> <code>db_name</code> Name of the database <code>None</code> <p>You can pass any of the above as keyword arguments to the <code>__init__()</code> method or pass an entire configuration object.</p>"},{"location":"user_guide/storing/index_epsilla/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>. However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document contains a <code>DocList</code> of other documents.</p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_hnswlib/","title":"Hnswlib Document Index","text":""},{"location":"user_guide/storing/index_hnswlib/#hnswlib-document-index","title":"Hnswlib Document Index","text":"<p>Note</p> <p>To use HnswDocumentIndex, you need to install the extra dependency with the following command:</p> <pre><code>pip install \"docarray[hnswlib]\"\n</code></pre> <p>HnswDocumentIndex is a lightweight Document Index implementation that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in hnswlib, and stores all other data in SQLite.</p> <p>Production readiness</p> <p>HnswDocumentIndex is a great starting point for small- to medium-sized datasets, but it is not battle tested in production. If scalability, uptime, etc. are important to you, we recommend you eventually transition to one of our database-backed Document Index implementations:</p> <ul> <li>QdrantDocumentIndex</li> <li>WeaviateDocumentIndex</li> <li>ElasticDocumentIndex</li> <li>RedisDocumentIndex</li> <li>MilvusDocumentIndex</li> </ul>"},{"location":"user_guide/storing/index_hnswlib/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of HnswDocumentIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of HnswDocumentIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import HnswDocumentIndex\nfrom docarray.typing import NdArray\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new HnswDocumentIndex instance and add the documents to the index.\ndoc_index = HnswDocumentIndex[MyDoc](work_dir='./tmp_0')\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#initialize","title":"Initialize","text":"<p>To create a Document Index, you first need a document class that defines the schema of your index:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\nembedding: NdArray[128]\ntext: str\ndb = HnswDocumentIndex[MyDoc](work_dir='./tmp_1')\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>HnswDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_hnswlib/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import HnswDocumentIndex\nclass MyDoc(TextDoc):\nembedding: NdArray[128]\ndb = HnswDocumentIndex[MyDoc](work_dir='./tmp_2')\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import HnswDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128)\ndb = HnswDocumentIndex[MyDoc](work_dir='./tmp_3')\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[TextDoc](\n[\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndb.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>import numpy as np\nfrom docarray import DocList\n# create some random data\ndocs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'text {i}') for i in range(100)]\n)\n# index the data\ndb.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all Documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[MyDoc]</code> and <code>HnswDocumentIndex[MyDoc]</code> both have <code>MyDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_hnswlib/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can use the <code>find()</code> function with a document of the type <code>MyDoc</code>  to find similar documents within the Document Index:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = MyDoc(embedding=np.random.rand(128), text='query')\n# find similar documents\nmatches, scores = db.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = db.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>To peform a vector search, you need to specify a <code>search_field</code>. This is the field that serves as the basis of comparison between your query and the documents in the Document Index.</p> <p>In this example you only have one field (<code>embedding</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_hnswlib/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by DocumentsSearch by raw vectors <pre><code># create some query Documents\nqueries = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# find similar documents\nmatches, scores = db.find_batched(queries, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = db.find_batched(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_hnswlib/#filter","title":"Filter","text":"<p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of DocArray's <code>filter_docs()</code> function.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Book(BaseDoc):\ntitle: str\nprice: int\nbooks = DocList[Book]([Book(title=f'title {i}', price=i * 10) for i in range(10)])\nbook_index = HnswDocumentIndex[Book](work_dir='./tmp_4')\n# filter for books that are cheaper than 29 dollars\nquery = {'price': {'$lt': 29}}\ncheap_books = book_index.filter(query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#text-search","title":"Text search","text":"<p>Note</p> <p>The HnswDocumentIndex implementation does not support text search.</p> <p>To see how to perform text search, you can check out other backends that offer support.</p> <p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p>"},{"location":"user_guide/storing/index_hnswlib/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <pre><code># Define the document schema.\nclass SimpleSchema(BaseDoc):\nyear: int\nprice: int\nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[SimpleSchema](SimpleSchema(year=2000-i, price=i, embedding=np.random.rand(128)) for i in range(10))\ndoc_index = HnswDocumentIndex[SimpleSchema](work_dir='./tmp_5')\ndoc_index.index(docs)\nquery = (\ndoc_index.build_query()  # get empty query object\n.filter(filter_query={'year': {'$gt': 1994}})  # pre-filtering\n.find(query=np.random.rand(128), search_field='embedding')  # add vector similarity search\n.filter(filter_query={'price': {'$lte': 3}})  # post-filtering\n.build()\n)\n# execute the combined query and return the results\nresults = doc_index.execute_query(query)\nprint(f'{results=}')\n</code></pre> <p>In the example above you can see how to form a hybrid query that combines vector similarity search and filtered search to obtain a combined set of results.</p> <p>The kinds of atomic queries that can be combined in this way depends on the backend. Some backends can combine text search and vector search, while others can perform filters and vectors search, etc.</p>"},{"location":"user_guide/storing/index_hnswlib/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndoc = db[ids[0]]  # get by single id\ndocs = db[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access Documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndel db[ids[0]]  # del by single id\ndel db[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#update-documents","title":"Update documents","text":"<p>In order to update a Document inside the index, you only need to re-index it with the updated attributes.</p> <p>First, let's create a schema for our Document Index: </p><pre><code>import numpy as np\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom docarray.index import HnswDocumentIndex\nclass MyDoc(BaseDoc):\ntext: str\nembedding: NdArray[128]\n</code></pre> <p>Now, we can instantiate our Index and add some data: </p><pre><code>docs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'I am the first version of Document {i}') for i in range(100)]\n)\nindex = HnswDocumentIndex[MyDoc]()\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>Let's retrieve our data and check its content: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the first version' in doc.text\n</code></pre> <p>Then, let's update all of the text of these documents and re-index them: </p><pre><code>for i, doc in enumerate(docs):\ndoc.text = f'I am the second version of Document {i}'\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>When we retrieve them again we can see that their text attribute has been updated accordingly: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the second version' in doc.text\n</code></pre>"},{"location":"user_guide/storing/index_hnswlib/#configuration","title":"Configuration","text":"<p>This section lays out the configurations and options that are specific to HnswDocumentIndex.</p>"},{"location":"user_guide/storing/index_hnswlib/#dbconfig","title":"DBConfig","text":"<p>The <code>DBConfig</code> of HnswDocumentIndex contains two argument: <code>work_dir</code> and <code>default_column_configs</code></p> <p><code>work_dir</code> is the location where all of the Index's data will be stored, namely the various HNSWLib indexes and the SQLite database.</p> <p>You can pass this directly to the constructor:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\nembedding: NdArray[128]\ntext: str\ndb = HnswDocumentIndex[MyDoc](work_dir='./tmp_6')\n</code></pre> <p>To load existing data, you can specify a directory that stores data from a previous session.</p> <p>Hnswlib file lock</p> <p>Hnswlib uses a file lock to prevent multiple processes from accessing the same index at the same time. This means that if you try to open an index that is already open in another process, you will get an error. To avoid this, you can specify a different <code>work_dir</code> for each process.</p> <p><code>default_column_configs</code> contains the default mapping from Python types to column configurations.</p> <p>You can see in the section below how to override configurations for specific fields. If you want to set configurations globally, i.e. for all vector fields in your documents, you can do that using <code>DBConfig</code> or passing it at <code>__init__</code>:</p> <pre><code>import numpy as np\ndb = HnswDocumentIndex[MyDoc](\nwork_dir='./tmp_7',\ndefault_column_config={\nnp.ndarray: {\n'dim': -1,\n'index': True,\n'space': 'ip',\n'max_elements': 2048,\n'ef_construction': 100,\n'ef': 15,\n'M': 8,\n'allow_replace_deleted': True,\n'num_threads': 5,\n},\nNone: {},\n},\n)\n</code></pre> <p>This will set the default configuration for all vector fields to the one specified in the example above.</p> <p>Note</p> <p>Even if your vectors come from PyTorch or TensorFlow, you can (and should) still use the <code>np.ndarray</code> configuration. This is because all tensors are converted to <code>np.ndarray</code> under the hood.</p> <p>Note</p> <p>max_elements is considered to have the initial maximum capacity of the index. However, the capacity of the index is doubled every time    that the number of Documents in the index exceeds this capacity. Expanding the capacity is an expensive operation, therefore it can be important to    choose an appropiate max_elements value at init time.</p> <p>For more information on these settings, see below.</p> <p>Fields that are not vector fields (e.g. of type <code>str</code> or <code>int</code> etc.) do not offer any configuration, as they are simply stored as-is in a SQLite database.</p>"},{"location":"user_guide/storing/index_hnswlib/#field-wise-configuration","title":"Field-wise configuration","text":"<p>There are various setting that you can tweak for every vector field that you index into Hnswlib.</p> <p>You pass all of those using the <code>field: Type = Field(...)</code> syntax:</p> <pre><code>from pydantic import Field\nclass Schema(BaseDoc):\ntens: NdArray[100] = Field(max_elements=12, space='cosine')\ntens_two: NdArray[10] = Field(M=4, space='ip')\ndb = HnswDocumentIndex[Schema](work_dir='./tmp_8')\n</code></pre> <p>In the example above you can see how to configure two different vector fields, with two different sets of settings.</p> <p>In this way, you can pass all options that Hnswlib supports:</p> Keyword Description Default <code>max_elements</code> Maximum number of vector that can be stored 1024 <code>space</code> Vector space (distance metric) the index operates in. Supports 'l2', 'ip', and 'cosine'. Note: In contrast to the other backends, for HnswDocumentIndex <code>'cosine'</code> refers to cosine distance, not cosine similarity. To transform one to the other, you can use: <code>cos_sim = 1 - cos_dist</code>. For more details see here. 'l2' <code>index</code> Whether or not an index should be built for this field. True <code>ef_construction</code> defines a construction time/accuracy trade-off 200 <code>ef</code> parameter controlling query time/accuracy trade-off 10 <code>M</code> parameter that defines the maximum number of outgoing connections in the graph 16 <code>allow_replace_deleted</code> enables replacing of deleted elements with new added ones True <code>num_threads</code> sets the number of cpu threads to use 1 <p>Note</p> <p>In HnswLibDocIndex  <code>space='cosine'</code> refers to cosine distance, not to cosine similarity, as it does for the other backends. </p> <p>You can find more details on the parameters here.</p>"},{"location":"user_guide/storing/index_hnswlib/#database-location","title":"Database location","text":"<p>For <code>HnswDocumentIndex</code> you need to specify a <code>work_dir</code> where the data will be stored; for other backends you usually specify a <code>host</code> and a <code>port</code> instead.</p> <p>In addition to a host and a port, most backends can also take an <code>index_name</code>, <code>table_name</code>, <code>collection_name</code> or similar. This specifies the name of the index/table/collection that will be created in the database. You don't have to specify this though: By default, this name will be taken from the name of the Document type that you use as schema. For example, for <code>WeaviateDocumentIndex[MyDoc](...)</code> the data will be stored in a Weaviate Class of name <code>MyDoc</code>.</p> <p>In any case, if the location does not yet contain any data, we start from a blank slate. If the location already contains data from a previous session, it will be accessible through the Document Index.</p>"},{"location":"user_guide/storing/index_hnswlib/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_in_memory/","title":"In-Memory Document Index","text":""},{"location":"user_guide/storing/index_in_memory/#in-memory-document-index","title":"In-Memory Document Index","text":"<p>InMemoryExactNNIndex stores all documents in memory using DocLists.  It is a great starting point for small datasets, where you may not want to launch a database server.</p> <p>For vector search and filtering InMemoryExactNNIndex  utilizes DocArray's <code>find()</code> and <code>filter_docs()</code> functions.</p> <p>Production readiness</p> <p>InMemoryExactNNIndex is a great starting point for small- to medium-sized datasets, but it is not battle tested in production. If scalability, uptime, etc. are important to you, we recommend you eventually transition to one of our database-backed Document Index implementations:</p> <ul> <li>QdrantDocumentIndex</li> <li>WeaviateDocumentIndex</li> <li>ElasticDocumentIndex</li> <li>RedisDocumentIndex</li> <li>MilvusDocumentIndex</li> </ul>"},{"location":"user_guide/storing/index_in_memory/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of InMemoryExactNNIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of InMemoryExactNNIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new InMemoryExactNNIndex instance and add the documents to the index.\ndoc_index = InMemoryExactNNIndex[MyDoc]()\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs, scores = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#initialize","title":"Initialize","text":"<p>To create a Document Index, you first need a document class that defines the schema of your index:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\nembedding: NdArray[128]\ntext: str\ndb = InMemoryExactNNIndex[MyDoc]()\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>InMemoryExactNNIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_in_memory/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import InMemoryExactNNIndex\nclass MyDoc(TextDoc):\nembedding: NdArray[128]\ndb = InMemoryExactNNIndex[MyDoc]()\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import InMemoryExactNNIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128)\ndb = InMemoryExactNNIndex[MyDoc]()\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[MyDoc](\n[\nMyDoc(text='hello world', embedding=np.random.rand(128)),\nMyDoc(text='hello world', embedding=np.random.rand(128)),\nMyDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndb.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>import numpy as np\nfrom docarray import DocList\n# create some random data\ndocs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'text {i}') for i in range(100)]\n)\n# index the data\ndb.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all Documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[MyDoc]</code> and <code>InMemoryExactNNIndex[MyDoc]</code> both have <code>MyDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_in_memory/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can use the <code>find()</code> function with a document of the type <code>MyDoc</code>  to find similar documents within the Document Index:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = MyDoc(embedding=np.random.rand(128), text='query')\n# find similar documents\nmatches, scores = db.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = db.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>To peform a vector search, you need to specify a <code>search_field</code>. This is the field that serves as the basis of comparison between your query and the documents in the Document Index.</p> <p>In this example you only have one field (<code>embedding</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_in_memory/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# find similar documents\nmatches, scores = db.find_batched(queries, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = db.find_batched(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_in_memory/#filter","title":"Filter","text":"<p>To filter Documents, the <code>InMemoryExactNNIndex</code> uses DocArray's <code>filter_docs()</code> function.</p> <p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of DocArray's <code>filter_docs()</code> function.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Book(BaseDoc):\ntitle: str\nprice: int\nbooks = DocList[Book]([Book(title=f'title {i}', price=i * 10) for i in range(10)])\nbook_index = InMemoryExactNNIndex[Book](books)\n# filter for books that are cheaper than 29 dollars\nquery = {'price': {'$lte': 29}}\ncheap_books = book_index.filter(query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#text-search","title":"Text search","text":"<p>Note</p> <p>The InMemoryExactNNIndex implementation does not support text search.</p> <p>To see how to perform text search, you can check out other backends that offer support.</p> <p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p>"},{"location":"user_guide/storing/index_in_memory/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <pre><code># Define the document schema.\nclass SimpleSchema(BaseDoc):\nyear: int\nprice: int\nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[SimpleSchema](SimpleSchema(year=2000-i, price=i, embedding=np.random.rand(128)) for i in range(10))\ndoc_index = InMemoryExactNNIndex[SimpleSchema](docs)\nquery = (\ndoc_index.build_query()  # get empty query object\n.filter(filter_query={'year': {'$gt': 1994}})  # pre-filtering\n.find(query=np.random.rand(128), search_field='embedding')  # add vector similarity search\n.filter(filter_query={'price': {'$lte': 3}})  # post-filtering\n.build()\n)\n# execute the combined query and return the results\nresults = doc_index.execute_query(query)\nprint(f'{results=}')\n</code></pre> <p>In the example above you can see how to form a hybrid query that combines vector similarity search and filtered search to obtain a combined set of results.</p> <p>The kinds of atomic queries that can be combined in this way depends on the backend. Some backends can combine text search and vector search, while others can perform filters and vectors search, etc.</p>"},{"location":"user_guide/storing/index_in_memory/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndoc = db[ids[0]]  # get by single id\ndocs = db[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access Documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndel db[ids[0]]  # del by single id\ndel db[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#update-documents","title":"Update documents","text":"<p>In order to update a Document inside the index, you only need to re-index it with the updated attributes.</p> <p>First, let's create a schema for our Document Index: </p><pre><code>import numpy as np\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom docarray.index import InMemoryExactNNIndex\nclass MyDoc(BaseDoc):\ntext: str\nembedding: NdArray[128]\n</code></pre> <p>Now, we can instantiate our Index and add some data: </p><pre><code>docs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'I am the first version of Document {i}') for i in range(100)]\n)\nindex = InMemoryExactNNIndex[MyDoc]()\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>Let's retrieve our data and check its content: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the first version' in doc.text\n</code></pre> <p>Then, let's update all of the text of these documents and re-index them: </p><pre><code>for i, doc in enumerate(docs):\ndoc.text = f'I am the second version of Document {i}'\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>When we retrieve them again we can see that their text attribute has been updated accordingly </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the second version' in doc.text\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#configuration","title":"Configuration","text":"<p>This section lays out the configurations and options that are specific to InMemoryExactNNIndex.</p> <p>The <code>DBConfig</code> of InMemoryExactNNIndex contains two entries: <code>index_file_path</code> and <code>default_column_mapping</code>, the default mapping from Python types to column configurations.</p> <p>You can see in the section below how to override configurations for specific fields. If you want to set configurations globally, i.e. for all vector fields in your Documents, you can do that using <code>DBConfig</code> or passing it at <code>__init__</code>::</p> <pre><code>from collections import defaultdict\nfrom docarray.typing.tensor.abstract_tensor import AbstractTensor\nnew_doc_index = InMemoryExactNNIndex[MyDoc](\ndefault_column_config=defaultdict(\ndict,\n{\nAbstractTensor: {'space': 'cosine_sim'},\n},\n)\n)\n</code></pre> <p>This will set the default configuration for all vector fields to the one specified in the example above.</p> <p>For more information on these settings, see below.</p> <p>Fields that are not vector fields (e.g. of type <code>str</code> or <code>int</code> etc.) do not offer any configuration.</p>"},{"location":"user_guide/storing/index_in_memory/#field-wise-configuration","title":"Field-wise configuration","text":"<p>For a vector field you can adjust the <code>space</code> parameter. It can be one of:</p> <ul> <li><code>'cosine_sim'</code> (default)</li> <li><code>'euclidean_dist'</code></li> <li><code>'sqeuclidean_dist'</code></li> </ul> <p>You pass it using the <code>field: Type = Field(...)</code> syntax:</p> <pre><code>from docarray import BaseDoc\nfrom pydantic import Field\nclass Schema(BaseDoc):\ntensor_1: NdArray[100] = Field(space='euclidean_dist')\ntensor_2: NdArray[100] = Field(space='sqeuclidean_dist')\n</code></pre> <p>In the example above you can see how to configure two different vector fields, with two different sets of settings.</p>"},{"location":"user_guide/storing/index_in_memory/#persist-and-load","title":"Persist and Load","text":"<p>You can pass an <code>index_file_path</code> argument to make sure that the index can be restored if persisted from that specific file. </p><pre><code>doc_index = InMemoryExactNNIndex[MyDoc](index_file_path='docs.bin')\ndoc_index.index(docs)\ndoc_index.persist()\n# Initialize a new document index using the saved binary file\nnew_doc_index = InMemoryExactNNIndex[MyDoc](index_file_path='docs.bin')\n</code></pre>"},{"location":"user_guide/storing/index_in_memory/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_milvus/","title":"Milvus Document Index","text":""},{"location":"user_guide/storing/index_milvus/#milvus-document-index","title":"Milvus Document Index","text":"<p>Install dependencies</p> <p>To use MilvusDocumentIndex, you need to install extra dependencies with the following command:</p> <pre><code>pip install \"docarray[milvus]\"\n</code></pre> <p>This is the user guide for the MilvusDocumentIndex, focusing on special features and configurations of Milvus.</p>"},{"location":"user_guide/storing/index_milvus/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of MilvusDocumentIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of MilvusDocumentIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <p>Single Search Field Requirement</p> <p>In order to utilize vector search, it's necessary to define 'is_embedding' for one field only.  This is due to Milvus' configuration, which permits a single vector for each data object.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import MilvusDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128] = Field(is_embedding=True)\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new MilvusDocumentIndex instance and add the documents to the index.\ndoc_index = MilvusDocumentIndex[MyDoc](index_name='tmp_index_1')\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs = doc_index.find(query, limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#initialize","title":"Initialize","text":"<p>First of all, you need to install and run Milvus. Download <code>docker-compose.yml</code> with the following command:</p> <pre><code>wget https://github.com/milvus-io/milvus/releases/download/v2.2.11/milvus-standalone-docker-compose.yml -O docker-compose.yml\n</code></pre> <p>And start Milvus by running: </p><pre><code>sudo docker-compose up -d\n</code></pre> <p>Learn more on Milvus documentation.</p> <p>Next, you can create a MilvusDocumentIndex instance using:</p> <pre><code>from docarray import BaseDoc\nfrom docarray.index import MilvusDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nclass MyDoc(BaseDoc):\nembedding: NdArray[128] = Field(is_embedding=True)\ntext: str\ndoc_index = MilvusDocumentIndex[MyDoc](index_name='tmp_index_2')\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>MilvusDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_milvus/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined Documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import MilvusDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: NdArray[128] = Field(is_embedding=True)\ndoc_index = MilvusDocumentIndex[MyDoc](index_name='tmp_index_3')\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import MilvusDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128, is_embedding=True)\ndoc_index = MilvusDocumentIndex[MyDoc](index_name='tmp_index_4')\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>import numpy as np\nfrom docarray import DocList\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128] = Field(is_embedding=True)\ndoc_index = MilvusDocumentIndex[MyDoc](index_name='tmp_index_5')\n# create some random data\ndocs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), title=f'text {i}') for i in range(100)]\n)\n# index the data\ndoc_index.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all Documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[MyDoc]</code> and <code>MilvusDocumentIndex[MyDoc]</code> both have <code>MyDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined Documents into a Document Index.</p>"},{"location":"user_guide/storing/index_milvus/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can perform a similarity search and find relevant documents by passing <code>MyDoc</code> or a raw vector to  the <code>find()</code> method:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = MyDoc(embedding=np.random.rand(128), title='query')\n# find similar documents\nmatches, scores = doc_index.find(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches.title=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = doc_index.find(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches.title=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_milvus/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_milvus/#filter","title":"Filter","text":"<p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of the Milvus.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Book(BaseDoc):\nprice: int\nembedding: NdArray[10] = Field(is_embedding=True)\nbooks = DocList[Book]([Book(price=i * 10, embedding=np.random.rand(10)) for i in range(10)])\nbook_index = MilvusDocumentIndex[Book](index_name='tmp_index_6')\nbook_index.index(books)\n# filter for books that are cheaper than 29 dollars\nquery = 'price &lt; 29'\ncheap_books = book_index.filter(filter_query=query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#text-search","title":"Text search","text":"<p>Note</p> <p>The MilvusDocumentIndex implementation does not support text search.</p> <p>To see how to perform text search, you can check out other backends that offer support.</p> <p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p>"},{"location":"user_guide/storing/index_milvus/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <pre><code># Define the document schema.\nclass SimpleSchema(BaseDoc):\nprice: int\nembedding: NdArray[128] = Field(is_embedding=True)\n# Create dummy documents.\ndocs = DocList[SimpleSchema](SimpleSchema(price=i, embedding=np.random.rand(128)) for i in range(10))\ndoc_index = MilvusDocumentIndex[SimpleSchema](index_name='tmp_index_7')\ndoc_index.index(docs)\nquery = (\ndoc_index.build_query()  # get empty query object\n.find(query=np.random.rand(128))  # add vector similarity search\n.filter(filter_query='price &lt; 3')  # add filter search\n.build()\n)\n# execute the combined query and return the results\nresults = doc_index.execute_query(query)\nprint(f'{results=}')\n</code></pre> <p>In the example above you can see how to form a hybrid query that combines vector similarity search and filtered search to obtain a combined set of results.</p> <p>The kinds of atomic queries that can be combined in this way depends on the backend. Some backends can combine text search and vector search, while others can perform filters and vectors search, etc.</p>"},{"location":"user_guide/storing/index_milvus/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[SimpleSchema](\nSimpleSchema(embedding=np.random.rand(128), price=i) for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the Documents by id\ndoc = doc_index[ids[0]]  # get by single id\ndocs = doc_index[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access Documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[SimpleSchema](\nSimpleSchema(embedding=np.random.rand(128), price=i) for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the Documents by id\ndel doc_index[ids[0]]  # del by single id\ndel doc_index[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#configuration","title":"Configuration","text":"<p>This section lays out the configurations and options that are specific to MilvusDocumentIndex.</p>"},{"location":"user_guide/storing/index_milvus/#dbconfig","title":"DBConfig","text":"<p>The following configs can be set in <code>DBConfig</code>:</p> Name Description Default <code>host</code> The host address for the Milvus server. <code>localhost</code> <code>port</code> The port number for the Milvus server 19530 <code>index_name</code> The name of the index in the Milvus database <code>None</code>. Data will be stored in an index named after the Document type used as schema. <code>user</code> The username for the Milvus server <code>None</code> <code>password</code> The password for the Milvus server <code>None</code> <code>token</code> Token for secure connection '' <code>collection_description</code> Description of the collection in the database '' <code>default_column_config</code> The default configurations for every column type. dict <p>You can pass any of the above as keyword arguments to the <code>__init__()</code> method or pass an entire configuration object.</p>"},{"location":"user_guide/storing/index_milvus/#field-wise-configuration","title":"Field-wise configuration","text":"<p><code>default_column_config</code> is the default configurations for every column type. Since there are many column types in Milvus, you can also consider changing the column config when defining the schema.</p> <pre><code>class SimpleDoc(BaseDoc):\ntensor: NdArray[128] = Field(is_embedding=True, index_type='IVF_FLAT', metric_type='L2')\ndoc_index = MilvusDocumentIndex[SimpleDoc](index_name='tmp_index_10')\n</code></pre>"},{"location":"user_guide/storing/index_milvus/#runtimeconfig","title":"RuntimeConfig","text":"<p>The <code>RuntimeConfig</code> dataclass of <code>MilvusDocumentIndex</code> consists of <code>batch_size</code> index/get/del operations.  You can change <code>batch_size</code> in the following way:</p> <pre><code>doc_index = MilvusDocumentIndex[SimpleDoc]()\ndoc_index.configure(MilvusDocumentIndex.RuntimeConfig(batch_size=128))\n</code></pre> <p>You can pass the above as keyword arguments to the <code>configure()</code> method or pass an entire configuration object.</p>"},{"location":"user_guide/storing/index_milvus/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_qdrant/","title":"Qdrant Document Index","text":""},{"location":"user_guide/storing/index_qdrant/#qdrant-document-index","title":"Qdrant Document Index","text":"<p>Install dependencies</p> <p>To use QdrantDocumentIndex, you need to install extra dependencies with the following command:</p> <pre><code>pip install \"docarray[qdrant]\"\n</code></pre> <p>The following is a starter script for using the QdrantDocumentIndex, based on the Qdrant vector search engine.</p>"},{"location":"user_guide/storing/index_qdrant/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of QdrantDocumentIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of QdrantDocumentIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import QdrantDocumentIndex\nfrom docarray.typing import NdArray\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new QdrantDocumentIndex instance and add the documents to the index.\ndoc_index = QdrantDocumentIndex[MyDoc](host='localhost')\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs, scores = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#initialize","title":"Initialize","text":"<p>You can initialize QdrantDocumentIndex in three different ways:</p> <p>Connecting to a local Qdrant instance running as a Docker container</p> <p>You can use docker-compose to create a local Qdrant service with the following <code>docker-compose.yml</code>.</p> <pre><code>version: '3.8'\nservices:\nqdrant:\nimage: qdrant/qdrant:v1.1.2\nports:\n- \"6333:6333\"\n- \"6334:6334\"\nulimits: # Only required for tests, as there are a lot of collections created\nnofile:\nsoft: 65535\nhard: 65535\n</code></pre> <p>Run the following command in the folder of the above <code>docker-compose.yml</code> to start the service:</p> <pre><code>docker-compose up\n</code></pre> <p>Next, you can create a QdrantDocumentIndex instance using:</p> <pre><code>qdrant_config = QdrantDocumentIndex.DBConfig('localhost')\ndoc_index = QdrantDocumentIndex[MyDoc](qdrant_config)\n# or just\ndoc_index = QdrantDocumentIndex[MyDoc](host='localhost')\n</code></pre> <p>Creating an in-memory Qdrant document index </p><pre><code>qdrant_config = QdrantDocumentIndex.DBConfig(location=\":memory:\")\ndoc_index = QdrantDocumentIndex[MyDoc](qdrant_config)\n</code></pre> <p>Connecting to Qdrant Cloud service </p><pre><code>qdrant_config = QdrantDocumentIndex.DBConfig(\n\"https://YOUR-CLUSTER-URL.aws.cloud.qdrant.io\", \napi_key=\"&lt;your-api-key&gt;\",\n)\ndoc_index = QdrantDocumentIndex[MyDoc](qdrant_config)\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>QdrantDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_qdrant/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import QdrantDocumentIndex\nclass MyDoc(TextDoc):\nembedding: NdArray[128]\ndoc_index = QdrantDocumentIndex[MyDoc](host='localhost')\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import QdrantDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128)\ndoc_index = QdrantDocumentIndex[MyDoc](host='localhost')\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[TextDoc](\n[\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndoc_index.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>import numpy as np\nfrom docarray import DocList\n# create some random data\ndocs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'text {i}') for i in range(100)]\n)\n# index the data\ndoc_index.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[MyDoc]</code> and <code>QdrantDocumentIndex[MyDoc]</code> both have <code>MyDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_qdrant/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can perform a similarity search and find relevant documents by passing <code>MyDoc</code> or a raw vector to  the <code>find()</code> method:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = MyDoc(embedding=np.random.rand(128), text='query')\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>To peform a vector search, you need to specify a <code>search_field</code>. This is the field that serves as the basis of comparison between your query and the documents in the Document Index.</p> <p>In this example you only have one field (<code>embedding</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_qdrant/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_qdrant/#filter","title":"Filter","text":"<p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of Qdrant.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom qdrant_client.http import models as rest\nclass Book(BaseDoc):\ntitle: str\nprice: int\nbooks = DocList[Book]([Book(title=f'title {i}', price=i * 10) for i in range(10)])\nbook_index = QdrantDocumentIndex[Book]()\nbook_index.index(books)\n# filter for books that are cheaper than 29 dollars\nquery = rest.Filter(\nmust=[rest.FieldCondition(key='price', range=rest.Range(lt=29))]\n)\ncheap_books = book_index.filter(filter_query=query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#text-search","title":"Text search","text":"<p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p> <p>You can use text search directly on the field of type <code>str</code>:</p> <pre><code>class NewsDoc(BaseDoc):\ntext: str\ndoc_index = QdrantDocumentIndex[NewsDoc](host='localhost')\nindex_docs = [\nNewsDoc(id='0', text='this is a news for sport'),\nNewsDoc(id='1', text='this is a news for finance'),\nNewsDoc(id='2', text='this is another news for sport'),\n]\ndoc_index.index(index_docs)\nquery = 'finance'\n# search with text\ndocs, scores = doc_index.text_search(query, search_field='text')\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <p>For example, you can build a hybrid serach query that performs range filtering, vector search and text search:</p> <pre><code>class SimpleDoc(BaseDoc):\ntens: NdArray[10]\nnum: int\ntext: str\ndoc_index = QdrantDocumentIndex[SimpleDoc](host='localhost')\nindex_docs = [\nSimpleDoc(id=f'{i}', tens=np.ones(10) * i, num=int(i / 2), text=f'Lorem ipsum {int(i/2)}')\nfor i in range(10)\n]\ndoc_index.index(index_docs)\nfind_query = np.ones(10)\ntext_search_query = 'ipsum 1'\nfilter_query = rest.Filter(\nmust=[\nrest.FieldCondition(\nkey='num',\nrange=rest.Range(\ngte=1,\nlt=5,\n),\n)\n]\n)\nquery = (\ndoc_index.build_query()\n.find(find_query, search_field='tens')\n.text_search(text_search_query, search_field='text')\n.filter(filter_query)\n.build(limit=5)\n)\ndocs = doc_index.execute_query(query)\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#access-documents","title":"Access documents","text":"<p>To access a document, you need to specify its <code>id</code>. You can also pass a list of <code>id</code>s to access multiple documents.</p> <pre><code># access a single Doc\ndoc_index[index_docs[16].id]\n# access multiple Docs\ndoc_index[index_docs[16].id, index_docs[17].id]\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#delete-documents","title":"Delete documents","text":"<p>To delete documents, use the built-in function <code>del</code> with the <code>id</code> of the documents that you want to delete. You can also pass a list of <code>id</code>s to delete multiple documents.</p> <pre><code># delete a single Doc\ndel doc_index[index_docs[16].id]\n# delete multiple Docs\ndel doc_index[index_docs[17].id, index_docs[18].id]\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#update-documents","title":"Update documents","text":"<p>In order to update a Document inside the index, you only need to re-index it with the updated attributes.</p> <p>First, let's create a schema for our Document Index: </p><pre><code>import numpy as np\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom docarray.index import QdrantDocumentIndex\nclass MyDoc(BaseDoc):\ntext: str\nembedding: NdArray[128]\n</code></pre> <p>Now, we can instantiate our Index and add some data: </p><pre><code>docs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(10), text=f'I am the first version of Document {i}') for i in range(100)]\n)\nindex = QdrantDocumentIndex[MyDoc]()\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>Let's retrieve our data and check its content: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the first version' in doc.text\n</code></pre> <p>Then, let's update all of the text of these documents and re-index them: </p><pre><code>for i, doc in enumerate(docs):\ndoc.text = f'I am the second version of Document {i}'\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>When we retrieve them again we can see that their text attribute has been updated accordingly: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the second version' in doc.text\n</code></pre>"},{"location":"user_guide/storing/index_qdrant/#configuration","title":"Configuration","text":"<p>!!! tip \"See all configuration options\" To see all configuration options for the QdrantDocumentIndex, you can do the following:</p> <pre><code>from docarray.index import QdrantDocumentIndex\n# the following can be passed to the __init__() method\ndb_config = QdrantDocumentIndex.DBConfig()\nprint(db_config)  # shows default values\n# the following can be passed to the configure() method\nruntime_config = QdrantDocumentIndex.RuntimeConfig()\nprint(runtime_config)  # shows default values\n</code></pre> <p>Note that the collection_name from the DBConfig is an Optional[str] with <code>None</code> as default value. This is because the QdrantDocumentIndex will take the name the Document type that you use as schema. For example, for QdrantDocumentIndexMyDoc  the data will be stored in a collection name MyDoc if no specific collection_name is passed in the DBConfig.</p>"},{"location":"user_guide/storing/index_qdrant/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_redis/","title":"Redis Document Index","text":""},{"location":"user_guide/storing/index_redis/#redis-document-index","title":"Redis Document Index","text":"<p>Install dependencies</p> <p>To use RedisDocumentIndex, you need to install extra dependencies with the following command:</p> <pre><code>pip install \"docarray[redis]\"\n</code></pre> <p>This is the user guide for the RedisDocumentIndex, focusing on special features and configurations of Redis.</p>"},{"location":"user_guide/storing/index_redis/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of RedisDocumentIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of RedisDocumentIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import RedisDocumentIndex\nfrom docarray.typing import NdArray\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new RedisDocumentIndex instance and add the documents to the index.\ndoc_index = RedisDocumentIndex[MyDoc](host='localhost')\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs = doc_index.find(query, search_field='embedding', limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_redis/#initialize","title":"Initialize","text":"<p>Before initializing RedisDocumentIndex,  make sure that you have a Redis service that you can connect to. </p> <p>You can create a local Redis service with the following command:</p> <p></p><pre><code>docker run --name redis-stack-server -p 6379:6379 -d redis/redis-stack-server:7.2.0-RC2\n</code></pre> Next, you can create RedisDocumentIndex: <pre><code>from docarray import BaseDoc\nfrom docarray.index import RedisDocumentIndex\nfrom docarray.typing import NdArray\nclass MyDoc(BaseDoc):\nembedding: NdArray[128]\ntext: str\ndoc_index = RedisDocumentIndex[MyDoc](host='localhost')\n</code></pre>"},{"location":"user_guide/storing/index_redis/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>RedisDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_redis/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined Documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined Documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import RedisDocumentIndex\nclass MyDoc(TextDoc):\nembedding: NdArray[128]\ndoc_index = RedisDocumentIndex[MyDoc]()\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import RedisDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128)\ndoc_index = RedisDocumentIndex[MyDoc]()\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[TextDoc](\n[\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndoc_index.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_redis/#index","title":"Index","text":"<p>Now that you have a Document Index, you can add data to it, using the <code>index()</code> method:</p> <pre><code>import numpy as np\nfrom docarray import DocList\n# create some random data\ndocs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'text {i}') for i in range(100)]\n)\n# index the data\ndoc_index.index(docs)\n</code></pre> <p>That call to <code>index()</code> stores all Documents in <code>docs</code> in the Document Index, ready to be retrieved in the next step.</p> <p>As you can see, <code>DocList[MyDoc]</code> and <code>RedisDocumentIndex[MyDoc]</code> both have <code>MyDoc</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined Documents into a Document Index.</p>"},{"location":"user_guide/storing/index_redis/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can perform a similarity search and find relevant documents by passing <code>MyDoc</code> or a raw vector to  the <code>find()</code> method:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = MyDoc(embedding=np.random.rand(128), text='query')\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(128)\n# find similar documents\nmatches, scores = doc_index.find(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>To peform a vector search, you need to specify a <code>search_field</code>. This is the field that serves as the basis of comparison between your query and the documents in the Document Index.</p> <p>In this example you only have one field (<code>embedding</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_redis/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 128)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, search_field='embedding', limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_redis/#filter","title":"Filter","text":"<p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of the Redis.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nclass Book(BaseDoc):\ntitle: str\nprice: int\nbooks = DocList[Book]([Book(title=f'title {i}', price=i * 10) for i in range(10)])\nbook_index = RedisDocumentIndex[Book]()\nbook_index.index(books)\n# filter for books that are cheaper than 29 dollars\nquery = '@price:[-inf 29]'\ncheap_books = book_index.filter(filter_query=query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_redis/#text-search","title":"Text search","text":"<p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p> <p>You can use text search directly on the field of type <code>str</code>:</p> <pre><code>class NewsDoc(BaseDoc):\ntext: str\ndoc_index = RedisDocumentIndex[NewsDoc]()\nindex_docs = [\nNewsDoc(id='0', text='this is a news for sport'),\nNewsDoc(id='1', text='this is a news for finance'),\nNewsDoc(id='2', text='this is another news for sport'),\n]\ndoc_index.index(index_docs)\nquery = 'finance'\n# search with text\ndocs, scores = doc_index.text_search(query, search_field='text')\n</code></pre>"},{"location":"user_guide/storing/index_redis/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>:</p> <pre><code># Define the document schema.\nclass SimpleSchema(BaseDoc):\nprice: int\nembedding: NdArray[128]\n# Create dummy documents.\ndocs = DocList[SimpleSchema](SimpleSchema(price=i, embedding=np.random.rand(128)) for i in range(10))\ndoc_index = RedisDocumentIndex[SimpleSchema](host='localhost')\ndoc_index.index(docs)\nquery = (\ndoc_index.build_query()  # get empty query object\n.find(query=np.random.rand(128), search_field='embedding')  # add vector similarity search\n.filter(filter_query='@price:[-inf 3]')  # add filter search\n.build()\n)\n# execute the combined query and return the results\nresults = doc_index.execute_query(query)\nprint(f'{results=}')\n</code></pre> <p>In the example above you can see how to form a hybrid query that combines vector similarity search and filtered search to obtain a combined set of results.</p> <p>The kinds of atomic queries that can be combined in this way depends on the backend. Some backends can combine text search and vector search, while others can perform filters and vectors search, etc.</p>"},{"location":"user_guide/storing/index_redis/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndoc = db[ids[0]]  # get by single id\ndocs = db[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_redis/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access Documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), text=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndb.index(data)\n# access the Documents by id\ndel db[ids[0]]  # del by single id\ndel db[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_redis/#update-documents","title":"Update documents","text":"<p>In order to update a Document inside the index, you only need to re-index it with the updated attributes.</p> <p>First, let's create a schema for our Document Index: </p><pre><code>import numpy as np\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom docarray.index import RedisDocumentIndex\nclass MyDoc(BaseDoc):\ntext: str\nembedding: NdArray[128]\n</code></pre> <p>Now, we can instantiate our Index and add some data: </p><pre><code>docs = DocList[MyDoc](\n[MyDoc(embedding=np.random.rand(128), text=f'I am the first version of Document {i}') for i in range(100)]\n)\nindex = RedisDocumentIndex[MyDoc]()\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>Let's retrieve our data and check its content: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the first version' in doc.text\n</code></pre> <p>Then, let's update all of the text of these documents and re-index them: </p><pre><code>for i, doc in enumerate(docs):\ndoc.text = f'I am the second version of Document {i}'\nindex.index(docs)\nassert index.num_docs() == 100\n</code></pre> <p>When we retrieve them again we can see that their text attribute has been updated accordingly: </p><pre><code>res = index.find(query=docs[0], search_field='embedding', limit=100)\nassert len(res.documents) == 100\nfor doc in res.documents:\nassert 'I am the second version' in doc.text\n</code></pre>"},{"location":"user_guide/storing/index_redis/#configuration","title":"Configuration","text":"<p>This section lays out the configurations and options that are specific to RedisDocumentIndex.</p>"},{"location":"user_guide/storing/index_redis/#dbconfig","title":"DBConfig","text":"<p>The following configs can be set in <code>DBConfig</code>:</p> Name Description Default <code>host</code> The host address for the Redis server. <code>localhost</code> <code>port</code> The port number for the Redis server 6379 <code>index_name</code> The name of the index in the Redis database <code>None</code>. Data will be stored in an index named after the Document type used as schema. <code>username</code> The username for the Redis server <code>None</code> <code>password</code> The password for the Redis server <code>None</code> <code>text_scorer</code> The method for scoring text during text search <code>BM25</code> <code>default_column_config</code> The default configurations for every column type. dict <p>You can pass any of the above as keyword arguments to the <code>__init__()</code> method or pass an entire configuration object.</p>"},{"location":"user_guide/storing/index_redis/#field-wise-configuration","title":"Field-wise configuration","text":"<p><code>default_column_config</code> is the default configurations for every column type. Since there are many column types in Redis, you can also consider changing the column config when defining the schema.</p> <pre><code>class SimpleDoc(BaseDoc):\ntensor: NdArray[128] = Field(algorithm='FLAT', distance='COSINE')\ndoc_index = RedisDocumentIndex[SimpleDoc]()\n</code></pre>"},{"location":"user_guide/storing/index_redis/#runtimeconfig","title":"RuntimeConfig","text":"<p>The <code>RuntimeConfig</code> dataclass of <code>RedisDocumentIndex</code> consists of <code>batch_size</code> index/get/del operations.  You can change <code>batch_size</code> in the following way:</p> <pre><code>doc_index = RedisDocumentIndex[SimpleDoc]()\ndoc_index.configure(RedisDocumentIndex.RuntimeConfig(batch_size=128))\n</code></pre> <p>You can pass the above as keyword arguments to the <code>configure()</code> method or pass an entire configuration object.</p>"},{"location":"user_guide/storing/index_redis/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/index_weaviate/","title":"Weaviate Document Index","text":""},{"location":"user_guide/storing/index_weaviate/#weaviate-document-index","title":"Weaviate Document Index","text":"<p>Install dependencies</p> <p>To use WeaviateDocumentIndex, you need to install extra dependencies with the following command:</p> <pre><code>pip install \"docarray[weaviate]\"\n</code></pre> <p>This is the user guide for the WeaviateDocumentIndex, focusing on special features and configurations of Weaviate.</p>"},{"location":"user_guide/storing/index_weaviate/#basic-usage","title":"Basic usage","text":"<p>This snippet demonstrates the basic usage of WeaviateDocumentIndex. It defines a document schema with a title and an embedding,  creates ten dummy documents with random embeddings, initializes an instance of WeaviateDocumentIndex to index these documents,  and performs a vector similarity search to retrieve the ten most similar documents to a given query vector.</p> <p>Single Search Field Requirement</p> <p>In order to utilize vector search, it's necessary to define 'is_embedding' for one field only.  This is due to Weaviate's configuration, which permits a single vector for each data object.</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.index import WeaviateDocumentIndex\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\n# Define the document schema.\nclass MyDoc(BaseDoc):\ntitle: str \nembedding: NdArray[128] = Field(is_embedding=True)\n# Create dummy documents.\ndocs = DocList[MyDoc](MyDoc(title=f'title #{i}', embedding=np.random.rand(128)) for i in range(10))\n# Initialize a new WeaviateDocumentIndex instance and add the documents to the index.\ndoc_index = WeaviateDocumentIndex[MyDoc]()\ndoc_index.index(docs)\n# Perform a vector search.\nquery = np.ones(128)\nretrieved_docs = doc_index.find(query, limit=10)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#initialize","title":"Initialize","text":""},{"location":"user_guide/storing/index_weaviate/#start-weaviate-service","title":"Start Weaviate service","text":"<p>To use WeaviateDocumentIndex, DocArray needs to hook into a running Weaviate service. There are multiple ways to start a Weaviate instance, depending on your use case.</p> <p>Options - Overview</p> Instance type General use case Configurability Notes Weaviate Cloud Services (WCS) Development and production Limited Recommended for most users Embedded Weaviate Experimentation Limited Experimental (as of Apr 2023) Docker-Compose Development Yes Recommended for development + customizability Kubernetes Production Yes"},{"location":"user_guide/storing/index_weaviate/#instantiation-instructions","title":"Instantiation instructions","text":"<p>WCS (managed instance)</p> <p>Go to the WCS console and create an instance using the visual interface, following this guide. </p> <p>Weaviate instances on WCS come pre-configured, so no further configuration is required.</p> <p>Docker-Compose (self-managed)</p> <p>Get a configuration file (<code>docker-compose.yaml</code>). You can build it using this interface, or download it directly with:</p> <pre><code>curl -o docker-compose.yml \"https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?modules=standalone&amp;runtime=docker-compose&amp;weaviate_version=v&lt;WEAVIATE_VERSION&gt;\"\n</code></pre> <p>Where <code>v&lt;WEAVIATE_VERSION&gt;</code> is the actual version, such as <code>v1.18.3</code>.</p> <pre><code>curl -o docker-compose.yml \"https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?modules=standalone&amp;runtime=docker-compose&amp;weaviate_version=v1.18.3\"\n</code></pre> <p>Start up Weaviate with Docker-Compose</p> <p>Then you can start up Weaviate by running from a shell:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Shut down Weaviate</p> <p>Then you can shut down Weaviate by running from a shell:</p> <pre><code>docker-compose down\n</code></pre> <p>Notes</p> <p>Unless data persistence or backups are set up, shutting down the Docker instance will remove all its data. </p> <p>See documentation on Persistent volume and Backups to prevent this if persistence is desired.</p> <pre><code>docker-compose up -d\n</code></pre> <p>Embedded Weaviate (from the application)</p> <p>With Embedded Weaviate, Weaviate database server can be launched from the client, using:</p> <pre><code>from docarray.index.backends.weaviate import EmbeddedOptions\nembedded_options = EmbeddedOptions()\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#authentication","title":"Authentication","text":"<p>Weaviate offers multiple authentication options, as well as authorization options. </p> <p>With DocArray, you can use any of:</p> <ul> <li>Anonymous access (public instance),</li> <li>OIDC with username &amp; password, and</li> <li>API-key based authentication.</li> </ul> <p>To access a Weaviate instance. In general, Weaviate recommends using API-key based authentication for balance between security and ease of use. You can create, for example, read-only keys to distribute to certain users, while providing read/write keys to administrators.</p> <p>See below for examples of connection to Weaviate for each scenario.</p>"},{"location":"user_guide/storing/index_weaviate/#connect-to-weaviate","title":"Connect to Weaviate","text":"<pre><code>from docarray.index.backends.weaviate import WeaviateDocumentIndex\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#public-instance","title":"Public instance","text":"<p>If using Embedded Weaviate:</p> <pre><code>from docarray.index.backends.weaviate import EmbeddedOptions\ndbconfig = WeaviateDocumentIndex.DBConfig(embedded_options=EmbeddedOptions())\n</code></pre> <p>For all other options:</p> <pre><code>dbconfig = WeaviateDocumentIndex.DBConfig(\nhost=\"http://localhost:8080\"\n)  # Replace with your endpoint)\n</code></pre> <p>OIDC with username + password</p> <p>To authenticate against a Weaviate instance with OIDC username &amp; password:</p> <pre><code>dbconfig = WeaviateDocumentIndex.DBConfig(\nusername=\"username\",  # Replace with your username\npassword=\"password\",  # Replace with your password\nhost=\"http://localhost:8080\",  # Replace with your endpoint\n)\n</code></pre> <pre><code># dbconfig = WeaviateDocumentIndex.DBConfig(\n#     username=\"username\",  # Replace with your username\n#     password=\"password\",  # Replace with your password\n#     host=\"http://localhost:8080\",  # Replace with your endpoint\n# )\n</code></pre> <p>API key-based authentication</p> <p>To authenticate against a Weaviate instance an API key:</p> <pre><code>dbconfig = WeaviateDocumentIndex.DBConfig(\nauth_api_key=\"apikey\",  # Replace with your own API key\nhost=\"http://localhost:8080\",  # Replace with your endpoint\n)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#create-an-instance","title":"Create an instance","text":"<p>Let's connect to a local Weaviate service and instantiate a <code>WeaviateDocumentIndex</code> instance: </p><pre><code>dbconfig = WeaviateDocumentIndex.DBConfig(\nhost=\"http://localhost:8080\"\n)\ndoc_index = WeaviateDocumentIndex[MyDoc](db_config=dbconfig)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#schema-definition","title":"Schema definition","text":"<p>In this code snippet, <code>WeaviateDocumentIndex</code> takes a schema of the form of <code>MyDoc</code>. The Document Index then creates a column for each field in <code>MyDoc</code>.</p> <p>The column types in the backend database are determined by the type hints of the document's fields. Optionally, you can customize the database types for every field.</p> <p>Most vector databases need to know the dimensionality of the vectors that will be stored. Here, that is automatically inferred from the type hint of the <code>embedding</code> field: <code>NdArray[128]</code> means that the database will store vectors with 128 dimensions.</p> <p>PyTorch and TensorFlow support</p> <p>Instead of using <code>NdArray</code> you can use <code>TorchTensor</code> or <code>TensorFlowTensor</code> and the Document Index will handle that for you. This is supported for all Document Index backends. No need to convert your tensors to NumPy arrays manually!</p>"},{"location":"user_guide/storing/index_weaviate/#using-a-predefined-document-as-schema","title":"Using a predefined document as schema","text":"<p>DocArray offers a number of predefined documents, like ImageDoc and TextDoc. If you try to use these directly as a schema for a Document Index, you will get unexpected behavior: Depending on the backend, an exception will be raised, or no vector index for ANN lookup will be built.</p> <p>The reason for this is that predefined documents don't hold information about the dimensionality of their <code>.embedding</code> field. But this is crucial information for any vector database to work properly!</p> <p>You can work around this problem by subclassing the predefined document and adding the dimensionality information:</p> Using type hintUsing Field() <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import NdArray\nfrom docarray.index import WeaviateDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: NdArray[128] = Field(is_embedding=True)\ndoc_index = WeaviateDocumentIndex[MyDoc]()\n</code></pre> <pre><code>from docarray.documents import TextDoc\nfrom docarray.typing import AnyTensor\nfrom docarray.index import WeaviateDocumentIndex\nfrom pydantic import Field\nclass MyDoc(TextDoc):\nembedding: AnyTensor = Field(dim=128, is_embedding=True)\ndoc_index = WeaviateDocumentIndex[MyDoc]()\n</code></pre> <p>Once you have defined the schema of your Document Index in this way, the data that you index can be either the predefined Document type or your custom Document type.</p> <p>The next section goes into more detail about data indexing, but note that if you have some <code>TextDoc</code>s, <code>ImageDoc</code>s etc. that you want to index, you don't need to cast them to <code>MyDoc</code>:</p> <pre><code>from docarray import DocList\n# data of type TextDoc\ndata = DocList[TextDoc](\n[\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\nTextDoc(text='hello world', embedding=np.random.rand(128)),\n]\n)\n# you can index this into Document Index of type MyDoc\ndoc_index.index(data)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#index","title":"Index","text":"<p>Putting it together, we can add data below using Weaviate as the Document Index:</p> <pre><code>import numpy as np\nfrom pydantic import Field\nfrom docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom docarray.index.backends.weaviate import WeaviateDocumentIndex\n# Define a document schema\nclass Document(BaseDoc):\ntext: str\nembedding: NdArray[2] = Field(\ndims=2, is_embedding=True\n)  # Embedding column -&gt; vector representation of the document\nfile: NdArray[100] = Field(dims=100)\n# Make a list of 3 docs to index\ndocs = DocList[Document](\n[\nDocument(\ntext=\"Hello world\",\nembedding=np.array([1, 2]),\nfile=np.random.rand(100),\nid=\"1\",\n),\nDocument(\ntext=\"Hello world, how are you?\",\nembedding=np.array([3, 4]),\nfile=np.random.rand(100),\nid=\"2\",\n),\nDocument(\ntext=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut\",\nembedding=np.array([5, 6]),\nfile=np.random.rand(100),\nid=\"3\",\n),\n]\n)\nbatch_config = {\n\"batch_size\": 20,\n\"dynamic\": False,\n\"timeout_retries\": 3,\n\"num_workers\": 1,\n}\nruntimeconfig = WeaviateDocumentIndex.RuntimeConfig(batch_config=batch_config)\nstore = WeaviateDocumentIndex[Document]()\nstore.configure(runtimeconfig)  # Batch settings being passed on\nstore.index(docs)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#notes","title":"Notes","text":"<ul> <li>To use vector search, you need to specify <code>is_embedding</code> for exactly one field.<ul> <li>This is because Weaviate is configured to allow one vector per data object.</li> <li>If you would like to see Weaviate support multiple vectors per object, upvote the issue which will help to prioritize it.</li> </ul> </li> <li>For a field to be considered as an embedding, its type needs to be of subclass <code>np.ndarray</code> or <code>AbstractTensor</code> and <code>is_embedding</code> needs to be set to <code>True</code>. <ul> <li>If <code>is_embedding</code> is set to <code>False</code> or not provided, the field will be treated as a <code>number[]</code>, and as a result, it will not be added to Weaviate's vector index.</li> </ul> </li> <li>It is possible to create a schema without specifying <code>is_embedding</code> for any field. <ul> <li>This will however mean that the document will not be vectorized and cannot be searched using vector search. </li> </ul> </li> </ul> <p>As you can see, <code>DocList[Document]</code> and <code>WeaviateDocumentIndex[Document]</code> both have <code>Document</code> as a parameter. This means that they share the same schema, and in general, both the Document Index and the data that you want to store need to have compatible schemas.</p> <p>When are two schemas compatible?</p> <p>The schemas of your Document Index and data need to be compatible with each other.</p> <p>Let's say A is the schema of your Document Index and B is the schema of your data. There are a few rules that determine if schema A is compatible with schema B. If any of the following are true, then A and B are compatible:</p> <ul> <li>A and B are the same class</li> <li>A and B have the same field names and field types</li> <li>A and B have the same field names, and, for every field, the type of B is a subclass of the type of A</li> </ul> <p>In particular, this means that you can easily index predefined documents into a Document Index.</p>"},{"location":"user_guide/storing/index_weaviate/#vector-search","title":"Vector search","text":"<p>Now that you have indexed your data, you can perform vector similarity search using the <code>find()</code> method.</p> <p>You can perform a similarity search and find relevant documents by passing <code>MyDoc</code> or a raw vector to  the <code>find()</code> method:</p> Search by DocumentSearch by raw vector <pre><code># create a query document\nquery = Document(\ntext=\"Hello world\",\nembedding=np.array([1, 2]),\nfile=np.random.rand(100),\n)\n# find similar documents\nmatches, scores = doc_index.find(query, limit=5)\nprint(f\"{matches=}\")\nprint(f\"{matches.text=}\")\nprint(f\"{scores=}\")\n</code></pre> <pre><code># create a query vector\nquery = np.random.rand(2)\n# find similar documents\nmatches, scores = store.find(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches.text=}')\nprint(f'{scores=}')\n</code></pre> <p>In this example you only have one field (<code>embedding</code>) that is a vector, so you can trivially choose that one. In general, you could have multiple fields of type <code>NdArray</code> or <code>TorchTensor</code> or <code>TensorFlowTensor</code>, and you can choose which one to use for the search.</p> <p>The <code>find()</code> method returns a named tuple containing the closest matching documents and their associated similarity scores.</p> <p>When searching on the subindex level, you can use the <code>find_subindex()</code> method, which returns a named tuple containing the subindex documents, similarity scores and their associated root documents.</p> <p>How these scores are calculated depends on the backend, and can usually be configured.</p>"},{"location":"user_guide/storing/index_weaviate/#batched-search","title":"Batched search","text":"<p>You can also search for multiple documents at once, in a batch, using the <code>find_batched()</code> method.</p> Search by documentsSearch by raw vectors <pre><code># create some query documents\nqueries = DocList[MyDoc](\nDocument(\ntext=f\"Hello world {i}\",\nembedding=np.array([i, i + 1]),\nfile=np.random.rand(100),\n)\nfor i in range(3)\n)\n# find similar documents\nmatches, scores = doc_index.find_batched(queries, limit=5)\nprint(f\"{matches=}\")\nprint(f\"{matches[0].text=}\")\nprint(f\"{scores=}\")\n</code></pre> <pre><code># create some query vectors\nquery = np.random.rand(3, 2)\n# find similar documents\nmatches, scores = doc_index.find_batched(query, limit=5)\nprint(f'{matches=}')\nprint(f'{matches[0].text=}')\nprint(f'{scores=}')\n</code></pre> <p>The <code>find_batched()</code> method returns a named tuple containing a list of <code>DocList</code>s, one for each query, containing the closest matching documents and their similarity scores.</p>"},{"location":"user_guide/storing/index_weaviate/#filter","title":"Filter","text":"<p>To perform filtering, follow the below syntax. </p> <p>This will perform a filtering on the field <code>text</code>: </p><pre><code>docs = store.filter({\"path\": [\"text\"], \"operator\": \"Equal\", \"valueText\": \"Hello world\"})\n</code></pre> <p>You can filter your documents by using the <code>filter()</code> or <code>filter_batched()</code> method with a corresponding  filter query.  The query should follow the query language of the Weaviate.</p> <p>In the following example let's filter for all the books that are cheaper than 29 dollars:</p> <pre><code>from docarray import BaseDoc, DocList\nfrom docarray.typing import NdArray\nfrom pydantic import Field\nimport numpy as np\nclass Book(BaseDoc):\nprice: int\nembedding: NdArray[10] = Field(is_embedding=True)\nbooks = DocList[Book]([Book(price=i * 10, embedding=np.random.rand(10)) for i in range(10)])\nbook_index = WeaviateDocumentIndex[Book](index_name='tmp_index')\nbook_index.index(books)\n# filter for books that are cheaper than 29 dollars\nquery = {\"path\": [\"price\"], \"operator\": \"LessThan\", \"valueInt\": 29}\ncheap_books = book_index.filter(filter_query=query)\nassert len(cheap_books) == 3\nfor doc in cheap_books:\ndoc.summary()\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#text-search","title":"Text search","text":"<p>In addition to vector similarity search, the Document Index interface offers methods for text search: <code>text_search()</code>, as well as the batched version <code>text_search_batched()</code>.</p> <p>You can use text search directly on the field of type <code>str</code>.</p> <p>The following line will perform a text search for the word \"hello\" in the field \"text\" and return the first two results:</p> <pre><code>docs = store.text_search(\"world\", search_field=\"text\", limit=2)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#hybrid-search","title":"Hybrid search","text":"<p>Document Index supports atomic operations for vector similarity search, text search and filter search.</p> <p>To combine these operations into a single, hybrid search query, you can use the query builder that is accessible through <code>build_query()</code>.</p> <p>To perform a hybrid search, follow the below syntax. </p> <p>This will perform a hybrid search for the word \"hello\" and the vector [1, 2] and return the first two results:</p> <p>Note: Hybrid search searches through the object vector and all fields. Accordingly, the <code>search_field</code> keyword will have no effect. </p> <pre><code>q = store.build_query().text_search(\"world\").find([1, 2]).limit(2).build()\ndocs = store.execute_query(q)\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#graphql-query","title":"GraphQL query","text":"<p>You can also perform a raw GraphQL query using any syntax as you might natively in Weaviate. This allows you to run any of the full range of queries that you might wish to.</p> <p>The below will perform a GraphQL query to obtain the count of <code>Document</code> objects. </p> <pre><code>graphql_query = \"\"\"\n{\n  Aggregate {\n    Document {\n      meta {\n        count\n      }\n    }\n  }\n}\n\"\"\"\nstore.execute_query(graphql_query)\n</code></pre> <p>Note that running a raw GraphQL query will return Weaviate-type responses, rather than a DocArray object type.</p> <p>You can find the documentation for Weaviate's GraphQL API here.</p>"},{"location":"user_guide/storing/index_weaviate/#access-documents","title":"Access documents","text":"<p>To retrieve a document from a Document Index you don't necessarily need to perform a fancy search.</p> <p>You can also access data by the <code>id</code> that was assigned to each document:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), title=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the documents by id\ndoc = doc_index[ids[0]]  # get by single id\ndocs = doc_index[ids]  # get by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#delete-documents","title":"Delete documents","text":"<p>In the same way you can access documents by <code>id</code>, you can also delete them:</p> <pre><code># prepare some data\ndata = DocList[MyDoc](\nMyDoc(embedding=np.random.rand(128), title=f'query {i}') for i in range(3)\n)\n# remember the Document ids and index the data\nids = data.id\ndoc_index.index(data)\n# access the documents by id\ndel doc_index[ids[0]]  # del by single id\ndel doc_index[ids[1:]]  # del by list of ids\n</code></pre>"},{"location":"user_guide/storing/index_weaviate/#configuration","title":"Configuration","text":""},{"location":"user_guide/storing/index_weaviate/#overview","title":"Overview","text":"<p>WCS instances come pre-configured, and as such additional settings are not configurable outside of those chosen at creation, such as whether to enable authentication.</p> <p>For other cases, such as Docker-Compose deployment, its settings can be modified through the configuration file, such as the <code>docker-compose.yaml</code> file. </p> <p>Some of the more commonly used settings include:</p> <ul> <li>Persistent volume: Set up data persistence so that data from inside the Docker container is not lost on shutdown</li> <li>Enabling a multi-node setup</li> <li>Backups</li> <li>Authentication (server-side)</li> <li>Modules enabled</li> </ul> <p>And a list of environment variables is available on this page.</p>"},{"location":"user_guide/storing/index_weaviate/#docarray-instantiation-configuration-options","title":"DocArray instantiation configuration options","text":"<p>Additionally, you can specify the below settings when you instantiate a configuration object in DocArray.</p> name type explanation default example Category: General host str Weaviate instance url http://localhost:8080 Category: Authentication username str Username known to the specified authentication provider (e.g. WCS) <code>None</code> <code>jp@weaviate.io</code> password str Corresponding password <code>None</code> <code>p@ssw0rd</code> auth_api_key str API key known to the Weaviate instance <code>None</code> <code>mys3cretk3y</code> Category: Data schema index_name str Class name to use to store the document The document class name, e.g. <code>MyDoc</code> for <code>WeaviateDocumentIndex[MyDoc]</code> <code>Document</code> Category: Embedded Weaviate embedded_options EmbeddedOptions Options for embedded weaviate <code>None</code> <p>The type <code>EmbeddedOptions</code> can be specified as described here</p>"},{"location":"user_guide/storing/index_weaviate/#runtime-configuration","title":"Runtime configuration","text":"<p>Weaviate strongly recommends using batches to perform bulk operations such as importing data, as it will significantly impact performance. You can specify a batch configuration as in the below example, and pass it on as runtime configuration.</p> <pre><code>batch_config = {\n\"batch_size\": 20,\n\"dynamic\": False,\n\"timeout_retries\": 3,\n\"num_workers\": 1,\n}\nruntimeconfig = WeaviateDocumentIndex.RuntimeConfig(batch_config=batch_config)\ndbconfig = WeaviateDocumentIndex.DBConfig(\nhost=\"http://localhost:8080\"\n)  # Replace with your endpoint and/or auth settings\nstore = WeaviateDocumentIndex[Document](db_config=dbconfig)\nstore.configure(runtimeconfig)  # Batch settings being passed on\n</code></pre> name type explanation default batch_config Dict[str, Any] dictionary to configure the weaviate client's batching logic see below <p>Read more: </p> <ul> <li>Weaviate docs on batching with the Python client</li> </ul>"},{"location":"user_guide/storing/index_weaviate/#available-column-types","title":"Available column types","text":"<p>Python data types are mapped to Weaviate type according to the below conventions.</p> Python type Weaviate type docarray.typing.ID string str text int int float number bool boolean np.ndarray number[] AbstractTensor number[] bytes blob <p>You can override this default mapping by passing a <code>col_type</code> to the <code>Field</code> of a schema. </p> <p>For example to map <code>str</code> to <code>string</code> you can:</p> <pre><code>class StringDoc(BaseDoc):\ntext: str = Field(col_type=\"string\")\n</code></pre> <p>A list of available Weaviate data types is here.</p>"},{"location":"user_guide/storing/index_weaviate/#nested-data-and-subindex-search","title":"Nested data and subindex search","text":"<p>The examples provided primarily operate on a basic schema where each field corresponds to a straightforward type such as <code>str</code> or <code>NdArray</code>.  However, it is also feasible to represent and store nested documents in a Document Index, including scenarios where a document  contains a <code>DocList</code> of other documents. </p> <p>Go to the Nested Data section to learn more.</p>"},{"location":"user_guide/storing/nested_data/","title":"Nested Data","text":""},{"location":"user_guide/storing/nested_data/#nested-data","title":"Nested Data","text":"<p>Most of the examples you've seen operate on a simple schema: each field corresponds to a \"basic\" type, such as <code>str</code> or <code>NdArray</code>.</p> <p>It is, however, also possible to represent nested documents and store them in a Document Index.</p> <p>Using a different vector database</p> <p>In the following examples, we will use <code>InMemoryExactNNIndex</code> as our Document Index.  You can easily use Weaviate, Qdrant, Redis, Milvus or Elasticsearch instead -- their APIs are largely identical! To do so, check their respective documentation sections.</p>"},{"location":"user_guide/storing/nested_data/#create-and-index","title":"Create and index","text":"<p>In the following example you can see a complex schema that contains nested documents. The <code>YouTubeVideoDoc</code> contains a <code>VideoDoc</code> and an <code>ImageDoc</code>, alongside some \"basic\" fields:</p> <pre><code>import numpy as np\nfrom pydantic import Field\nfrom docarray import BaseDoc, DocList\nfrom docarray.index import InMemoryExactNNIndex\nfrom docarray.typing import AnyTensor, ImageUrl, VideoUrl\n# define a nested schema\nclass ImageDoc(BaseDoc):\nurl: ImageUrl\ntensor: AnyTensor = Field(space='cosine_sim', dim=64)\nclass VideoDoc(BaseDoc):\nurl: VideoUrl\ntensor: AnyTensor = Field(space='cosine_sim', dim=128)\nclass YouTubeVideoDoc(BaseDoc):\ntitle: str\ndescription: str\nthumbnail: ImageDoc\nvideo: VideoDoc\ntensor: AnyTensor = Field(space='cosine_sim', dim=256)\n# create a Document Index\ndoc_index = InMemoryExactNNIndex[YouTubeVideoDoc]()\n# create some data\nindex_docs = [\nYouTubeVideoDoc(\ntitle=f'video {i+1}',\ndescription=f'this is video from author {10*i}',\nthumbnail=ImageDoc(url=f'http://example.ai/images/{i}', tensor=np.ones(64)),\nvideo=VideoDoc(url=f'http://example.ai/videos/{i}', tensor=np.ones(128)),\ntensor=np.ones(256),\n)\nfor i in range(8)\n]\n# index the Documents\ndoc_index.index(index_docs)\n</code></pre>"},{"location":"user_guide/storing/nested_data/#search","title":"Search","text":"<p>You can perform search on any nesting level by using the dunder operator to specify the field defined in the nested data.</p> <p>In the following example, you can see how to perform vector search on the <code>tensor</code> field of the <code>YouTubeVideoDoc</code> or on the <code>tensor</code> field of the nested <code>thumbnail</code> and <code>video</code> fields:</p> <pre><code># create a query document\nquery_doc = YouTubeVideoDoc(\ntitle=f'video query',\ndescription=f'this is a query video',\nthumbnail=ImageDoc(url=f'http://example.ai/images/1024', tensor=np.ones(64)),\nvideo=VideoDoc(url=f'http://example.ai/videos/1024', tensor=np.ones(128)),\ntensor=np.ones(256),\n)\n# find by the `youtubevideo` tensor; root level\ndocs, scores = doc_index.find(query_doc, search_field='tensor', limit=3)\n# find by the `thumbnail` tensor; nested level\ndocs, scores = doc_index.find(query_doc, search_field='thumbnail__tensor', limit=3)\n# find by the `video` tensor; neseted level\ndocs, scores = doc_index.find(query_doc, search_field='video__tensor', limit=3)\n</code></pre>"},{"location":"user_guide/storing/nested_data/#nested-data-with-subindex-search","title":"Nested data with subindex search","text":"<p>Documents can be nested by containing a <code>DocList</code> of other documents, which is a slightly more complicated scenario than the one above.</p> <p>If a document contains a <code>DocList</code>, it can still be stored in a Document Index. In this case, the <code>DocList</code> will be represented as a new index (or table, collection, etc., depending on the database backend), that is linked with the parent index (table, collection, etc).</p> <p>This still lets you index and search through all of your data, but if you want to avoid the creation of additional indexes you can refactor your document schemas without the use of <code>DocLists</code>.</p>"},{"location":"user_guide/storing/nested_data/#index","title":"Index","text":"<p>In the following example, you can see a complex schema that contains nested <code>DocLists</code> of documents where we'll utilize subindex search.</p> <p>The <code>MyDoc</code> contains a <code>DocList</code> of <code>VideoDoc</code>, which contains a <code>DocList</code> of <code>ImageDoc</code>, alongside some \"basic\" fields:</p> <pre><code>class ImageDoc(BaseDoc):\nurl: ImageUrl\ntensor_image: AnyTensor = Field(space='cosine_sim', dim=64)\nclass VideoDoc(BaseDoc):\nurl: VideoUrl\nimages: DocList[ImageDoc]\ntensor_video: AnyTensor = Field(space='cosine_sim', dim=128)\nclass MyDoc(BaseDoc):\ndocs: DocList[VideoDoc]\ntensor: AnyTensor = Field(space='cosine_sim', dim=256)\n# create a Document Index\ndoc_index = InMemoryExactNNIndex[MyDoc]()\n# create some data\nindex_docs = [\nMyDoc(\ndocs=DocList[VideoDoc](\n[\nVideoDoc(\nurl=f'http://example.ai/videos/{i}-{j}',\nimages=DocList[ImageDoc](\n[\nImageDoc(\nurl=f'http://example.ai/images/{i}-{j}-{k}',\ntensor_image=np.ones(64),\n)\nfor k in range(10)\n]\n),\ntensor_video=np.ones(128),\n)\nfor j in range(10)\n]\n),\ntensor=np.ones(256),\n)\nfor i in range(10)\n]\n# index the Documents\ndoc_index.index(index_docs)\n</code></pre>"},{"location":"user_guide/storing/nested_data/#search_1","title":"Search","text":"<p>You can perform search on any level by using <code>find_subindex()</code> method  and the dunder operator <code>'root__subindex'</code> to specify the index to search on:</p> <pre><code># find by the `VideoDoc` tensor\nroot_docs, sub_docs, scores = doc_index.find_subindex(\nnp.ones(128), subindex='docs', search_field='tensor_video', limit=3\n)\n# find by the `ImageDoc` tensor\nroot_docs, sub_docs, scores = doc_index.find_subindex(\nnp.ones(64), subindex='docs__images', search_field='tensor_image', limit=3\n)\n</code></pre>"},{"location":"user_guide/storing/doc_store/store_file/","title":"Store on-disk","text":""},{"location":"user_guide/storing/doc_store/store_file/#store-on-disk","title":"Store on-disk","text":"<p>When you want to use your DocList in another place, you can use:</p> <ul> <li>the <code>.push()</code> method to push the DocList  to one place.</li> <li>the <code>.pull()</code> method to pull its content back. </li> </ul>"},{"location":"user_guide/storing/doc_store/store_file/#push-and-pull","title":"Push and pull","text":"<p>To use the store locally, you need to pass a local file path to the function starting with <code>'file://'</code>.</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\ndl = DocList[SimpleDoc]([SimpleDoc(text=f'doc {i}') for i in range(8)])\ndl.push('file://simple_dl')\ndl_pull = DocList[SimpleDoc].pull('file://simple_dl')\n</code></pre> <p>A file with the name of <code>simple_dl.docs</code> will be created in <code>$HOME/.docarray/cache</code> to store the <code>DocList</code>.</p>"},{"location":"user_guide/storing/doc_store/store_file/#push-and-pull-with-streaming","title":"Push and pull with streaming","text":"<p>When you have a large amount of documents to push and pull, you can use the streaming method: <code>.push_stream()</code> and  <code>.pull_stream()</code> stream the <code>DocList</code> to save memory usage. You set multiple <code>DocList</code>s to pull from the same source as well:</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\nstore_docs = [SimpleDoc(text=f'doc {i}') for i in range(8)]\nDocList[SimpleDoc].push_stream(\niter(store_docs),\n'file://dl_stream',\n)\ndl_pull_stream_1 = DocList[SimpleDoc].pull_stream('file://dl_stream')\ndl_pull_stream_2 = DocList[SimpleDoc].pull_stream('file://dl_stream')\nfor d1, d2 in zip(dl_pull_stream_1, dl_pull_stream_2):\nprint(f'get {d1}, get {d2}')\n</code></pre> Output <pre><code>get SimpleDoc(id='5a4b92af27aadbb852d636892506998b', text='doc 0'), get SimpleDoc(id='5a4b92af27aadbb852d636892506998b', text='doc 0')\nget SimpleDoc(id='705e4f6acbab0a6ff10d11a07c03b24c', text='doc 1'), get SimpleDoc(id='705e4f6acbab0a6ff10d11a07c03b24c', text='doc 1')\nget SimpleDoc(id='4fb5c01bd5f935bbe91cf73e271ad590', text='doc 2'), get SimpleDoc(id='4fb5c01bd5f935bbe91cf73e271ad590', text='doc 2')\nget SimpleDoc(id='381498cef78f1d4f1d80415d67918940', text='doc 3'), get SimpleDoc(id='381498cef78f1d4f1d80415d67918940', text='doc 3')\nget SimpleDoc(id='d968bc6fa235b1cfc69eded92926157e', text='doc 4'), get SimpleDoc(id='d968bc6fa235b1cfc69eded92926157e', text='doc 4')\nget SimpleDoc(id='30bf347427a4bd50ce8ada1841320fe3', text='doc 5'), get SimpleDoc(id='30bf347427a4bd50ce8ada1841320fe3', text='doc 5')\nget SimpleDoc(id='1389877ac97b3e6d0e8eb17568934708', text='doc 6'), get SimpleDoc(id='1389877ac97b3e6d0e8eb17568934708', text='doc 6')\nget SimpleDoc(id='264b0eff2cd138d296f15c685e15bf23', text='doc 7'), get SimpleDoc(id='264b0eff2cd138d296f15c685e15bf23', text='doc 7')\n</code></pre>"},{"location":"user_guide/storing/doc_store/store_s3/","title":"Store on S3","text":""},{"location":"user_guide/storing/doc_store/store_s3/#store-on-s3","title":"Store on S3","text":"<p>When you want to use your <code>DocList</code> in another place, you can use the  <code>.push</code> method to push the <code>DocList</code> to S3 and later use the <code>.pull</code> function to pull its content back. </p> <p>Note</p> <p>To store on S3, you need to install the extra dependency with the following line </p><pre><code>pip install \"docarray[aws]\"\n</code></pre>"},{"location":"user_guide/storing/doc_store/store_s3/#push-pull","title":"Push &amp; pull","text":"<p>To use the store <code>DocList</code> on S3, you need to pass an S3 path to the function starting with <code>'s3://'</code>.</p> <p>In the following demo, we use <code>MinIO</code> as a local S3 service. You could use the following docker-compose file to start the service in a Docker container.</p> <p></p><pre><code>version: \"3\"\nservices:\nminio:\ncontainer_name: minio\nimage: \"minio/minio:RELEASE.2023-03-13T19-46-17Z\"\nports:\n- \"9005:9000\"\ncommand: server /data\n</code></pre> Save the above file as <code>docker-compose.yml</code> and run the following line in the same folder as the file. <pre><code>docker-compose up\n</code></pre> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\nif __name__ == '__main__':\nimport boto3\nfrom botocore.client import Config\nBUCKET = 'tmp_bucket'\nmy_session = boto3.session.Session()\ns3 = my_session.resource(\nservice_name='s3',\nregion_name=\"us-east-1\",\nuse_ssl=False,\nendpoint_url=\"http://localhost:9005\",\naws_access_key_id=\"minioadmin\",\naws_secret_access_key=\"minioadmin\",\nconfig=Config(signature_version=\"s3v4\"),\n)\n# make a bucket\ns3.create_bucket(Bucket=BUCKET)\nstore_docs = [SimpleDoc(text=f'doc {i}') for i in range(8)]\ndocs = DocList[SimpleDoc]()\ndocs.extend([SimpleDoc(text=f'doc {i}') for i in range(8)])\n# .push() and .pull() use the default boto3 client\nboto3.Session.client.__defaults__ = (\n\"us-east-1\",\nNone,\nFalse,\nNone,\n\"http://localhost:9005\",\n\"minioadmin\",\n\"minioadmin\",\nNone,\nConfig(signature_version=\"s3v4\"),\n)\ndocs.push(f's3://{BUCKET}/simple_docs')\ndocs_pull = DocList[SimpleDoc].pull(f's3://{BUCKET}/simple_docs')\n</code></pre> <p>Under the bucket <code>tmp_bucket</code>, there is a file with the name of <code>simple_docs.docs</code> being created to store the <code>DocList</code>.</p> <p>Note</p> <p>When using <code>.push()</code> and <code>.pull()</code>, <code>DocList</code> calls the default boto3 client. Be sure your default session is correctly set up.</p>"},{"location":"user_guide/storing/doc_store/store_s3/#push-pull-with-streaming","title":"Push &amp; pull with streaming","text":"<p>When you have a large amount of documents to push and pull, you could use the streaming function.  <code>.push_stream()</code> and  <code>.pull_stream()</code> can help you to stream the  <code>DocList</code> in order to save the memory usage. You set multiple <code>DocList</code> to pull from the same source as well. The usage is the same as using streaming with local files. Please refer to Push &amp; Pull with streaming with local files.</p>"},{"location":"user_guide/storing/doc_store/store_s3/#delete","title":"Delete","text":"<p>To delete the store, you need to use the static method <code>.delete()</code> of <code>S3DocStore</code> class.</p> <pre><code>from docarray import BaseDoc, DocList\nclass SimpleDoc(BaseDoc):\ntext: str\nif __name__ == '__main__':\nimport boto3\nfrom botocore.client import Config\nBUCKET = 'tmp_bucket'\nmy_session = boto3.session.Session()\ns3 = my_session.resource(\nservice_name='s3',\nregion_name=\"us-east-1\",\nuse_ssl=False,\nendpoint_url=\"http://localhost:9005\",\naws_access_key_id=\"minioadmin\",\naws_secret_access_key=\"minioadmin\",\nconfig=Config(signature_version=\"s3v4\"),\n)\n# make a bucket\ns3.create_bucket(Bucket=BUCKET)\nstore_docs = [SimpleDoc(text=f'doc {i}') for i in range(8)]\ndocs = DocList[SimpleDoc]()\ndocs.extend([SimpleDoc(text=f'doc {i}') for i in range(8)])\n# .push() and .pull() use the default boto3 client\nboto3.Session.client.__defaults__ = (\n\"us-east-1\",\nNone,\nFalse,\nNone,\n\"http://localhost:9005\",\n\"minioadmin\",\n\"minioadmin\",\nNone,\nConfig(signature_version=\"s3v4\"),\n)\ndocs.push(f's3://{BUCKET}/simple_docs')\n# delete bucket\nfrom docarray.store import S3DocStore\nsuccess = S3DocStore.delete('{BUCKET}/simple_docs')\n</code></pre>"}]}